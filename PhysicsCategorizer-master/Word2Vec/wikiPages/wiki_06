<doc id="125280" url="https://en.wikipedia.org/wiki?curid=125280" title="Matrix multiplication">
Matrix multiplication

In mathematics, matrix multiplication is a binary operation that takes a pair of matrices, and produces another matrix. Numbers such as the real or complex numbers can be multiplied according to elementary arithmetic. On the other hand, matrices are "arrays of numbers", so there is no unique way to define "the" multiplication of matrices. As such, in general the term "matrix multiplication" refers to a number of different ways to multiply matrices. The key features of any matrix multiplication include: the number of rows and columns the original matrices have (called the "size", "order" or "dimension"), and specifying how the entries of the matrices generate the new matrix.
Like vectors, matrices of any size can be multiplied by scalars, which amounts to multiplying every entry of the matrix by the same number. Similar to the entrywise definition of adding or subtracting matrices, multiplication of two matrices of the same size can be defined by multiplying the corresponding entries, and this is known as the Hadamard product. Another definition is the Kronecker product of two matrices, to obtain a block matrix.
One can form many other definitions. However, the most useful definition can be motivated by linear equations and linear transformations on vectors, which have numerous applications in applied mathematics, physics, and engineering. This definition is often called "the" matrix product. In words, if is an matrix and is an matrix, their matrix product is an matrix, in which the entries across the rows of are multiplied with the entries down the columns of (the precise definition is below).
This definition is not commutative, although it still retains the associative property and is distributive over entrywise addition of matrices. The identity element of the matrix product is the identity matrix (analogous to multiplying numbers by 1), and a square matrix may have an inverse matrix (analogous to the multiplicative inverse of a number). A consequence of the matrix product is determinant multiplicativity. The matrix product is an important operation in linear transformations, matrix groups, and the theory of group representations and irreps.
Computing matrix products is both a central operation in many numerical algorithms and potentially time consuming, making it one of the most well-studied problems in numerical computing. Various algorithms have been devised for computing , especially for large matrices.
This article will use the following notational conventions: matrices are represented by capital letters in bold, e.g. , vectors in lowercase bold, e.g. , and entries of vectors and matrices are italic (since they are scalars), e.g. and . Index notation is often the clearest way to express definitions, and is used as standard in the literature. The entry of matrix is indicated by or , whereas a numerical label (not matrix entries) on a collection of matrices is subscripted only, e.g. , etc.
Scalar multiplication.
The simplest form of multiplication associated with matrices is scalar multiplication, which is a special case of the Kronecker product.
The left scalar multiplication of a matrix with a scalar gives another matrix of the same size as . The entries of are defined by
explicitly:
Similarly, the right scalar multiplication of a matrix with a scalar is defined to be
explicitly:
When the underlying ring is commutative, for example, the real or complex number field, these two multiplications are the same, and are simply called "scalar multiplication". However, for matrices over a more general ring that are "not" commutative, such as the quaternions, they may not be equal.
For a real scalar and matrix:
For quaternion scalars and matrices:
where are the quaternion units. The non-commutativity of quaternion multiplication prevents the transition of changing to .
Matrix product (two matrices).
Assume two matrices are to be multiplied (the generalization to any number is discussed below).
General definition of the matrix product.
If is an matrix and is an matrix,
the matrix product (denoted without multiplication signs or dots) is defined to be the matrix
where each entry is given by multiplying the entries (across row of ) by the entries (down column of ), for , and summing the results over :
Thus the product is defined only if the number of columns in is equal to the number of rows in , in this case . Each entry may be computed one at a time. Sometimes, the summation convention is used as it is understood to sum over the repeated index . To prevent any ambiguity, this convention will not be used in the article.
Usually the entries are numbers or expressions, but can even be matrices themselves (see block matrix). The matrix product can still be calculated exactly the same way. See below for details on how the matrix product can be calculated in terms of blocks taking the forms of rows and columns.
Illustration.
The figure to the right illustrates diagrammatically the product of two matrices and , showing how each intersection in the product matrix corresponds to a row of and a column of .
The values at the intersections marked with circles are:
Examples of matrix products.
Row vector and column vector.
If
their matrix products are:
and
Note and are two different matrices: the first is a matrix while the second is a matrix. Such expressions occur for real-valued Euclidean vectors in Cartesian coordinates, displayed as row and column matrices, in which case is the matrix form of their dot product, while the matrix form of their dyadic or tensor product.
Square matrix and column vector.
If
their matrix product is:
however is not defined.
The product of a square matrix multiplied by a column matrix arises naturally in linear algebra; for solving linear equations and representing linear transformations. By choosing in appropriately, can represent a variety of transformations such as rotations, scaling and reflections, shears, of a geometric shape in space.
Square matrices.
If
their matrix products are:
and
In this case, both products and are defined, and the entries show that and are not equal in general.
Multiplying square matrices which represent linear transformations corresponds to the composite transformation (see below for details).
Row vector, square matrix, and column vector.
If
their matrix product is:
however is not defined. Note that , this is one of many general properties listed below. Expressions of the form occur when calculating the inner product of two vectors displayed as row and column vectors in an arbitrary coordinate system, and the metric tensor in these coordinates written as the square matrix.
Rectangular matrices.
If
their matrix products are:
and
Properties of the matrix product (two matrices).
Analogous to numbers (elements of a field), matrices satisfy the following general properties, although there is one subtlety, due to the nature of matrix multiplication.
Matrix product (any number).
Matrix multiplication can be extended to the case of more than two matrices, provided that for each sequential pair, their dimensions match.
The product of matrices with sizes (where are all simply positive integers and the subscripts are labels corresponding to the matrices, nothing more), is the matrix:
In index notation:
Properties of the matrix product (any number).
The same properties will hold, as long as the ordering of matrices is not changed. Some of the previous properties for more than two matrices generalize as follows.
Examples of chain multiplication.
Similarity transformations involving similar matrices are matrix products of the three square matrices, in the form:
where is the similarity matrix and and are said to be similar if this relation holds. This product appears frequently in linear algebra and applications, such as diagonalizing square matrices and the equivalence between different matrix representations of the same linear operator.
Operations derived from the matrix product.
More operations on square matrices can be defined using the matrix product, such as powers and nth roots by repeated matrix products, the matrix exponential can be defined by a power series, the matrix logarithm is the inverse of matrix exponentiation, and so on.
Powers of matrices.
Square matrices can be multiplied by themselves repeatedly in the same way as ordinary numbers, because they always have the same number of rows and columns. This repeated multiplication can be described as a power of the matrix, a special case of the ordinary matrix product. On the contrary, "rectangular" matrices do not have the same number of rows and columns so they can "never" be raised to a power. An matrix raised to a positive integer is defined as
and the following identities hold, where is a scalar:
The naive computation of matrix powers is to multiply times the matrix to the result, starting with the identity matrix just like the scalar case. This can be improved using exponentiation by squaring, a method commonly used for scalars. For diagonalizable matrices, an even better method is to use the eigenvalue decomposition of . Another method based on the Cayley–Hamilton theorem finds an identity using the matrices' characteristic polynomial, producing a more effective equation for in which a "scalar" is raised to the required power, rather than an entire "matrix".
A special case is the power of a diagonal matrix. Since the product of diagonal matrices amounts to simply multiplying corresponding diagonal elements together, the power of a diagonal matrix will have entries raised to the power. Explicitly;
meaning it is easy to raise a diagonal matrix to a power. When raising an arbitrary matrix (not necessarily a diagonal matrix) to a power, it is often helpful to exploit this property by diagonalizing the matrix first.
Applications of the matrix product.
Linear transformations.
Matrices offer a concise way of representing linear transformations between vector spaces, and matrix multiplication corresponds to the composition of linear transformations. The matrix product of two matrices can be defined when their entries belong to the same ring, and hence can be added and multiplied.
Let , and be vector spaces over the same field with given bases, and be linear transformations and be their composition.
Suppose that , and are the matrices representing the transformations , and with respect to the given bases.
Then , that is, the matrix of the composition (or the product) of linear transformations is the product of their matrices with respect to the given bases.
Linear systems of equations.
A system of linear equations with the same number of equations as variables can be solved by collecting the coefficients of the equations into a square matrix, then inverting the matrix equation.
A similar procedure can be used to solve a system of linear differential equations, see also phase plane.
The inner and outer products.
Given two "column vectors" and , the Euclidean inner product and outer product are the simplest special cases of the matrix product.
Inner product.
The inner product of two vectors in matrix form is equivalent to a column vector multiplied on the left by a row vector:
where denotes the transpose of a.
The matrix product itself can be expressed in terms of inner product. Suppose that the first matrix A is decomposed into its row vectors , and the second matrix into its column vectors :
where
Then:
It is also possible to express a matrix product in terms of concatenations of products of matrices and row or column vectors:
These decompositions are particularly useful for matrices that are envisioned as concatenations of particular types of row vectors or column vectors, e.g. orthogonal matrices (whose rows and columns are unit vectors orthogonal to each other) and Markov matrices (whose rows or columns sum to 1).
Outer product.
The outer product (also known as the dyadic product or tensor product) of two vectors in matrix form is equivalent to a row vector multiplied on the left by a column vector:
An alternative method is to express the matrix product in terms of the outer product. The decomposition is done the other way around, the first matrix is decomposed into column vectors and the second matrix into row vectors :
where this time
This method emphasizes the effect of individual column/row pairs on the result, which is a useful point of view with e.g. covariance matrices, where each such pair corresponds to the effect of a single sample point.
Algorithms for efficient matrix multiplication.
The running time of square matrix multiplication, if carried out naïvely, is . The running time for multiplying rectangular matrices (one -matrix with one -matrix) is , however, more efficient algorithms exist, such as Strassen's algorithm, devised by Volker Strassen in 1969 and often referred to as "fast matrix multiplication". It is based on a way of multiplying two -matrices which requires only 7 multiplications (instead of the usual 8), at the expense of several additional addition and subtraction operations. Applying this recursively gives an algorithm with a multiplicative cost of formula_42. Strassen's algorithm is more complex, and the numerical stability is reduced compared to the naïve algorithm. Nevertheless, it appears in several libraries, such as BLAS, where it is significantly more efficient for matrices with dimensions "n" > 100, and is very useful for large matrices over exact domains such as finite fields, where numerical stability is not an issue.
The current algorithm with the lowest known exponent is a generalization of the Coppersmith–Winograd algorithm that has an asymptotic complexity of , by François Le Gall. This algorithm, and the Coppersmith–Winograd algorithm on which it is based, are similar to Strassen's algorithm: a way is devised for multiplying two -matrices with fewer than multiplications, and this technique is applied recursively. However, the constant coefficient hidden by the Big O notation is so large that these algorithms are only worthwhile for matrices that are too large to handle on present-day computers.
Since any algorithm for multiplying two -matrices has to process all -entries, there is an asymptotic lower bound of operations. Raz (2002) proves a lower bound of for bounded coefficient arithmetic circuits over the real or complex numbers.
Cohn "et al." (2003, 2005) put methods such as the Strassen and Coppersmith–Winograd algorithms in an entirely different group-theoretic context, by utilising triples of subsets of finite groups which satisfy a disjointness property called the triple product property (TPP). They show that if families of wreath products of Abelian groups with symmetric groups realise families of subset triples with a simultaneous version of the TPP, then there are matrix multiplication algorithms with essentially quadratic complexity. Most researchers believe that this is indeed the case. However, Alon, Shpilka and Chris Umans have recently shown that some of these conjectures implying fast matrix multiplication are incompatible with another plausible conjecture, the sunflower conjecture.
Freivalds' algorithm is a simple Monte Carlo algorithm that given matrices verifies in time if .
Parallel matrix multiplication.
Because of the nature of matrix operations and the layout of matrices in memory, it is typically possible to gain substantial performance gains through use of parallelization and vectorization. Several algorithms are possible, among which divide and conquer algorithms based on the block matrix decomposition
that also underlies Strassen's algorithm. Here, , and are presumed to be by (square) matrices, and etc. are by submatrices. From this decomposition, one derives
which consists of eight multiplications of pairs of submatrices, which can all be performed in parallel, followed by an addition step. Applying this recursively, and performing the additions in parallel as well, one obtains an algorithm that runs in time on an ideal machine with an infinite number of processors, and has a maximum possible speedup of on any real computer (although the algorithm isn't practical, a more practical variant achieves speedup).
It should be noted that some lower time-complexity algorithms on paper may have indirect time complexity costs on real machines.
Communication-avoiding and distributed algorithms.
On modern architectures with hierarchical memory, the cost of loading and storing input matrix elements tends to dominate the cost of arithmetic. On a single machine this is the amount of data transferred between RAM and cache, while on a distributed memory multi-node machine it is the amount transferred between nodes; in either case it is called the "communication bandwidth". The naïve algorithm using three nested loops uses communication bandwidth.
Cannon's algorithm, also known as the "2D algorithm", partitions each input matrix into a block matrix whose elements are submatrices of size by , where is the size of fast memory. The naïve algorithm is then used over the block matrices, computing products of submatrices entirely in fast memory. This reduces communication bandwidth to , which is asymptotically optimal (for algorithms performing computation).
In a distributed setting with processors arranged in a by 2D mesh, one submatrix of the result can be assigned to each processor, and the product can be computed with each processor transmitting words, which is asymptotically optimal assuming that each node stores the minimum elements. This can be improved by the "3D algorithm," which arranges the processors in a 3D cube mesh, assigning every product of two input submatrices to a single processor. The result submatrices are then generated by performing a reduction over each row. This algorithm transmits words per processor, which is asymptotically optimal. However, this requires replicating each input matrix element times, and so requires a factor of more memory than is needed to store the inputs. This algorithm can be combined with Strassen to further reduce runtime. "2.5D" algorithms provide a continuous tradeoff between memory usage and communication bandwidth. On modern distributed computing environments such as MapReduce, specialized multiplication algorithms have been developed.
Other forms of multiplication.
Some other ways to multiply two matrices are given below; some, in fact, are simpler than the definition above.
The Cracovian product is yet another form.
Hadamard product.
For two matrices of the same dimensions, there is the Hadamard product, also known as the element-wise product, pointwise product, entrywise product and the Schur product. For two matrices and of the same dimensions, the Hadamard product is a matrix of the same dimensions, the element of is multiplied with the element of , that is:
displayed fully:
This operation is identical to multiplying many ordinary numbers ( of them) all at once; thus the Hadamard product is commutative, associative and distributive over entrywise addition. It is also a principal submatrix of the Kronecker product. It appears in lossy compression algorithms such as JPEG.
Frobenius product.
The Frobenius inner product, sometimes denoted , often denoted formula_47, is the component-wise inner product of two matrices as though they are vectors. It is also the sum of the entries of the Hadamard product. Explicitly,
where "tr" denotes the trace of a matrix and vec denotes vectorization. This inner product induces the Frobenius norm: formula_49.
Kronecker product.
For two matrices and of "any" different dimensions and respectively (no constraints on the dimensions of each matrix), the Kronecker product is the matrix
with dimensions . This is the application of the more general tensor product applied to matrices.

</doc>
<doc id="1543735" url="https://en.wikipedia.org/wiki?curid=1543735" title="Matrix norm">
Matrix norm

In mathematics, a matrix norm is a natural extension of the notion of a vector norm to matrices.
Definition.
In what follows, formula_1 will denote the field of real or complex numbers. Let formula_2 denote the vector space containing all matrices with formula_3 rows and formula_4 columns with entries in formula_1. Throughout, formula_6 denotes the conjugate transpose of matrix formula_7.
A matrix norm is a vector norm on formula_2. That is, if formula_9 denotes the norm of the matrix formula_7, then,
Additionally, in the case of square matrices (thus, "m" = "n"), some (but not all) matrix norms satisfy the following condition, which is related to the fact that matrices are more than just vectors:
A matrix norm that satisfies this additional property is called a sub-multiplicative norm (in some books, the terminology "matrix norm" is used only for those norms which are sub-multiplicative). The set of all "n"-by-"n" matrices, together with such a sub-multiplicative norm, is an example of a Banach algebra.
Induced norm.
If vector norms on "K""m" and "K""n" are given ("K" is the field of real or complex numbers), then one defines the corresponding "induced norm" or "operator norm" on the space of "m"-by-"n" matrices as the following maxima:
The operator norm corresponding to the "p"-norm for vectors is:
These are different from the entrywise "p"-norms and the Schatten "p"-norms for matrices treated below, which are also usually denoted
by formula_29
In some cases, the norms can be computed or estimated by 
For example, if the matrix "A" is defined by
then we have ||"A"||1 = max(|−3|+2+0, 5+6+2, 7+4+8) = max(5,13,19) = 19. and ||"A"||∞ = max(|−3|+5+7, 2+6+4,0+2+8) = max(15,12,10) = 15.
In the special case of "p" = 2 (the Euclidean norm), the induced matrix norm is the "spectral norm". The spectral norm of a matrix "A" is the largest singular value of "A" i.e. the square root of the largest eigenvalue of the positive-semidefinite matrix "A"*"A":
where "A"* denotes the conjugate transpose of "A".
More generally, one can define the subordinate matrix norm on formula_35 induced by 
formula_36 on formula_37, and formula_38 on formula_39 as:
Subordinate norms are consistent with the norms that induce them, giving
For formula_42, any induced operator norm is a sub-multiplicative matrix norm since formula_43 and formula_44
Any induced norm satisfies the inequality
where ρ("A") is the spectral radius of "A". For a symmetric or hermitian matrix formula_7, we have equality for the 2-norm, since in this case the 2-norm is the spectral radius of formula_7. For an arbitrary matrix, we may not have equality for any norm. Take
the spectral radius of formula_7 is 0, but formula_7 is not the zero matrix, and so none of the induced norms are equal to the spectral radius of formula_7.
Furthermore,
for square matrices
we have the spectral radius formula:
"Entrywise" norms.
These vector norms treat an formula_53 matrix as a vector of size formula_54, and 
use one of the familiar vector norms.
For example, using the "p"-norm for vectors, we get:
This is a different norm from the induced "p"-norm (see above) and the Schatten "p"-norm (see below), but the notation is the same.
The special case "p" = 2 is the Frobenius norm, and "p" = ∞ yields the maximum norm.
L2,1 norm.
Let formula_56 be the columns of matrix formula_7. The formula_58 norm
is the sum of the Euclidean norms of the columns of the matrix:
In this norm, the two indices formula_60 and formula_61 of formula_62 are treated differently; all matrix norms introduced prior to the formula_58 norm treat the two indicees symmetrically. The formula_58 norm is used in robust data analysis and sparse coding for feature selection.
The formula_58 norm can be generalized to the formula_66 norm, defined by 
Frobenius norm.
For "p" = "q" = 2, this is called the Frobenius norm or the Hilbert–Schmidt norm, though the latter term is often reserved for operators on Hilbert space. This norm can be defined in various ways:
where "A"* denotes the conjugate transpose of "A", "σi" are the singular values of "A", and the trace function is used. The Frobenius norm is similar to the Euclidean norm on "K""n" and comes from the Frobenius inner product on the space of all matrices.
The Frobenius norm is sub-multiplicative and is very useful for numerical linear algebra. This norm is often easier to compute than induced norms and has the useful property of being invariant under rotations, that is, formula_69 for any rotation matrix formula_70. This property follows from the trace definition restricted to real matrices,
and
where we have used the orthogonal nature of formula_70, that is, formula_74, and the cyclic nature of the trace, formula_75. More generally the norm is invariant under a unitary transformation for complex matrices.
Max norm.
The max norm is the elementwise norm with "p" = ∞:
This norm is not sub-multiplicative.
Schatten norms.
The Schatten "p"-norms arise when applying the "p"-norm to the vector of singular values of a matrix. If the singular values are denoted by "σi", then the Schatten "p"-norm is defined by
These norms again share the notation with the induced and entrywise "p"-norms, but they are different.
All Schatten norms are sub-multiplicative. They are also unitarily invariant, which means that ||"A"|| = ||"UAV"|| for all matrices "A" and all unitary matrices "U" and "V".
The most familiar cases are "p" = 1, 2, ∞. The case "p" = 2 yields the Frobenius norm, introduced before. The case "p" = ∞ yields the spectral norm, which is the matrix norm induced by the vector 2-norm (see above). Finally, "p" = 1 yields the nuclear norm (also known as the "trace norm", or the Ky Fan 'n'-norm), defined as
Consistent norms.
A matrix norm formula_83 on formula_2 is called "consistent" with a vector norm formula_85 on formula_37 and a vector norm formula_87 on formula_39 if:
for all formula_90. All induced norms are consistent by definition.
Compatible norms.
A matrix norm formula_87 on formula_92 is called "compatible" with a vector norm formula_85 on formula_37 if:
for all formula_96. Induced norms are compatible by definition.
Equivalence of norms.
For any two vector norms formula_36 and formula_38, we have
for some positive numbers "r" and "s", for all matrices "A" in formula_2. In other words, all norms on formula_2 are "equivalent"; they induce the same topology on formula_2. This is true because the vector space formula_2 has the finite dimension formula_104.
Moreover, for every vector norm formula_105 on formula_106, there exists a unique positive real number formula_107 such that formula_108 is a sub-multiplicative matrix norm for every formula_109.
A sub-multiplicative matrix norm formula_36 is said to be "minimal" if there exists no other sub-multiplicative matrix norm formula_38 satisfying formula_112.
Examples of norm equivalence.
For matrix formula_113 of rank formula_114, the following inequalities hold:
Here, formula_120 refers to the matrix norm induced by the vector "p"-norm.
Another useful inequality between matrix norms is
which is a special case of Hölder's inequality.

</doc>
<doc id="437758" url="https://en.wikipedia.org/wiki?curid=437758" title="Matrix normal distribution">
Matrix normal distribution

</math>
In statistics, the matrix normal distribution is a probability distribution that is a generalization of the multivariate normal distribution to matrix-valued random variables.
Definition.
The probability density function for the random matrix X ("n" × "p") that follows the matrix normal distribution formula_4 has the form:
where formula_6 denotes trace and M is "n" × "p", U is "n" × "n" and V is "p" × "p".
The matrix normal is related to the multivariate normal distribution in the following way:
if and only if
where formula_9 denotes the Kronecker product and formula_10 denotes the vectorization of formula_1.
Proof.
The equivalence between the above "matrix normal" and "multivariate normal" density functions can be shown using several properties of the trace and Kronecker product, as follows. We start with the argument of the exponent of the matrix normal PDF: 
which is the argument of the exponent of the multivariate normal PDF. The proof is completed by using the determinant property: formula_13
Properties.
If formula_14, then we have the following properties:
Expected values.
The mean, or expected value is:
and we have the following second-order expectations:
where formula_18 denotes trace. 
More generally, for appropriately dimensioned matrices A,B,C:
Transformation.
Transpose transform:
Linear transform: let D ("r"-by-"n"), be of full rank "r ≤ n" and C ("p"-by-"s"), be of full rank "s ≤ p", then:
Example.
Let's imagine a sample of "n" independent "p"-dimensional random variables identically distributed according to a multivariate normal distribution:
When defining the "n" × "p" matrix formula_23 for which the "i"th row is formula_24, we obtain:
where each row of formula_1 is equal to formula_27, that is formula_28, formula_2 is the "n" × "n" identity matrix, that is the rows are independent, and formula_30.
Maximum Likelihood Parameter Estimation.
Given "k" matrices, each of size "n" × "p", denoted formula_31, which we assume have been sampled i.i.d. from a matrix normal distribution, the maximum likelihood estimate of the parameters can be obtained by maximizing:
The solution for the mean has a closed form, namely
but the covariance parameters do not. However, these parameters can be iteratively maximized by zero-ing their gradients at: 
and
See for example and references therein. The covariance parameters are non-identifiable in the sense that for any scale factor, "s>0", we have:
Drawing values from the distribution.
Sampling from the matrix normal distribution is a special case of the sampling procedure for the multivariate normal distribution. Let formula_23 be an "n" by "p" matrix of "np" independent samples from the standard normal distribution, so that
Then let 
so that
where A and B can be chosen by Cholesky decomposition or a similar matrix square root operation.
Relation to other distributions.
Dawid (1981) provides a discussion of the relation of the matrix-valued normal distribution to other distributions, including the Wishart distribution, Inverse Wishart distribution and matrix t-distribution, but uses different notation from that employed here.

</doc>
<doc id="19673093" url="https://en.wikipedia.org/wiki?curid=19673093" title="Matter">
Matter

Before the 20th century, the term matter included ordinary matter composed of atoms and excluded other energy phenomena such as light or sound. This concept of matter may be generalized from atoms to include any objects having mass even when at rest, but this is ill-defined because an object's mass can arise from its (possibly massless) constituents' motion and interaction energies. Thus, matter does not have a universal definition, nor is it a fundamental concept in physics today. Matter is also used loosely as a general term for the substance that makes up all observable physical objects.
All the objects from everyday life that we can bump into, touch or squeeze
are composed of atoms. This atomic matter is in turn made up of interacting subatomic particles—usually a nucleus of protons and neutrons, and a cloud of orbiting electrons. 
Typically, science considers these composite particles matter because they have both rest mass and volume. By contrast, massless particles, such as photons, are not considered matter, because they have neither rest mass nor volume. However, not all particles with rest mass have a classical volume, since fundamental particles such as quarks and leptons (sometimes equated with matter) are considered "point particles" with no effective size or volume. Nevertheless, quarks and leptons together make up "ordinary matter", and their interactions contribute to the effective volume of the composite particles that make up ordinary matter.
Matter commonly exists in four "states" (or "phases"): solid, liquid and gas, and plasma. However, advances in experimental techniques have revealed other previously theoretical phases, such as Bose–Einstein condensates and fermionic condensates. A focus on an elementary-particle view of matter also leads to new phases of matter, such as the quark–gluon plasma. For much of the history of the natural sciences people have contemplated the exact nature of matter. The idea that matter was built of discrete building blocks, the so-called "particulate theory of matter", was first put forward by the Greek philosophers Leucippus (~490 BC) and Democritus (~470–380 BC).
Matter should not be confused with mass, as the two are not quite the same in modern physics. For example, mass is a conserved quantity, which means that its value is unchanging through time, within closed systems. However, matter is "not" conserved in such systems, although this is not obvious in ordinary conditions on Earth, where matter is approximately conserved. Still, special relativity shows that matter may disappear by conversion into energy, even inside closed systems, and it can also be created from energy, within such systems. However, because "mass" (like energy) can neither be created nor destroyed, the quantity of mass and the quantity of energy remain the same during a transformation of matter (which represents a certain amount of energy) into non-material (i.e., non-matter) energy. This is also true in the reverse transformation of energy into matter.
Different fields of science use the term matter in different, and sometimes incompatible, ways. Some of these ways are based on loose historical meanings, from a time when there was no reason to distinguish mass and matter. As such, there is no single universally agreed scientific meaning of the word "matter". Scientifically, the term "mass" is well-defined, but "matter" is not. Sometimes in the field of physics "matter" is simply equated with particles that exhibit rest mass (i.e., that cannot travel at the speed of light), such as quarks and leptons. However, in both physics and chemistry, matter exhibits both wave-like and particle-like properties, the so-called wave–particle duality.
Definition.
Common definition.
The common definition of matter is "anything that has mass and volume (occupies space)". For example, a car would be said to be made of matter, as it occupies space, and has mass.
The observation that matter occupies space goes back to antiquity. However, an explanation for why matter occupies space is recent, and is argued to be a result of the phenomenon described in the Pauli exclusion principle. Two particular examples where the exclusion principle clearly relates matter to the occupation of space are white dwarf stars and neutron stars, discussed further below.
Relativity.
In the context of relativity, mass is not an additive quantity, in the sense that one can add the rest masses of particles in a system to get the total rest mass of the system. Thus, in relativity usually a more general view is that it is not the sum of rest masses, but the energy–momentum tensor that quantifies the amount of matter. This tensor gives the rest mass for the entire system. "Matter" therefore is sometimes considered as anything that contributes to the energy–momentum of a system, that is, anything that is not purely gravity. This view is commonly held in fields that deal with general relativity such as cosmology. In this view, light and other massless particles and fields are part of matter.
The reason for this is that in this definition, electromagnetic radiation (such as light) as well as the energy of electromagnetic fields contributes to the mass of systems, and therefore appears to add matter to them. For example, light radiation (or thermal radiation) trapped inside a box would contribute to the mass of the box, as would any kind of energy inside the box, including the kinetic energy of particles held by the box. Nevertheless, isolated individual particles of light (photons) and the isolated kinetic energy of massive particles, are normally not considered to be "matter."
A difference between matter and mass therefore may seem to arise when single particles are examined. In such cases, the mass of single photons is zero. For particles with rest mass, such as leptons and quarks, isolation of the particle in a frame where it is not moving, removes its kinetic energy.
A source of definition difficulty in relativity arises from two definitions of mass in common use, one of which is formally equivalent to total energy (and is thus observer dependent), and the other of which is referred to as rest mass or invariant mass and is independent of the observer. Only "rest mass" is loosely equated with matter (since it can be weighed). Invariant mass is usually applied in physics to unbound systems of particles. However, energies which contribute to the "invariant mass" may be weighed also in special circumstances, such as when a system that has invariant mass is confined and has no net momentum (as in the box example above). Thus, a photon with no mass may (confusingly) still add mass to a system in which it is trapped. The same is true of the kinetic energy of particles, which by definition is not part of their rest mass, but which does add rest mass to systems in which these particles reside (an example is the mass added by the motion of gas molecules of a bottle of gas, or by the thermal energy of any hot object).
Since such mass (kinetic energies of particles, the energy of trapped electromagnetic radiation and stored potential energy of repulsive fields) is measured as part of the mass of ordinary "matter" in complex systems, the "matter" status of "massless particles" and fields of force becomes unclear in such systems. These problems contribute to the lack of a rigorous definition of matter in science, although mass is easier to define as the total stress–energy above (this is also what is weighed on a scale, and what is the source of gravity).
Atoms definition.
A definition of "matter" based on its physical and chemical structure is: "matter is made up of atoms". As an example, deoxyribonucleic acid molecules (DNA) are matter under this definition because they are made of atoms. This definition can extend to include charged atoms and molecules, so as to include plasmas (gases of ions) and electrolytes (ionic solutions), which are not obviously included in the atoms definition. Alternatively, one can adopt the "protons, neutrons, and electrons" definition.
Protons, neutrons and electrons definition.
A definition of "matter" more fine-scale than the atoms and molecules definition is: "matter is made up of what atoms and molecules are made of", meaning anything made of positively charged protons, neutral neutrons, and negatively charged electrons. This definition goes beyond atoms and molecules, however, to include substances made from these building blocks that are "not" simply atoms or molecules, for example white dwarf matter—typically, carbon and oxygen nuclei in a sea of degenerate electrons. At a microscopic level, the constituent "particles" of matter such as protons, neutrons, and electrons obey the laws of quantum mechanics and exhibit wave–particle duality. At an even deeper level, protons and neutrons are made up of quarks and the force fields (gluons) that bind them together (see Quarks and leptons definition below).
Quarks and leptons definition.
As seen in the above discussion, many early definitions of what can be called "ordinary matter" were based upon its structure or "building blocks". On the scale of elementary particles, a definition that follows this tradition can be stated as: "ordinary matter is everything that is composed of elementary fermions, namely quarks and leptons." The connection between these formulations follows.
Leptons (the most famous being the electron), and quarks (of which baryons, such as protons and neutrons, are made) combine to form atoms, which in turn form molecules. Because atoms and molecules are said to be matter, it is natural to phrase the definition as: "ordinary matter is anything that is made of the same things that atoms and molecules are made of". (However, notice that one also can make from these building blocks matter that is "not" atoms or molecules.) Then, because electrons are leptons, and protons, and neutrons are made of quarks, this definition in turn leads to the definition of matter as being "quarks and leptons", which are the two types of elementary fermions. Carithers and Grannis state: "Ordinary matter is composed entirely of first-generation particles, namely the [up] and [down] quarks, plus the electron and its neutrino." (Higher generations particles quickly decay into first-generation particles, and thus are not commonly encountered.)
This definition of ordinary matter is more subtle than it first appears. All the particles that make up ordinary matter (leptons and quarks) are elementary fermions, while all the force carriers are elementary bosons. The W and Z bosons that mediate the weak force are not made of quarks or leptons, and so are not ordinary matter, even if they have mass. In other words, mass is not something that is exclusive to ordinary matter.
The quark–lepton definition of ordinary matter, however, identifies not only the elementary building blocks of matter, but also includes composites made from the constituents (atoms and molecules, for example). Such composites contain an interaction energy that holds the constituents together, and may constitute the bulk of the mass of the composite. As an example, to a great extent, the mass of an atom is simply the sum of the masses of its constituent protons, neutrons and electrons. However, digging deeper, the protons and neutrons are made up of quarks bound together by gluon fields (see dynamics of quantum chromodynamics) and these gluons fields contribute significantly to the mass of hadrons. In other words, most of what composes the "mass" of ordinary matter is due to the binding energy of quarks within protons and neutrons. For example, the sum of the mass of the three quarks in a nucleon is approximately , which is low compared to the mass of a nucleon (approximately ). The bottom line is that most of the mass of everyday objects comes from the interaction energy of its elementary components.
Smaller building blocks issue.
The Standard Model groups matter particles into three generations, where each generation consists of two quarks and two leptons. The first generation is the "up" and "down" quarks, the "electron" and the "electron neutrino"; the second includes the "charm" and "strange" quarks, the "muon" and the "muon neutrino"; the third generation consists of the "top" and "bottom" quarks and the "tau" and "tau neutrino". The most natural explanation for this would be that quarks and leptons of higher generations are excited states of the first generations. If this turns out to be the case, it would imply that quarks and leptons are composite particles, rather than elementary particles.
Structure.
In particle physics, fermions are particles that obey Fermi–Dirac statistics. Fermions can be elementary, like the electron—or composite, like the proton and neutron. In the Standard Model, there are two types of elementary fermions: quarks and leptons, which are discussed next.
Quarks.
Quarks are particles of spin-, implying that they are fermions. They carry an electric charge of − e (down-type quarks) or + e (up-type quarks). For comparison, an electron has a charge of −1 e. They also carry colour charge, which is the equivalent of the electric charge for the strong interaction. Quarks also undergo radioactive decay, meaning that they are subject to the weak interaction. Quarks are massive particles, and therefore are also subject to gravity.
Baryonic matter.
Baryons are strongly interacting fermions, and so are subject to Fermi–Dirac statistics. Amongst the baryons are the protons and neutrons, which occur in atomic nuclei, but many other unstable baryons exist as well. The term baryon usually refers to triquarks—particles made of three quarks. "Exotic" baryons made of four quarks and one antiquark are known as the pentaquarks, but their existence is not generally accepted.
Baryonic matter is the part of the universe that is made of baryons (including all atoms). This part of the universe does not include dark energy, dark matter, black holes or various forms of degenerate matter, such as compose white dwarf stars and neutron stars. Microwave light seen by Wilkinson Microwave Anisotropy Probe (WMAP), suggests that only about 4.6% of that part of the universe within range of the best telescopes (that is, matter that may be visible because light could reach us from it), is made of baryonic matter. About 23% is dark matter, and about 72% is dark energy.
Degenerate matter.
In physics, degenerate matter refers to the ground state of a gas of fermions at a temperature near absolute zero. The Pauli exclusion principle requires that only two fermions can occupy a quantum state, one spin-up and the other spin-down. Hence, at zero temperature, the fermions fill up sufficient levels to accommodate all the available fermions—and in the case of many fermions, the maximum kinetic energy (called the "Fermi energy") and the pressure of the gas becomes very large, and depends on the number of fermions rather than the temperature, unlike normal states of matter.
Degenerate matter is thought to occur during the evolution of heavy stars. The demonstration by Subrahmanyan Chandrasekhar that white dwarf stars have a maximum allowed mass because of the exclusion principle caused a revolution in the theory of star evolution.
Degenerate matter includes the part of the universe that is made up of neutron stars and white dwarfs.
Strange matter.
Strange matter is a particular form of quark matter, usually thought of as a "liquid" of up, down, and strange quarks. It is contrasted with nuclear matter, which is a liquid of neutrons and protons (which themselves are built out of up and down quarks), and with non-strange quark matter, which is a quark liquid that contains only up and down quarks. At high enough density, strange matter is expected to be color superconducting. Strange matter is hypothesized to occur in the core of neutron stars, or, more speculatively, as isolated droplets that may vary in size from femtometers (strangelets) to kilometers (quark stars).
Two meanings of the term "strange matter".
In particle physics and astrophysics, the term is used in two ways, one broader and the other more specific.
Leptons.
Leptons are particles of spin-, meaning that they are fermions. They carry an electric charge of −1 e (charged leptons) or 0 e (neutrinos). Unlike quarks, leptons do not carry colour charge, meaning that they do not experience the strong interaction. Leptons also undergo radioactive decay, meaning that they are subject to the weak interaction. Leptons are massive particles, therefore are subject to gravity.
Phases.
In bulk, matter can exist in several different forms, or states of aggregation, known as "phases", depending on ambient pressure, temperature and volume. A phase is a form of matter that has a relatively uniform chemical composition and physical properties (such as density, specific heat, refractive index, and so forth). These phases include the three familiar ones (solids, liquids, and gases), as well as more exotic states of matter (such as plasmas, superfluids, supersolids, Bose–Einstein condensates, ...). A "fluid" may be a liquid, gas or plasma. There are also paramagnetic and ferromagnetic phases of magnetic materials. As conditions change, matter may change from one phase into another. These phenomena are called phase transitions, and are studied in the field of thermodynamics. In nanomaterials, the vastly increased ratio of surface area to volume results in matter that can exhibit properties entirely different from those of bulk material, and not well described by any bulk phase (see nanomaterials for more details).
Phases are sometimes called "states of matter", but this term can lead to confusion with thermodynamic states. For example, two gases maintained at different pressures are in different "thermodynamic states" (different pressures), but in the same "phase" (both are gases).
Antimatter.
In particle physics and quantum chemistry, antimatter is matter that is composed of the antiparticles of those that constitute ordinary matter. If a particle and its antiparticle come into contact with each other, the two annihilate; that is, they may both be converted into other particles with equal energy in accordance with Einstein's equation . These new particles may be high-energy photons (gamma rays) or other particle–antiparticle pairs. The resulting particles are endowed with an amount of kinetic energy equal to the difference between the rest mass of the products of the annihilation and the rest mass of the original particle–antiparticle pair, which is often quite large.
Antimatter is not found naturally on Earth, except very briefly and in vanishingly small quantities (as the result of radioactive decay, lightning or cosmic rays). This is because antimatter that came to exist on Earth outside the confines of a suitable physics laboratory would almost instantly meet the ordinary matter that Earth is made of, and be annihilated. Antiparticles and some stable antimatter (such as antihydrogen) can be made in tiny amounts, but not in enough quantity to do more than test a few of its theoretical properties.
There is considerable speculation both in science and science fiction as to why the observable universe is apparently almost entirely matter, and whether other places are almost entirely antimatter instead. In the early universe, it is thought that matter and antimatter were equally represented, and the disappearance of antimatter requires an asymmetry in physical laws called the charge parity (or CP symmetry) violation. CP symmetry violation can be obtained from the Standard Model, but at this time the apparent asymmetry of matter and antimatter in the visible universe is one of the great unsolved problems in physics. Possible processes by which it came about are explored in more detail under baryogenesis.
Other types.
Ordinary matter, in the quarks and leptons definition, constitutes about 4% of the energy of the observable universe. The remaining energy is theorized to be due to exotic forms, of which 23% is dark matter and 73% is dark energy.
Dark matter.
In astrophysics and cosmology, dark matter is matter of unknown composition that does not emit or reflect enough electromagnetic radiation to be observed directly, but whose presence can be inferred from gravitational effects on visible matter. Observational evidence of the early universe and the big bang theory require that this matter have energy and mass, but is not composed of either elementary fermions (as above) OR gauge bosons. The commonly accepted view is that most of the dark matter is non-baryonic in nature. As such, it is composed of particles as yet unobserved in the laboratory. Perhaps they are supersymmetric particles, which are not Standard Model particles, but relics formed at very high energies in the early phase of the universe and still floating about.
Dark energy.
In cosmology, dark energy is the name given to the antigravitating influence that is accelerating the rate of expansion of the universe. It is known not to be composed of known particles like protons, neutrons or electrons, nor of the particles of dark matter, because these all gravitate.
Exotic matter.
Exotic matter is a hypothetical concept of particle physics. It covers any material that violates one or more classical conditions or is not made of known baryonic particles. Such materials would possess qualities like negative mass or being repelled rather than attracted by gravity.
Historical development.
Origins.
The pre-Socratics were among the first recorded speculators about the underlying nature of the visible world. Thales (c. 624 BC–c. 546 BC) regarded water as the fundamental material of the world. Anaximander (c. 610 BC–c. 546 BC) posited that the basic material was wholly characterless or limitless: the Infinite ("apeiron"). Anaximenes (flourished 585 BC, d. 528 BC) posited that the basic stuff was "pneuma" or air. Heraclitus (c. 535–c. 475 BC) seems to say the basic element is fire, though perhaps he means that all is change. Empedocles (c. 490–430 BC) spoke of four elements of which everything was made: earth, water, air, and fire. Meanwhile, Parmenides argued that change does not exist, and Democritus argued that everything is composed of minuscule, inert bodies of all shapes called atoms, a philosophy called atomism. All of these notions had deep philosophical problems.
Aristotle (384 BC – 322 BC) was the first to put the conception on a sound philosophical basis, which he did in his natural philosophy, especially in "Physics" book I. He adopted as reasonable suppositions the four Empedoclean elements, but added a fifth, aether. Nevertheless, these elements are not basic in Aristotle's mind. Rather they, like everything else in the visible world, are composed of the basic "principles" matter and form.
The word Aristotle uses for matter, ὑλη ("hyle" or "hule"), can be literally translated as wood or timber, that is, "raw material" for building. Indeed, Aristotle's conception of matter is intrinsically linked to something being made or composed. In other words, in contrast to the early modern conception of matter as simply occupying space, matter for Aristotle is definitionally linked to process or change: matter is what underlies a change of substance.
For example, a horse eats grass: the horse changes the grass into itself; the grass as such does not persist in the horse, but some aspect of it—its matter—does. The matter is not specifically described (e.g., as atoms), but consists of whatever persists in the change of substance from grass to horse. Matter in this understanding does not exist independently (i.e., as a substance), but exists interdependently (i.e., as a "principle") with form and only insofar as it underlies change. It can be helpful to conceive of the relationship of matter and form as very similar to that between parts and whole. For Aristotle, matter as such can only "receive" actuality from form; it has no activity or actuality in itself, similar to the way that parts as such only have their existence "in" a whole (otherwise they would be independent wholes).
Early modernity.
René Descartes (1596–1650) originated the modern conception of matter. He was primarily a geometer. Instead of, like Aristotle, deducing the existence of matter from the physical reality of change, Descartes arbitrarily postulated matter to be an abstract, mathematical substance that occupies space:
For Descartes, matter has only the property of extension, so its only activity aside from locomotion is to exclude other bodies: this is the mechanical philosophy. Descartes makes an absolute distinction between mind, which he defines as unextended, thinking substance, and matter, which he defines as unthinking, extended substance. They are independent things. In contrast, Aristotle defines matter and the formal/forming principle as complementary "principles" that together compose one independent thing (substance). In short, Aristotle defines matter (roughly speaking) as what things are actually made of (with a "potential" independent existence), but Descartes elevates matter to an actual independent thing in itself.
The continuity and difference between Descartes' and Aristotle's conceptions is noteworthy. In both conceptions, matter is passive or inert. In the respective conceptions matter has different relationships to intelligence. For Aristotle, matter and intelligence (form) exist together in an interdependent relationship, whereas for Descartes, matter and intelligence (mind) are definitionally opposed, independent substances.
Descartes' justification for restricting the inherent qualities of matter to extension is its permanence, but his real criterion is not permanence (which equally applied to color and resistance), but his desire to use geometry to explain all material properties. Like Descartes, Hobbes, Boyle, and Locke argued that the inherent properties of bodies were limited to extension, and that so-called secondary qualities, like color, were only products of human perception.
Isaac Newton (1643–1727) inherited Descartes' mechanical conception of matter. In the third of his "Rules of Reasoning in Philosophy", Newton lists the universal qualities of matter as "extension, hardness, impenetrability, mobility, and inertia". Similarly in "Optics" he conjectures that God created matter as "solid, massy, hard, impenetrable, movable particles", which were "...even so very hard as never to wear or break in pieces". The "primary" properties of matter were amenable to mathematical description, unlike "secondary" qualities such as color or taste. Like Descartes, Newton rejected the essential nature of secondary qualities.
Newton developed Descartes' notion of matter by restoring to matter intrinsic properties in addition to extension (at least on a limited basis), such as mass. Newton's use of gravitational force, which worked "at a distance", effectively repudiated Descartes' mechanics, in which interactions happened exclusively by contact.
Though Newton's gravity would seem to be a "power" of bodies, Newton himself did not admit it to be an "essential" property of matter. Carrying the logic forward more consistently, Joseph Priestley argued that corporeal properties transcend contact mechanics: chemical properties require the "capacity" for attraction. He argued matter has other inherent powers besides the so-called primary qualities of Descartes, et al.
Since Priestley's time, there has been a massive expansion in knowledge of the constituents of the material world (viz., molecules, atoms, subatomic particles), but there has been no further development in the "definition" of matter. Rather the question has been set aside. Noam Chomsky summarizes the situation that has prevailed since that time:
So matter is whatever physics studies and the object of study of physics is matter: there is no independent general definition of matter, apart from its fitting into the methodology of measurement and controlled experimentation. In sum, the boundaries between what constitutes matter and everything else remains as vague as the demarcation problem of delimiting science from everything else.
Late nineteenth and early twentieth centuries.
In the 19th century, following the development of the periodic table, and of atomic theory, atoms were seen as being the fundamental constituents of matter; atoms formed molecules and compounds.
The common definition in terms of occupying space and having mass is in contrast with most physical and chemical definitions of matter, which rely instead upon its structure and upon attributes not necessarily related to volume and mass. At the turn of the nineteenth century, the knowledge of matter began a rapid evolution.
Aspects of the Newtonian view still held sway. James Clerk Maxwell discussed matter in his work "Matter and Motion". He carefully separates "matter" from space and time, and defines it in terms of the object referred to in Newton's first law of motion.
However, the Newtonian picture was not the whole story. In the 19th century, the term "matter" was actively discussed by a host of scientists and philosophers, and a brief outline can be found in Levere. A textbook discussion from 1870 suggests matter is what is made up of atoms:Three divisions of matter are recognized in science: masses, molecules and atoms. A Mass of matter is any portion of matter appreciable by the senses. A Molecule is the smallest particle of matter into which a body can be divided without losing its identity. An Atom is a still smaller particle produced by division of a molecule. 
Rather than simply having the attributes of mass and occupying space, matter was held to have chemical and electrical properties. The famous physicist J. J. Thomson wrote about the "constitution of matter" and was concerned with the possible connection between matter and electrical charge.
Later developments.
There is an entire literature concerning the "structure of matter", ranging from the "electrical structure" in the early 20th century, to the more recent "quark structure of matter", introduced today with the remark: "Understanding the quark structure of matter has been one of the most important advances in contemporary physics." In this connection, physicists speak of "matter fields", and speak of particles as "quantum excitations of a mode of the matter field". And here is a quote from de Sabbata and Gasperini: "With the word "matter" we denote, in this context, the sources of the interactions, that is spinor fields (like quarks and leptons), which are believed to be the fundamental components of matter, or scalar fields, like the Higgs particles, which are used to introduced mass in a gauge theory (and that, however, could be composed of more fundamental fermion fields)."
The modern conception of matter has been refined many times in history, in light of the improvement in knowledge of just "what" the basic building blocks are, and in how they interact.
In the late 19th century with the discovery of the electron, and in the early 20th century, with the discovery of the atomic nucleus, and the birth of particle physics, matter was seen as made up of electrons, protons and neutrons interacting to form atoms. Today, we know that even protons and neutrons are not indivisible, they can be divided into quarks, while electrons are part of a particle family called leptons. Both quarks and leptons are elementary particles, and are currently seen as being the fundamental constituents of matter.
These quarks and leptons interact through four fundamental forces: gravity, electromagnetism, weak interactions, and strong interactions. The Standard Model of particle physics is currently the best explanation for all of physics, but despite decades of efforts, gravity cannot yet be accounted for at the quantum level; it is only described by classical physics (see quantum gravity and graviton). Interactions between quarks and leptons are the result of an exchange of force-carrying particles (such as photons) between quarks and leptons. The force-carrying particles are not themselves building blocks. As one consequence, mass and energy (which cannot be created or destroyed) cannot always be related to matter (which can be created out of non-matter particles such as photons, or even out of pure energy, such as kinetic energy). Force carriers are usually not considered matter: the carriers of the electric force (photons) possess energy (see Planck relation) and the carriers of the weak force (W and Z bosons) are massive, but neither are considered matter either. However, while these particles are not considered matter, they do contribute to the total mass of atoms, subatomic particles, and all systems that contain them.
Summary.
The term "matter" is used throughout physics in a bewildering variety of contexts: for example, one refers to "condensed matter physics", "elementary matter", "partonic" matter, "dark" matter, "anti"-matter, "strange" matter, and "nuclear" matter. In discussions of matter and antimatter, normal matter has been referred to by Alfvén as "koinomatter" (Gk. "common matter"). It is fair to say that in physics, there is no broad consensus as to a general definition of matter, and the term "matter" usually is used in conjunction with a specifying modifier.
See also.
Antimatter
Cosmology
Dark matter
Philosophy
Other

</doc>
<doc id="229104" url="https://en.wikipedia.org/wiki?curid=229104" title="Matter wave">
Matter wave

All matter can exhibit wave-like behaviour. For example a beam of electrons can be diffracted just like a beam of light or a water wave. Matter waves are a central part of the theory of quantum mechanics, being an example of wave–particle duality. The concept that matter behaves like a wave is also referred to as the de Broglie hypothesis () due to having been proposed by Louis de Broglie in 1924. Matter waves are often referred to as de Broglie waves.
The de Broglie wavelength is the wavelength, , associated with a massive particle and is related to its momentum, , through the Planck constant, :
Wave-like behaviour of matter was first experimentally demonstrated in the Davisson–Germer experiment using electrons, and it has also been confirmed for other elementary particles, neutral atoms and even molecules. The wave-like behaviour of matter is crucial to the modern theory of atomic structure and particle physics.
Historical context.
At the end of the 19th century, light was thought to consist of waves of electromagnetic fields which propagated according to Maxwell’s equations, while matter was thought to consist of localized particles (See history of wave and particle viewpoints). In 1900, this division was exposed to doubt, when, investigating the theory of black body thermal radiation, Max Planck proposed that light is emitted in discrete quanta of energy. It was thoroughly challenged in 1905. Extending Planck's investigation in several ways, including its connection with the photoelectric effect, Albert Einstein proposed that light is also propagated and absorbed in quanta. Light quanta are now called photons. These quanta would have an energy given by the Planck–Einstein relation:
and a momentum
where (lowercase Greek letter nu) and (lowercase Greek letter lambda) denote the frequency and wavelength of the light, the speed of light, and Planck’s constant. In the modern convention, frequency is symbolized by "f" as is done in the rest of this article. Einstein’s postulate was confirmed experimentally by Robert Millikan and Arthur Compton over the next two decades.
The de Broglie hypothesis.
De Broglie, in his 1924 PhD thesis, proposed that just as light has both wave-like and particle-like properties, electrons also have wave-like properties.
By rearranging the momentum equation stated in the above section, we find a relationship between the wavelength, associated with an electron and its momentum, , through the Planck constant, :
The relationship is now known to hold for all types of matter: all matter exhibits properties of both particles and waves.
In 1926, Erwin Schrödinger published an equation describing how a matter wave should evolve—the matter wave analogue of Maxwell’s equations—and used it to derive the energy spectrum of hydrogen.
Experimental confirmation.
Matter waves were first experimentally confirmed to occur in the Davisson-Germer experiment for electrons, and the de Broglie hypothesis has been confirmed for other elementary particles. Furthermore, neutral atoms and even molecules have been shown to be wave-like.
Electrons.
In 1927 at Bell Labs, Clinton Davisson and Lester Germer fired slow-moving electrons at a crystalline nickel target. The angular dependence of the diffracted electron intensity was measured, and was determined to have the same diffraction pattern as those predicted by Bragg for x-rays. Before the acceptance of the de Broglie hypothesis, diffraction was a property that was thought to be only exhibited by waves. Therefore, the presence of any diffraction effects by matter demonstrated the wave-like nature of matter. When the de Broglie wavelength was inserted into the Bragg condition, the observed diffraction pattern was predicted, thereby experimentally confirming the de Broglie hypothesis for electrons.
This was a pivotal result in the development of quantum mechanics. Just as the photoelectric effect demonstrated the particle nature of light, the Davisson–Germer experiment showed the wave-nature of matter, and completed the theory of wave-particle duality. For physicists this idea was important because it meant that not only could any particle exhibit wave characteristics, but that one could use wave equations to describe phenomena in matter if one used the de Broglie wavelength.
Neutral atoms.
Experiments with Fresnel diffraction and an atomic mirror for specular reflection of neutral atoms confirm the application of the de Broglie hypothesis to atoms, i.e. the existence of atomic waves which undergo diffraction, interference and allow quantum reflection by the tails of the attractive potential. Advances in laser cooling have allowed cooling of neutral atoms down to nanokelvin temperatures. At these temperatures, the thermal de Broglie wavelengths come into the micrometre range. Using Bragg diffraction of atoms and a Ramsey interferometry technique, the de Broglie wavelength of cold sodium atoms was explicitly measured and found to be consistent with the temperature measured by a different method.
This effect has been used to demonstrate atomic holography, and it may allow the construction of an atom probe imaging system with nanometer resolution. The description of these phenomena is based on the wave properties of neutral atoms, confirming the de Broglie hypothesis.
The effect has also been used to explain the spatial version of the quantum Zeno effect, in which an otherwise unstable object may be stabilised by rapidly-repeated observations.
Molecules.
Recent experiments even confirm the relations for molecules and even macromolecules that otherwise might be supposed too large to undergo quantum mechanical effects. In 1999, a research team in Vienna demonstrated diffraction for molecules as large as fullerenes. The researchers calculated a De Broglie wavelength of the most probable C60 velocity as 2.5 pm.
More recent experiments prove the quantum nature of molecules with a mass up to 6910 amu.
de Broglie relations.
The de Broglie equations relate the wavelength to the momentum , and frequency to the total energy of a particle:
where "h" is Planck's constant. The equations can also be written as
where formula_5 is the reduced Planck's constant, formula_6 is the wave vector, and formula_7 is the angular frequency.
In each pair, the second equation is also referred to as the Planck-Einstein relation, since it was also proposed by Planck and Einstein.
Special relativity.
Using two formulas from special relativity, one for the relativistic momentum and one for the energy
allows the equations to be written as
where formula_11 denotes the particle's rest mass, formula_12 its velocity, formula_13 the Lorentz factor, and formula_14 the speed of light in a vacuum. See below for details of the derivation of the de Broglie relations. Group velocity (equal to the particle's speed) should not be confused with phase velocity (equal to the product of the particle's frequency and its wavelength). In the case of a non-dispersive medium, they happen to be equal, but otherwise they are not.
Group velocity.
Albert Einstein first explained the wave–particle duality of light in 1905. Louis de Broglie hypothesized that any particle should also exhibit such a duality. The velocity of a particle, he concluded, should always equal the group velocity of the corresponding wave. The magnitude of the group velocity is equal to the particle's speed.
Both in relativistic and non-relativistic quantum physics, we can identify the group velocity of a particle's wave function with the particle velocity. Quantum mechanics has very accurately demonstrated this hypothesis, and the relation has been shown explicitly for particles as large as molecules. 
De Broglie deduced that if the duality equations already known for light were the same for any particle, then his hypothesis would hold. This means that
where "E" is the total energy of the particle, "p" is its momentum, "ħ" is the reduced Planck constant. For a free non-relativistic particle it follows that
where "m" is the mass of the particle and "v" its velocity.
Also in special relativity we find that
where formula_11 is the rest mass of the particle and formula_14 is the speed of light in a vacuum. But (see below), using that the phase velocity is formula_20, therefore
where "v" is the velocity of the particle regardless of wave behavior.
Phase velocity.
In quantum mechanics, particles also behave as waves with complex phases. The phase velocity is equal to the product of the frequency multiplied by the wavelength.
By the de Broglie hypothesis, we see that
Using relativistic relations for energy and momentum, we have
where "E" is the total energy of the particle (i.e. rest energy plus kinetic energy in kinematic sense), "p" the momentum, formula_13 the Lorentz factor, "c" the speed of light, and β the speed as a fraction of "c". The variable "v" can either be taken to be the speed of the particle or the group velocity of the corresponding matter wave. Since the particle speed formula_25 for any particle that has mass (according to special relativity), the phase velocity of matter waves always exceeds "c", i.e.
and as we can see, it approaches "c" when the particle speed is in the relativistic range. The superluminal phase velocity does not violate special relativity, because phase propagation carries no energy. See the article on "Dispersion (optics)" for details.
Four-vectors.
Using 4-Vectors, the De Broglie relations form a single equation:
which is frame-independent.
Likewise, the relation between group/particle velocity and phase velocity is given in frame-independent form by:
where
Interpretations.
The physical reality underlying de Broglie waves is a subject of ongoing debate. Some theories treat either the particle or the wave aspect as its fundamental nature, seeking to explain the other as an emergent property. Some, such as the hidden variable theory, treat the wave and the particle as distinct entities. Yet others propose some intermediate entity that is neither quite wave nor quite particle but only appears as such when we measure one or the other property. The Copenhagen interpretation states that the nature of the underlying reality is unknowable and beyond the bounds of scientific enquiry.
Schrödinger's quantum mechanical waves are conceptually different from ordinary physical waves such as water or sound. Ordinary physical waves are characterized by undulating real-number 'displacements' of dimensioned physical variables at each point of ordinary physical space at each instant of time. Schrödinger's "waves" are characterized by the undulating value of a dimensionless complex number at each point of an abstract multi-dimensional space, for example of configuration space.
At the Fifth Solvay Conference in 1927, Max Born and Werner Heisenberg reported as follows
At the same conference, Erwin Schrödinger reported likewise.
In 1955, Heisenberg reiterated this.
It is mentioned above that the "displaced quantity" of the Schrödinger wave has values that are dimensionless complex numbers. One may ask what is the physical meaning of those numbers. According to Heisenberg, rather than being of some ordinary physical quantity such as for example Maxwell's electric field intensity, or for example mass density, the Schrödinger-wave packet's "displaced quantity" is probability amplitude. He wrote that instead of using the term 'wave packet', it is preferable to speak of a probability packet. The probability amplitude supports calculation of probability of location or momentum of discrete particles. Heisenberg recites Duane's account of particle diffraction by probabilistic quantal translation momentum transfer, which allows, for example in Young's two-slit experiment, each diffracted particle probabilistically to pass discretely through a particular slit. Thus one does not need necessarily think of the matter wave, as it were, as 'composed of smeared matter'.
These ideas may be expressed in ordinary language as follows. In the account of ordinary physical waves, a 'point' refers to a position in ordinary physical space at an instant of time, at which there is specified a 'displacement' of some physical quantity. But in the account of quantum mechanics, a 'point' refers to a configuration of the system at an instant of time, every particle of the system being in a sense present in every 'point' of configuration space, each particle at such a 'point' being located possibly at a different position in ordinary physical space. There is no explicit definite indication that, at an instant, this particle is 'here' and that particle is 'there' in some separate 'location' in configuration space. This conceptual difference entails that, in contrast to de Broglie's pre-quantum mechanical wave description, the quantum mechanical probability packet description does not directly and explicitly express the Aristotelian idea, referred to by Newton, that causal efficacy propagates through ordinary space by contact, nor the Einsteinian idea that such propagation is no faster than light. In contrast, these ideas are so expressed in the classical wave account, through the Green's function, though it is inadequate for the observed quantal phenomena. The physical reasoning for this was first recognized by Einstein.
De Broglie's phase wave and periodic phenomenon.
De Broglie's thesis started from the hypothesis, "that to each portion of energy with a proper mass one may associate a periodic phenomenon of the frequency , such that one finds: . The frequency is to be measured, of course, in the rest frame of the energy packet. This hypothesis is the basis of our theory."
De Broglie followed his initial hypothesis of a periodic phenomenon, with frequency  , associated with the energy packet. He used the special theory of relativity to find, in the frame of the observer of the electron energy packet that is moving with velocity formula_12, that its frequency was apparently reduced to
Then
using the same notation as above. The quantity formula_33 is the velocity of what de Broglie called the "phase wave". Its wavelength is formula_34 and frequency formula_35. De Broglie reasoned that his hypothetical intrinsic particle periodic phenomenon is in phase with that phase wave. This was his basic matter wave conception. He noted, as above, that formula_36, and the phase wave does not transfer energy.
While the concept of waves being associated with matter is correct, de Broglie did not leap directly to the final understanding of quantum mechanics with no missteps. There are conceptual problems with the approach that de Broglie took in his thesis that he was not able to resolve, despite trying a number of different fundamental hypotheses in different papers published while working on, and shortly after publishing, his thesis.
These difficulties were resolved by Erwin Schrödinger, who developed the wave mechanics approach, starting from a somewhat different basic hypothesis.

</doc>
<doc id="298420" url="https://en.wikipedia.org/wiki?curid=298420" title="Maxima and minima">
Maxima and minima

In mathematical analysis, the maxima and minima (the plural of maximum and minimum) of a function, known collectively as extrema (the plural of extremum), are the largest and smallest value of the function, either within a given range (the local or relative extrema) or on the entire domain of a function (the global or absolute extrema). Pierre de Fermat was one of the first mathematicians to propose a general technique, adequality, for finding the maxima and minima of functions.
As defined in set theory, the maximum and minimum of a set are the greatest and least elements in the set, respectively. Unbounded infinite sets, such as the set of real numbers, have no minimum or maximum.
Definition.
A real-valued function "f" defined on a domain "X" has a global (or absolute) maximum point at "x"∗ if "f"("x"∗) ≥ "f"("x") for all "x" in "X". Similarly, the function has a global (or absolute) minimum point at "x"∗ if "f"("x"∗) ≤ "f"("x") for all "x" in "X". The value of the function at a maximum point is called the maximum value of the function and the value of the function at a minimum point is called the minimum value of the function.
If the domain "X" is a metric space then "f" is said to have a local (or relative) maximum point at the point "x"∗ if there exists some "ε" > 0 such that "f"("x"∗) ≥ "f"("x") for all "x" in "X" within distance "ε" of "x"∗. Similarly, the function has a local minimum point at "x"∗ if "f"("x"∗) ≤ "f"("x") for all "x" in "X" within distance "ε" of "x"∗. A similar definition can be used when "X" is a topological space, since the definition just given can be rephrased in terms of neighbourhoods. Note that a global maximum point is always a local maximum point, and similarly for minimum points.
In both the global and local cases, the concept of a strict extremum can be defined. For example, "x"∗ is a strict global maximum point if, for all "x" in "X" with "x" ≠ "x"∗, we have "f"("x"∗) > "f"("x"), and "x"∗ is a strict local maximum point if there exists some "ε" > 0 such that, for all "x" in "X" within distance "ε" of "x"∗ with "x" ≠ "x"∗, we have "f"("x"∗) > "f"("x"). Note that a point is a strict global maximum point if and only if it is the unique global maximum point, and similarly for minimum points.
A continuous real-valued function with a compact domain always has a maximum point and a minimum point. An important example is a function whose domain is a closed (and bounded) interval of real numbers (see the graph above).
Finding functional maxima and minima.
Finding global maxima and minima is the goal of mathematical optimization. If a function is continuous on a closed interval, then by the extreme value theorem global maxima and minima exist. Furthermore, a global maximum (or minimum) either must be a local maximum (or minimum) in the interior of the domain, or must lie on the boundary of the domain. So a method of finding a global maximum (or minimum) is to look at all the local maxima (or minima) in the interior, and also look at the maxima (or minima) of the points on the boundary, and take the largest (or smallest) one.
Local extrema of differentiable functions can be found by Fermat's theorem, which states that they must occur at critical points. One can distinguish whether a critical point is a local maximum or local minimum by using the first derivative test, second derivative test, or higher-order derivative test, given sufficient differentiability.
For any function that is defined piecewise, one finds a maximum (or minimum) by finding the maximum (or minimum) of each piece separately, and then seeing which one is largest (or smallest).
Functions of more than one variable.
For functions of more than one variable, similar conditions apply. For example, in the (enlargeable) figure at the right, the necessary conditions for a "local" maximum are similar to those of a function with only one variable. The first partial derivatives as to "z" (the variable to be maximized) are zero at the maximum (the glowing dot on top in the figure). The second partial derivatives are negative. These are only necessary, not sufficient, conditions for a local maximum because of the possibility of a saddle point. For use of these conditions to solve for a maximum, the function "z" must also be differentiable throughout. The second partial derivative test can help classify the point as a relative maximum or relative minimum.
In contrast, there are substantial differences between functions of one variable and functions of more than one variable in the identification of global extrema. For example, if a bounded differentiable function "f" defined on a closed interval in the real line has a single critical point, which is a local minimum, then it is also a global minimum (use the intermediate value theorem and Rolle's theorem to prove this by reductio ad absurdum). In two and more dimensions, this argument fails, as the function
shows. Its only critical point is at (0,0), which is a local minimum with ƒ(0,0) = 0. However, it cannot be a global one, because ƒ(2,3) = −5.
Maxima or minima of a functional.
If the domain of a function for which an extremum is to be found consists itself of functions, i.e. if an extremum is to be found of a functional, the extremum is found using the calculus of variations.
In relation to sets.
Maxima and minima can also be defined for sets. In general, if an ordered set "S" has a greatest element "m", "m" is a maximal element. Furthermore, if "S" is a subset of an ordered set "T" and "m" is the greatest element of "S" with respect to order induced by "T", "m" is a least upper bound of "S" in "T". The similar result holds for least element, minimal element and greatest lower bound.
In the case of a general partial order, the least element (smaller than all other) should not be confused with a minimal element (nothing is smaller). Likewise, a greatest element of a partially ordered set (poset) is an upper bound of the set which is contained within the set, whereas a maximal element "m" of a poset "A" is an element of "A" such that if "m" ≤ "b" (for any "b" in "A") then "m" = "b". Any least element or greatest element of a poset is unique, but a poset can have several minimal or maximal elements. If a poset has more than one maximal element, then these elements will not be mutually comparable.
In a totally ordered set, or "chain", all elements are mutually comparable, so such a set can have at most one minimal element and at most one maximal element. Then, due to mutual comparability, the minimal element will also be the least element and the maximal element will also be the greatest element. Thus in a totally ordered set we can simply use the terms minimum and maximum. If a chain is finite then it will always have a maximum and a minimum. If a chain is infinite then it need not have a maximum or a minimum. For example, the set of natural numbers has no maximum, though it has a minimum. If an infinite chain "S" is bounded, then the closure "Cl(S)" of the set occasionally has a minimum and a maximum, in such case they are called the greatest lower bound and the least upper bound of the set "S", respectively.

</doc>
<doc id="921525" url="https://en.wikipedia.org/wiki?curid=921525" title="Maxwell relations">
Maxwell relations

Maxwell's relations are a set of equations in thermodynamics which are derivable from the symmetry of second derivatives and from the definitions of the thermodynamic potentials. These relations are named for the nineteenth-century physicist James Clerk Maxwell.
Equation.
The structure of Maxwell relations is a statement of equality among the second derivatives for continuous function. It follows directly from the fact that the order of differentiation of an analytic function of two variables is irrelevant (Schwarz theorem). In the case of Maxwell relations the function considered is a thermodynamic potential and "xi" and "xj" are two different natural variables for that potential:
where the partial derivatives are taken with all other natural variables held constant. It is seen that for every thermodynamic potential there are "n"("n" − 1)/2 possible Maxwell relations where "n" is the number of natural variables for that potential.
The four most common Maxwell relations.
The four most common Maxwell relations are the equalities of the second derivatives of each of the four thermodynamic potentials, with respect to their thermal natural variable (temperature "T"; or entropy "S") and their "mechanical" natural variable (pressure "P"; or volume "V"):
where the potentials as functions of their natural thermal and mechanical variables are the internal energy "U"("S, V"), enthalpy "H"("S, P"), Helmholtz free energy "F"("T, V") and Gibbs free energy "G"("T, P"). The thermodynamic square can be used as a mnemonic to recall and derive these relations.
Derivation.
Maxwell relations are based on simple partial differentiation rules, in particular the total differential of a function and the symmetry of evaluating second order partial derivatives.
General Maxwell relationships.
The above are not the only Maxwell relationships. When other work terms involving other natural variables besides the volume work are considered or when the number of particles is included as a natural variable, other Maxwell relations become apparent. For example, if we have a single-component gas, then the number of particles "N"  is also a natural variable of the above four thermodynamic potentials. The Maxwell relationship for the enthalpy with respect to pressure and particle number would then be:
where μ is the chemical potential. In addition, there are other thermodynamic potentials besides the four that are commonly used, and each of these potentials will yield a set of Maxwell relations.
Each equation can be re-expressed using the relationship
which are sometimes also known as Maxwell relations.

</doc>
<doc id="19737" url="https://en.wikipedia.org/wiki?curid=19737" title="Maxwell's equations">
Maxwell's equations

Maxwell's equations are a set of partial differential equations that, together with the Lorentz force law, form the foundation of classical electrodynamics, classical optics, and electric circuits. These fields in turn underlie modern electrical and communications technologies. Maxwell's equations describe how electric and magnetic fields are generated and altered by each other and by charges and currents. They are named after the physicist and mathematician James Clerk Maxwell, who published an early form of those equations between 1861 and 1862.
The equations have two major variants. The "microscopic" set of Maxwell's equations uses total charge and total current, including the complicated charges and currents in materials at the atomic scale; it has universal applicability but may be infeasible to calculate. The "macroscopic" set of Maxwell's equations defines two new auxiliary fields that describe large-scale behaviour without having to consider these atomic scale details, but it requires the use of parameters characterizing the electromagnetic properties of the relevant materials.
The term "Maxwell's equations" is often used for other forms of Maxwell's equations. For example, space-time formulations are commonly used in high energy and gravitational physics. These formulations, defined on space-time rather than space and time separately, are manifestly compatible with special and general relativity. In quantum mechanics and analytical mechanics, versions of Maxwell's equations based on the electric and magnetic potentials are preferred.
Since the mid-20th century, it has been understood that Maxwell's equations are not exact but are a classical field theory approximation to the more accurate and fundamental theory of quantum electrodynamics. In many situations, though, deviations from Maxwell's equations are immeasurably small. Exceptions include nonclassical light, photon-photon scattering, quantum optics, and many other phenomena related to photons or virtual photons.
Formulation in terms of electric and magnetic fields.
The powerful and most widely familiar form of Maxwell's equations, whose formulation is due to Oliver Heaviside, in the vector calculus formalism, is used throughout unless otherwise explicitly stated.
Symbols in bold represent vector quantities, and symbols in "italics" represent scalar quantities, unless otherwise indicated.
The equations introduce the electric field, , a vector field, and the magnetic field, , a pseudovector field, where each generally have time-dependence. The sources of these fields are electric charges and electric currents, which can be expressed as local densities namely charge density and current density . A separate law of nature, the Lorentz force law, describes how the electric and magnetic field act on charged particles and currents. A version of this law was included in the original equations by Maxwell but, by convention, is no longer.
In the electric and magnetic field formulation there are four equations. Two of them describe how the fields vary in space due to sources, if any; electric fields emanating from electric charges in Gauss's law, and magnetic fields as closed field lines "not due to magnetic monopoles" in Gauss's law for magnetism. The other two describe how the fields "circulate" around their respective sources; the magnetic field "circulates" around electric currents and time varying electric fields in Ampère's law with Maxwell's addition, while the electric field "circulates" around time varying magnetic fields in Faraday's law.
The precise formulation of Maxwell's equations depends on the precise definition of the quantities involved. Conventions differ with the unit systems, because various definitions and dimensions are changed by absorbing dimensionful factors like the speed of light . This makes constants come out differently.
Conventional formulation in SI units.
The equations in this section are given in the convention used with SI units. Other units commonly used are Gaussian units based on the cgs system, Lorentz–Heaviside units (used mainly in particle physics), and Planck units (used in theoretical physics). See below for the formulation with Gaussian units.
where the universal constants appearing in the equations are 
In the differential equations, a "local" description of the fields, 
The sources are taken to be 
In the integral equations, a description of the fields within a region of space, 
Here "fixed" means the volume or surface do not change in time. Although it is possible to formulate Maxwell's equations with time-dependent surfaces and volumes, this is not actually necessary: the equations are correct and complete with time-independent surfaces. The sources are correspondingly the total amounts of charge and current within these volumes and surfaces, found by integration.
The "total charge or current" refers to including free and bound charges, or free and bound currents. These are used in the macroscopic formulation.
Relationship between differential and integral formulations.
The differential and integral formulations of the equations are mathematically equivalent, by the divergence theorem in the case of Gauss's law and Gauss's law for magnetism, and by the Kelvin–Stokes theorem in the case of Faraday's law and Ampère's law. Both the differential and integral formulations are useful. The integral formulation can often be used to simplify and directly calculate fields from symmetric distributions of charges and currents. On the other hand, the differential formulation is a more natural starting point for calculating the fields in more complicated (less symmetric) situations, for example using finite element analysis.
Flux and divergence.
The "fields emanating from the sources" can be inferred from the surface integrals of the fields through the closed surface , defined as the electric flux and magnetic flux , as well as their respective divergences and . These surface integrals and divergences are connected by the divergence theorem.
Circulation and curl.
The "circulation of the fields" can be interpreted from the line integrals of the fields around the closed curve ∂Σ:
where is the differential vector element of "path length" tangential to the path/curve, as well as their curls:
These line integrals and curls are connected by Stokes' theorem, and are analogous to quantities in classical fluid dynamics: the circulation of a fluid is the line integral of the fluid's flow velocity field around a closed loop, and the vorticity of the fluid is the curl of the velocity field.
Time evolution.
The "dynamics" or "time evolution of the fields" is due to the partial derivatives of the fields with respect to time:
These derivatives are crucial for the prediction of field propagation in the form of electromagnetic waves. Since the surface is taken to be time-independent, we can make the following transition in Faraday's law:
see differentiation under the integral sign for more on this result.
Conceptual descriptions.
Gauss's law.
Gauss's law describes the relationship between a static electric field and the electric charges that cause it: The static electric field points away from positive charges and towards negative charges. In the field line description, electric field lines begin only at positive electric charges and end only at negative electric charges. 'Counting' the number of field lines passing through a closed surface, therefore, yields the total charge (including bound charge due to polarization of material) enclosed by that surface divided by dielectricity of free space (the vacuum permittivity). More technically, it relates the electric flux through any hypothetical closed "Gaussian surface" to the enclosed electric charge.
Gauss's law for magnetism.
Gauss's law for magnetism states that there are no "magnetic charges" (also called magnetic monopoles), analogous to electric charges. Instead, the magnetic field due to materials is generated by a configuration called a dipole. Magnetic dipoles are best represented as loops of current but resemble positive and negative 'magnetic charges', inseparably bound together, having no net 'magnetic charge'. In terms of field lines, this equation states that magnetic field lines neither begin nor end but make loops or extend to infinity and back. In other words, any magnetic field line that enters a given volume must somewhere exit that volume. Equivalent technical statements are that the sum total magnetic flux through any Gaussian surface is zero, or that the magnetic field is a solenoidal vector field.
Faraday's law.
The Maxwell-Faraday's equation version of Faraday's law describes how a time varying magnetic field creates ("induces") an electric field. This dynamically induced electric field has closed field lines just as the magnetic field, if not superposed by a static (charge induced) electric field. This aspect of electromagnetic induction is the operating principle behind many electric generators: for example, a rotating bar magnet creates a changing magnetic field, which in turn generates an electric field in a nearby wire.
Ampère's law with Maxwell's addition.
Ampère's law with Maxwell's addition states that magnetic fields can be generated in two ways: by electric current (this was the original "Ampère's law") and by changing electric fields (this was "Maxwell's addition").
Maxwell's addition to Ampère's law is particularly important: it shows that not only does a changing magnetic field induce an electric field, but also a changing electric field induces a magnetic field. Therefore, these equations allow self-sustaining "electromagnetic waves" to travel through empty space (see electromagnetic wave equation).
The speed calculated for electromagnetic waves, which could be predicted from experiments on charges and currents, exactly matches the speed of light; indeed, light "is" one form of electromagnetic radiation (as are X-rays, radio waves, and others). Maxwell understood the connection between electromagnetic waves and light in 1861, thereby unifying the theories of electromagnetism and optics.
Vacuum equations, electromagnetic waves and speed of light.
In a region with no charges () and no currents (), such as in a vacuum, Maxwell's equations reduce to:
Taking the curl of the curl equations, and using the curl of the curl identity we obtain the wave equations
which identify
with the speed of light in free space. In materials with relative permittivity, , and relative permeability, , the phase velocity of light becomes
which is usually less than .
In addition, and are mutually perpendicular to each other and the direction of wave propagation, and are in phase with each other. A sinusoidal plane wave is one special solution of these equations. Maxwell's equations explain how these waves can physically propagate through space. The changing magnetic field creates a changing electric field through Faraday's law. In turn, that electric field creates a changing magnetic field through Maxwell's addition to Ampère's law. This perpetual cycle allows these waves, now known as electromagnetic radiation, to move through space at velocity, .
"Microscopic" versus "macroscopic".
The "microscopic" variant of Maxwell's equation is the version given above. It expresses the electric field and the magnetic field in terms of the "total charge" and "total current" present, including the charges and currents at the atomic level. The "microscopic" form is sometimes called the "general" form of Maxwell's equations. The macroscopic variant of Maxwell's equation is equally general, however, with the difference being one of bookkeeping.
The "microscopic" variant is sometimes called "Maxwell's equations in a vacuum". This refers to the fact that the material medium is not built into the structure of the equation; it does not mean that space is empty of charge or current.
"Maxwell's macroscopic equations", also known as Maxwell's equations in matter, are more similar to those that Maxwell introduced himself.
Unlike the "microscopic" equations, the "macroscopic" equations separate out the bound charge and bound current to obtain equations that depend only on the free charges and currents . This factorization can be made by splitting the total electric charge and current as follows:
Correspondingly, the total current density splits into free and bound components, and similarly the total charge density splits into free and bound parts.
The cost of this factorization is that additional fields, the displacement field and the magnetizing field , are defined and need to be determined. Phenomenological constituent equations relate the additional fields to the electric field and the magnetic -field, often through a simple linear relation.
For a detailed description of the differences between the microscopic ("total" charge and current including material contributes or in air/vacuum)
and macroscopic ("free" charge and current; practical to use on materials) variants of Maxwell's equations, see below.
Bound charge and current.
When an electric field is applied to a dielectric material its molecules respond by forming microscopic electric dipoles – their atomic nuclei move a tiny distance in the direction of the field, while their electrons move a tiny distance in the opposite direction. This produces a "macroscopic" "bound charge" in the material even though all of the charges involved are bound to individual molecules. For example, if every molecule responds the same, similar to that shown in the figure, these tiny movements of charge combine to produce a layer of positive bound charge on one side of the material and a layer of negative charge on the other side. The bound charge is most conveniently described in terms of the polarization of the material, its dipole moment per unit volume. If is uniform, a macroscopic separation of charge is produced only at the surfaces where enters and leaves the material. For non-uniform , a charge is also produced in the bulk.
Somewhat similarly, in all materials the constituent atoms exhibit magnetic moments that are intrinsically linked to the angular momentum of the components of the atoms, most notably their electrons. The connection to angular momentum suggests the picture of an assembly of microscopic current loops. Outside the material, an assembly of such microscopic current loops is not different from a macroscopic current circulating around the material's surface, despite the fact that no individual charge is traveling a large distance. These "bound currents" can be described using the magnetization .
The very complicated and granular bound charges and bound currents, therefore, can be represented on the macroscopic scale in terms of and which average these charges and currents on a sufficiently large scale so as not to see the granularity of individual atoms, but also sufficiently small that they vary with location in the material. As such, "Maxwell's macroscopic equations" ignore many details on a fine scale that can be unimportant to understanding matters on a gross scale by calculating fields that are averaged over some suitable volume.
Auxiliary fields, polarization and magnetization.
The "definitions" (not constitutive relations) of the auxiliary fields are:
where is the polarization field and is the magnetization field which are defined in terms of microscopic bound charges and bound currents respectively. The macroscopic bound charge density and bound current density in terms of polarization and magnetization are then defined as
If we define the total, bound, and free charge and current density by
and use the defining relations above to eliminate , and , the "macroscopic" Maxwell's equations reproduce the "microscopic" equations.
Constitutive relations.
In order to apply 'Maxwell's macroscopic equations', it is necessary to specify the relations between displacement field and the electric field , as well as the magnetizing field and the magnetic field . Equivalently, we have to specify the dependence of the polarisation (hence the bound charge) and the magnetisation (hence the bound current) on the applied electric and magnetic field. The equations specifying this response are called constitutive relations. For real-world materials, the constitutive relations are rarely simple, except approximately, and usually determined by experiment. See the main article on constitutive relations for a fuller description.
For materials without polarisation and magnetisation ("vacuum"), the constitutive relations are
where is the permittivity of free space and the permeability of free space. Since there is no bound charge, the total and the free charge and current are equal.
More generally, for linear materials the constitutive relations are
where is the permittivity and the permeability of the material. Even the linear case can have various complications, however.
Even more generally, in the case of non-linear materials (see for example nonlinear optics), and are not necessarily proportional to , similarly is not necessarily proportional to or . In general and depend on both and , on location and time, and possibly other physical quantities.
In applications one also has to describe how the free currents and charge density behave in terms of and possibly coupled to other physical quantities like pressure, and the mass, number density, and velocity of charge-carrying particles. E.g., the original equations given by Maxwell (see History of Maxwell's equations) included Ohms law in the form
Equations in Gaussian units.
Gaussian units are a popular system of units, that is part of the centimetre–gram–second system of units (cgs). When using cgs units it is conventional to use a slightly different definition of electric field . This implies that the modified electric and magnetic field have the same units (in the SI convention this is not the case: e.g. for EM waves in vacuum, , making dimensional analysis of the equations different). Then it uses a unit of charge defined in such a way that the permittivity of the vacuum , hence .
Using these different conventions, the Maxwell equations become:
Alternative formulations.
Following is a summary of some of the numerous other ways to write the microscopic Maxwell's equations, showing they can be formulated using different points of view and mathematical formalisms that describe the same physics. Often, they are also called the Maxwell equations. The direct space–time formulations make manifest that the Maxwell equations are relativistically invariant (in fact studying the hidden symmetry of the vector calculus formulation was a major source of inspiration for relativity theory). In addition, the formulation using potentials was originally introduced as a convenient way to solve the equations but with all the observable physics contained in the fields. The potentials play a central role in quantum mechanics, however, and act quantum mechanically with observable consequences even when the fields vanish (Aharonov–Bohm effect). See the main articles for the details of each formulation. SI units are used throughout.
where
Other formulations include the geometric algebra formulation and a matrix representation of Maxwell's equations. Historically, a quaternionic formulation was used.
Solutions.
Maxwell's equations are partial differential equations that relate the electric and magnetic fields to each other and to the electric charges and currents. Often, the charges and currents are themselves dependent on the electric and magnetic fields via the Lorentz force equation and the constitutive relations. These all form a set of coupled partial differential equations, which are often very difficult to solve. In fact, the solutions of these equations encompass all the diverse phenomena in the entire field of classical electromagnetism. A thorough discussion is far beyond the scope of the article, but some general notes follow.
Like any differential equation, boundary conditions and initial conditions are necessary for a unique solution. For example, even with no charges and no currents anywhere in spacetime, many solutions to Maxwell's equations are possible, not just the obvious solution . Another solution is , , while yet other solutions have electromagnetic waves filling spacetime. In some cases, Maxwell's equations are solved through infinite space, and boundary conditions are given as asymptotic limits at infinity. In other cases, Maxwell's equations are solved in just a finite region of space, with appropriate boundary conditions on that region: For example, the boundary could be an artificial absorbing boundary representing the rest of the universe, or periodic boundary conditions, or (as with a waveguide or cavity resonator) the boundary conditions may describe the walls that isolate a small region from the outside world.
Jefimenko's equations (or the closely related Liénard–Wiechert potentials) are the explicit solution to Maxwell's equations for the electric and magnetic fields created by any given distribution of charges and currents. It assumes specific initial conditions to obtain the so-called "retarded solution", where the only fields present are the ones created by the charges. Jefimenko's equations are not so helpful in situations when the charges and currents are themselves affected by the fields they create.
Numerical methods for differential equations can be used to approximately solve Maxwell's equations when an exact solution is impossible. These methods usually require a computer, and include the finite element method and finite-difference time-domain method. For more details, see Computational electromagnetics.
Maxwell's equations "seem" overdetermined, in that they involve six unknowns (the three components of and ) but eight equations (one for each of the two Gauss's laws, three vector components each for Faraday's and Ampere's laws). (The currents and charges are not unknowns, being freely specifiable subject to charge conservation.) This is related to a certain limited kind of redundancy in Maxwell's equations: It can be proven that any system satisfying Faraday's law and Ampere's law "automatically" also satisfies the two Gauss's laws, as long as the system's initial condition does. Although it is possible to simply ignore the two Gauss's laws in a numerical algorithm (apart from the initial conditions), the imperfect precision of the calculations can lead to ever-increasing violations of those laws. By introducing dummy variables characterizing these violations, the four equations become not overdetermined after all. The resulting formulation can lead to more accurate algorithms that take all four laws into account.
Limitations for a theory of electromagnetism.
While Maxwell's equations (along with the rest of classical electromagnetism) are extraordinarily successful at explaining and predicting a variety of phenomena, they are not exact, but approximations. In some special situations, they can be noticeably inaccurate. Examples include extremely strong fields (see Euler–Heisenberg Lagrangian) and extremely short distances (see vacuum polarization). Moreover, various phenomena occur in the world even though Maxwell's equations predict them to be impossible, such as "nonclassical light" and quantum entanglement of electromagnetic fields (see quantum optics). Finally, any phenomenon involving individual photons, such as the photoelectric effect, Planck's law, the Duane–Hunt law, single-photon light detectors, etc., would be difficult or impossible to explain if Maxwell's equations were exactly true, as Maxwell's equations do not involve photons. For the most accurate predictions in all situations, Maxwell's equations have been superseded by quantum electrodynamics.
Variations.
Popular variations on the Maxwell equations as a classical theory of electromagnetic fields are relatively scarce because the standard equations have stood the test of time remarkably well.
Magnetic monopoles.
Maxwell's equations posit that there is electric charge, but no magnetic charge (also called magnetic monopoles), in the universe. Indeed, magnetic charge has never been observed (despite extensive searches) and may not exist. If they did exist, both Gauss's law for magnetism and Faraday's law would need to be modified, and the resulting four equations would be fully symmetric under the interchange of electric and magnetic fields.
Historical publications.
The developments before relativity:

</doc>
<doc id="19830" url="https://en.wikipedia.org/wiki?curid=19830" title="Maxwell–Boltzmann distribution">
Maxwell–Boltzmann distribution

In statistics the Maxwell–Boltzmann distribution is a particular probability distribution named after James Clerk Maxwell and Ludwig Boltzmann. 
It was first defined and used in physics (in particular in statistical mechanics) for describing particle speeds in idealized gases where the particles move freely inside a stationary container without interacting with one another, except for very brief collisions in which they exchange energy and momentum with each other or with their thermal environment. Particle in this context refers to gaseous particles (atoms or molecules), and the system of particles is assumed to have reached thermodynamic equilibrium.
While the distribution was first derived by Maxwell in 1860 on heuristic grounds, Boltzmann later carried out significant investigations into the physical origins of this distribution.
A particle speed probability distribution indicates which speeds are more likely: a particle will have a speed selected randomly from the distribution, and is more likely to be within one range of speeds than another. The distribution depends on the temperature of the system and the mass of the particle.
The Maxwell–Boltzmann distribution applies to the classical ideal gas, which is an idealization of real gases. In real gases, there are various effects (e.g., van der Waals interactions, vortical flow, relativistic speed limits, and quantum exchange interactions) that make their speed distribution sometimes very different from the Maxwell–Boltzmann form. However, rarefied gases at ordinary temperatures behave very nearly like an ideal gas and the Maxwell speed distribution is an excellent approximation for such gases. Thus, it forms the basis of the kinetic theory of gases, which provides a simplified explanation of many fundamental gaseous properties, including pressure and diffusion.
Distribution function.
The Maxwell–Boltzmann distribution is the function
where formula_9 is the particle mass and formula_10 is the product of Boltzmann's constant and thermodynamic temperature.
This probability density function gives the probability, per unit speed, of finding the particle with a speed near formula_11. This equation is simply the Maxwell distribution (given in the infobox) with distribution parameter formula_12. In probability theory the Maxwell–Boltzmann distribution is a chi distribution with three degrees of freedom and scale parameter formula_12.
The simplest ordinary differential equation satisfied by the distribution is:
or in unitless presentation:
Typical speeds.
The mean speed, most probable speed (mode), and root-mean-square can be obtained from properties of the Maxwell distribution.
= \sqrt { \frac{8RT}{\pi M}} = \frac{2}{\sqrt{\pi}} v_p </math>
The typical speeds are related as follows:
Derivation and related distributions.
The original derivation in 1860 by James Clerk Maxwell was an argument based on demanding certain symmetries in the speed distribution function. After Maxwell, Ludwig Boltzmann in 1872 derived the distribution on more mechanical grounds by using the assumptions of his kinetic theory, and showed that gases should over time tend toward this distribution, due to collisions (see H-theorem). He later (1877) derived the distribution again under the framework of statistical thermodynamics. The derivations in this section are along the lines of Boltzmann's 1877 derivation, starting with result known as Maxwell–Boltzmann statistics (from statistical thermodynamics). Maxwell–Boltzmann statistics gives the average number of particles found in a given single-particle microstate, under certain assumptions:
where:
The assumptions of this equation are that the particles do not interact, and that they are classical; this means that each particle's state can be considered independently from the other particles' states. Additionally, the particles are assumed to be in thermal equilibrium. The denominator in Equation () is simply a normalizing factor so that the "N""i"/"N" add up to 1 — in other words it is a kind of partition function (for the single-particle system, not the usual partition function of the entire system).
Because velocity and speed are related to energy, Equation () can be used to derive relationships between temperature and the speeds of gas particles. All that is needed is to discover the density of microstates in energy, which is determined by dividing up momentum space into equal sized regions.
Distribution for the momentum vector.
The potential energy is taken to be zero, so that all energy is in the form of kinetic energy.
The relationship between kinetic energy and momentum for massive non-relativistic particles is
where "p"2 is the square of the momentum vector 
p = ["p""x", "p""y", "p""z"]. We may therefore rewrite Equation () as:
where "Z" is the partition function, corresponding to the denominator in Equation (). Here "m" is the molecular mass of the gas, "T" is the thermodynamic temperature and "k" is the Boltzmann constant. This distribution of "N"i/"N" is proportional to the probability density function "f"p for finding a molecule with these values of momentum components, so:
The normalizing constant "c", can be determined by recognizing that the probability of a molecule having "some" momentum must be 1. Therefore the integral of equation () over all "p""x", "p""y", and "p""z" must be 1. 
It can be shown that:
Substituting Equation () into Equation () gives:
The distribution is seen to be the product of three independent normally distributed variables formula_20, formula_21, and formula_22, with variance formula_23. Additionally, it can be seen that the magnitude of momentum will be distributed as a Maxwell–Boltzmann distribution, with formula_24.
The Maxwell–Boltzmann distribution for the momentum (or equally for the velocities) can be obtained more fundamentally using the H-theorem at equilibrium within the kinetic theory framework.
Distribution for the energy.
The energy distribution is found imposing
where formula_25 is the infinitesimal phase-space volume of momenta corresponding to the energy interval formula_26.
Making use of the spherical symmetry of the energy-momentum dispersion relation formula_27,
this can be expressed in terms of formula_26 as
Using then () in (), and expressing everything in terms of the energy formula_29, we get
and finally
 \left( \frac{1}{kT} \right)^{3/2} \exp\left(\frac{-E}{kT} \right)</math>
Since the energy is proportional to the sum of the squares of the three normally distributed momentum components, this distribution is a gamma distribution; in particular, it is a chi-squared distribution with three degrees of freedom.
By the equipartition theorem, this energy is evenly distributed among all three degrees of freedom, so that the energy per degree of freedom is distributed as a chi-squared distribution with one degree of freedom:
where formula_32 is the energy per degree of freedom. At equilibrium, this distribution will hold true for any number of degrees of freedom. For example, if the particles are rigid mass dipoles of fixed dipole moment, they will have three translational degrees of freedom and two additional rotational degrees of freedom. The energy in each degree of freedom will be described according to the above chi-squared distribution with one degree of freedom, and the total energy will be distributed according to a chi-squared distribution with five degrees of freedom. This has implications in the theory of the specific heat of a gas.
The Maxwell–Boltzmann distribution can also be obtained by considering the gas to be a type of quantum gas.
Distribution for the velocity vector.
Recognizing that the velocity probability density "f"v is proportional to the momentum probability density function by
and using p = mv we get
which is the Maxwell–Boltzmann velocity distribution. The probability of finding a particle with velocity in the infinitesimal element ["dv""x", "dv""y", "dv""z"] about velocity v = ["v""x", "v""y", "v""z"] is
Like the momentum, this distribution is seen to be the product of three independent normally distributed variables formula_35, formula_36, and formula_37, but with variance formula_38. It can also be seen that the Maxwell–Boltzmann velocity distribution for the vector velocity
["v""x", "v""y", "v""z"] is the product of the distributions for each of the three directions:
where the distribution for a single direction is
Each component of the velocity vector has a normal distribution with mean formula_41 and standard deviation formula_42, so the vector has a 3-dimensional normal distribution, a particular kind of multivariate normal distribution, with mean formula_43 and standard deviation formula_44.
The Maxwell–Boltzmann distribution for the speed follows immediately from the distribution of the velocity vector, above. Note that the speed is
and the volume element in spherical coordinates
where formula_47 and formula_48 are the "course" (azimuth of the velocity vector) and "path angle" (elevation angle of the velocity vector). Integration of the normal probability density function of the velocity, above, over the course (from 0 to formula_49) and path angle (from 0 to formula_50), with substitution of the speed for the sum of the squares of the vector components, yields the speed distribution.

</doc>
<doc id="19192" url="https://en.wikipedia.org/wiki?curid=19192" title="Mean">
Mean

In mathematics, mean has several different definitions depending on the context.
In probability and statistics, mean and expected value are used synonymously to refer to one measure of the central tendency either of a probability distribution or of the random variable characterized by that distribution. In the case of a discrete probability distribution of a random variable "X", the mean is equal to the sum over every possible value weighted by the probability of that value; that is, it is computed by taking the product of each possible value "x" of "X" and its probability P("x"), and then adding all these products together, giving formula_1. An analogous formula applies to the case of a continuous probability distribution. Not every probability distribution has a defined mean; see the Cauchy distribution for an example. Moreover, for some distributions the mean is infinite: for example, when the probability of the value formula_2 is formula_3 for n = 1, 2, 3, ...
For a data set, the terms arithmetic mean, mathematical expectation, and sometimes average are used synonymously to refer to a central value of a discrete set of numbers: specifically, the sum of the values divided by the number of values. The arithmetic mean of a set of numbers "x"1, "x"2, ..., "xn" is typically denoted by formula_4, pronounced ""x" bar". If the data set were based on a series of observations obtained by sampling from a statistical population, the arithmetic mean is termed the sample mean (denoted formula_4) to distinguish it from the population mean (denoted formula_6 or formula_7).
For a finite population, the population mean of a property is equal to the arithmetic mean of the given property while considering every member of the population. For example, the population mean height is equal to the sum of the heights of every individual divided by the total number of individuals. The sample mean may differ from the population mean, especially for small samples. The law of large numbers dictates that the larger the size of the sample, the more likely it is that the sample mean will be close to the population mean.
Outside of probability and statistics, a wide range of other notions of "mean" are often used in geometry and analysis; examples are given below.
Types of mean.
Pythagorean means.
Arithmetic mean (AM).
The "arithmetic mean" (or simply "mean") of a sample formula_8, usually denoted by formula_4, is the sum of the sampled values divided by the number of items in the sample:
For example, the arithmetic mean of five values: 4, 36, 45, 50, 75 is
Geometric mean (GM).
The geometric mean is an average that is useful for sets of positive numbers that are interpreted according to their product and not their sum (as is the case with the arithmetic mean) e.g. rates of growth.
For example, the geometric mean of five values: 4, 36, 45, 50, 75 is:
Harmonic mean (HM).
The harmonic mean is an average which is useful for sets of numbers which are defined in relation to some unit, for example speed (distance per unit of time).
For example, the harmonic mean of the five values: 4, 36, 45, 50, 75 is
Relationship between AM, GM, and HM.
AM, GM, and HM satisfy these inequalities:
Equality holds only when all the elements of the given sample are equal.
Statistical location.
In descriptive statistics, the mean may be confused with the median, mode or mid-range, as any of these may be called an "average" (more formally, a measure of central tendency). The mean of a set of observations is the arithmetic average of the values; however, for skewed distributions, the mean is not necessarily the same as the middle value (median), or the most likely value (mode). For example, mean income is typically skewed upwards by a small number of people with very large incomes, so that the majority have an income lower than the mean. By contrast, the median income is the level at which half the population is below and half is above. The mode income is the most likely income, and favors the larger number of people with lower incomes. While the median and mode are often more intuitive measures for such skewed data, many skewed distributions are in fact best described by their mean, including the exponential and Poisson distributions.
Mean of a probability distribution.
The mean of a probability distribution is the long-run arithmetic average value of a random variable having that distribution. In this context, it is also known as the expected value. For a discrete probability distribution, the mean is given by formula_17, where the sum is taken over all possible values of the random variable and formula_18 is the probability mass function. For a continuous distribution, the mean is formula_19, where formula_20 is the probability density function. In all cases, including those in which the distribution is neither discrete nor continuous, the mean is the Lebesgue integral of the random variable with respect to its probability measure. The mean need not exist or be finite; for some probability distributions the mean is infinite ( or ), while others have no mean.
Generalized means.
Power mean.
The generalized mean, also known as the power mean or Hölder mean, is an abstraction of the quadratic, arithmetic, geometric and harmonic means. It is defined for a set of "n" positive numbers "x"i by
By choosing different values for the parameter "m", the following types of means are obtained:
"ƒ"-mean.
This can be generalized further as the generalized f-mean
and again a suitable choice of an invertible "ƒ" will give
Weighted arithmetic mean.
The weighted arithmetic mean (or weighted average) is used if one wants to combine average values from samples of the same population with different sample sizes:
The weights formula_24 represent the sizes of the different samples. In other applications they represent a measure for the reliability of the influence upon the mean by the respective values.
Truncated mean.
Sometimes a set of numbers might contain outliers, i.e., data values which are much lower or much higher than the others.
Often, outliers are erroneous data caused by artifacts. In this case, one can use a truncated mean. It involves discarding given parts of the data at the top or the bottom end, typically an equal amount at each end, and then taking the arithmetic mean of the remaining data. The number of values removed is indicated as a percentage of total number of values.
Interquartile mean.
The interquartile mean is a specific example of a truncated mean. It is simply the arithmetic mean after removing the lowest and the highest quarter of values.
assuming the values have been ordered, so is simply a specific example of a weighted mean for a specific set of weights.
Mean of a function.
In some circumstances mathematicians may calculate a mean of an infinite (even an uncountable) set of values. This can happen when calculating the mean value formula_26 of a function formula_20. Intuitively this can be thought of as calculating the area under a section of a curve and then dividing by the length of that section. This can be done crudely by counting squares on graph paper or more precisely by integration. The integration formula is written as:
Care must be taken to make sure that the integral converges. But the mean may be finite even if the function itself tends to infinity at some points.
Mean of angles.
Sometimes the usual calculations of means fail on cyclical quantities such as angles, times of day, and other situations where modular arithmetic is used. For those quantities it might be appropriate to use a mean of circular quantities to take account of the modular values, or to adjust the values before calculating the mean.
Fréchet mean.
The Fréchet mean gives a manner for determining the "center" of a mass distribution on a surface or, more generally, Riemannian manifold. Unlike many other means, the Fréchet mean is defined on a space whose elements cannot necessarily be added together or multiplied by scalars.
It is sometimes also known as the Karcher mean (named after Hermann Karcher).
Distribution of the sample mean.
The arithmetic mean of a population, or population mean, is denoted "μ". The sample mean (the arithmetic mean of a sample of values drawn from the population) makes a good estimator of the population mean, as its expected value is equal to the population mean (that is, it is an unbiased estimator). The sample mean is a random variable, not a constant, since its calculated value will randomly differ depending on which members of the population are sampled, and consequently it will have its own distribution. For a random sample of "n" observations from a normally distributed population, the sample mean distribution is normally distributed with mean and variance as follows:
Often, since the population "variance" is an unknown parameter, it is estimated by the mean sum of squares; when this estimated value is used, the distribution of the sample mean is no longer a normal distribution but rather a Student's t distribution with "n" − 1 degrees of freedom.

</doc>
<doc id="20351457" url="https://en.wikipedia.org/wiki?curid=20351457" title="Mean square weighted deviation">
Mean square weighted deviation

Mean square weighted deviation is a statistical method used extensively in geochronology also known as the reduced chi-squared.
The Mean Square Weighted Deviation (MSWD) is a measure of goodness of fit that takes into account the relative importance of both the internal and external reproducibility, with most common usage in isotopic dating. This statistic is synonmous with the reduced chi-squared statistic, which has widespread usage in statistics and other fields of science. 
In general when:
MSWD = 1 if the age data fit a univariate normal distribution in t (for the arithmetic mean age) or log(t) (for the geometric mean age) space, or if the compositional data fit a bivariate normal distribution in [log(U/He),log(Th/He)]-space (for the central age).
MSWD < 1 if the observed scatter is less than that predicted by the analytical uncertainties. In this case, the data are said to be "underdispersed", indicating that the analytical uncertainties were overestimated.
MSWD > 1 if the observed scatter exceeds that predicted by the analytical uncertainties. In this case, the data are said to be "overdispersed". This situation is the rule rather than the exception in (U-Th)/He geochronology, indicating an incomplete understanding of the isotope system. Several reasons have been proposed to explain the overdispersion of (U-Th)/He data, including unevenly distributed U-Th distributions and radiation damage.
The MSWD statistic is widely used in the field of isotopic dating. Wendt and Carl (1991) provide an "expert" publication on this statistic. Note however that MSWD is directly equivalent to reduced chi squared statistic. Wendt, I., and Carl, C., 1991,The statistical distribution of the mean squared weighted deviation, Chemical Geology, 275-285. 
Often the geochronologist will determine a series of age measurements on a single sample, with the measured value formula_1 having a weighting formula_2 and an associated error formula_3 for each age determination. As regards weighting, one can either weight all of the measured ages equally, or weight them by the proportion of the sample that they represent. For example, if two thirds of the sample was used for the first measurement and one third for the second and final measurement then one might weight the first measurement twice that of the second.
The arithmetic mean of the age determinations is:
but this value can be misleading unless each determination of the age is of equal significance.
When each measured value can be assumed to have the same weighting, or significance, the biased and unbiased (or "sample" and "population", respectively) estimators of the variance are computed as follows:
The standard deviation is the square root of the variance.
When individual determinations of an age are not of equal significance it is better to use a weighted mean to obtain an 'average' age, as follows:
The biased weighted estimator of variance can be shown to be:
which can be computed on the fly as
The unbiased weighted estimator of the sample variance can be computed as follows:
Again the corresponding standard deviation is the square root of the variance.
The unbiased weighted estimator of the sample variance can also be computed on the fly as follows:
The unweighted mean square of the weighted deviations (unweighted MSWD) can then be computed, as follows:
By analogy the weighted mean square of the weighted deviations (weighted MSWD) can be computed, as follows:
Notes and references.
Examples of MSWD in current practical use can be found below 
Discussions of the basic mathematical principles

</doc>
<doc id="3069520" url="https://en.wikipedia.org/wiki?curid=3069520" title="Measurement uncertainty">
Measurement uncertainty

In metrology, measurement uncertainty is a non-negative parameter characterizing the dispersion of the values attributed to a measured quantity. The uncertainty has a probabilistic basis and reflects incomplete knowledge of the quantity. All measurements are subject to uncertainty and a measured value is only complete if it is accompanied by a statement of the associated uncertainty. Relative uncertainty is the measurement uncertainty divided by the measured value.
Background.
The purpose of measurement is to provide information about a quantity of interest – a measurand. For example, the measurand might be the size of a cylindrical feature per ASME Y14.5-2009, the volume of a vessel, the potential difference between the terminals of a battery, or the mass concentration of lead in a flask of water.
No measurement is exact. When a quantity is measured, the outcome depends on the measuring system, the measurement procedure, the skill of the operator, the environment, and other effects. Even if the quantity were to be measured several times, in the same way and in the same circumstances, a different measured value would in general be obtained each time, assuming the measuring system has sufficient resolution to distinguish between the values.
The dispersion of the measured values would relate to how well the measurement is performed. 
Their average would provide an estimate of the true value of the quantity that generally would be more reliable than an individual measured value. 
The dispersion and the number of measured values would provide information relating to the average value as an estimate of the true value. 
However, this information would not generally be adequate.
The measuring system may provide measured values that are not dispersed about the true value, but about some value offset from it. Take a domestic bathroom scale. Suppose it is not set to show zero when there is nobody on the scale, but to show some value offset from zero. Then, no matter how many times the person's mass were re-measured, the effect of this offset would be inherently present in the average of the values.
Measurement uncertainty has important economic consequences for calibration and measurement activities. In calibration reports, the magnitude of the uncertainty is often taken as an indication of the quality of the laboratory, and smaller uncertainty values generally are of higher value and of higher cost. The American Society of Mechanical Engineers (ASME) has produced a suite of standards addressing various aspects of measurement uncertainty. ASME B89.7.3.1, Guidelines for Decision Rules in Determining Conformance to Specifications addresses the role of measurement uncertainty when accepting or rejecting products based on a measurement result and a product specification. ASME B89.7.3.2, Guidelines for the Evaluation of Dimensional Measurement Uncertainty, provides a simplified approach (relative to the GUM) to the evaluation of dimensional measurement uncertainty. ASME B89.7.3.3, Guidelines for Assessing the Reliability of Dimensional Measurement Uncertainty Statements, examines how to resolve disagreements over the magnitude of the measurement uncertainty statement. ASME B89.7.4, Measurement Uncertainty and Conformance Testing: Risk Analysis, provides guidance on the risks involved in any product acceptance/rejection decision.
The "Guide to the Expression of Uncertainty in Measurement", commonly known as the GUM, is the definitive document on this subject. The GUM has been adopted by all major National Measurement Institutes (NMIs), by international laboratory accreditation standards such as ISO 17025 which is required for international laboratory Accreditation, and employed in most modern national and international documentary standards on measurement methods and technology. See Joint Committee for Guides in Metrology.
Indirect measurement.
The above discussion concerns the direct measurement of a quantity, which incidentally occurs rarely. For example, the bathroom scale may convert a measured extension of a spring into an estimate of the measurand, the mass of the person on the scale. The particular relationship between extension and mass is determined by the calibration of the scale. A measurement model converts a quantity value into the corresponding value of the measurand.
There are many types of measurement in practice and therefore many models. A simple measurement model (for example for a scale, where the mass is proportional to the extension of the spring) might be sufficient for everyday domestic use. Alternatively, a more sophisticated model of a weighing, involving additional effects such as air buoyancy, is capable of delivering better results for industrial or scientific purposes. In general there are often several different quantities, for example temperature, humidity and displacement, that contribute to the definition of the measurand, and that need to be measured.
Correction terms should be included in the measurement model when the conditions of measurement are not exactly as stipulated. These terms correspond to systematic errors. Given an estimate of a correction term, the relevant quantity should be corrected by this estimate. There will be an uncertainty associated with the estimate, even if the estimate is zero, as is often the case. Instances of systematic errors arise in height measurement, when the alignment of the measuring instrument is not perfectly vertical, and the ambient temperature is different from that prescribed. Neither the alignment of the instrument nor the ambient temperature is specified exactly, but information concerning these effects is available, for example the lack of alignment is at most 0.001° and the ambient temperature at the time of measurement differs from that stipulated by at most 2 °C.
As well as raw data representing measured values, there is another form of data that is frequently needed in a measurement model. Some such data relate to quantities representing physical constants, each of which is known imperfectly. Examples are material constants such as modulus of elasticity and specific heat. There are often other relevant data given in reference books, calibration certificates, etc., regarded as estimates of further quantities.
The items required by a measurement model to define a measurand are known as input quantities in a measurement model. The model is often referred to as a functional relationship. The output quantity in a measurement model is the measurand.
Formally, the output quantity, denoted by formula_1, about which information is required, is often related to input quantities, denoted by formula_2, about which information is available, by a measurement model in the form of
where formula_4 is known as the measurement function. A general expression for a measurement model is
It is taken that a procedure exists for calculating formula_1 given formula_2, and that formula_1 is uniquely defined by this equation.
Propagation of distributions.
The true values of the input quantities formula_2 are unknown. 
In the GUM approach, formula_2 are characterized by probability distributions and treated mathematically as random variables. 
These distributions describe the respective probabilities of their true values lying in different intervals, and are assigned based on available knowledge concerning formula_2. 
Sometimes, some or all of are interrelated and the relevant distributions, which are known as joint, apply to these quantities taken together.
Consider estimates formula_13, respectively, of the input quantities formula_2, obtained from certificates and reports, manufacturers' specifications, the analysis of measurement data, and so on. 
The probability distributions characterizing formula_2 are chosen such that the estimates formula_13, respectively, are the expectations of formula_2. 
Moreover, for the formula_18th input quantity, consider a so-called "standard uncertainty", given the symbol formula_19, defined as the standard deviation of the input quantity formula_20. 
This standard uncertainty is said to be associated with the (corresponding) estimate formula_21.
The use of available knowledge to establish a probability distribution to characterize each quantity of interest applies to the formula_20 and also to formula_1. 
In the latter case, the characterizing probability distribution for formula_1 is determined by the measurement model together with the probability distributions for the formula_20. 
The determination of the probability distribution for formula_1 from this information is known as the "propagation of distributions".
The figure below depicts a measurement model formula_27 in the case where formula_28 and formula_29 are each characterized by a (different) rectangular, or uniform, probability distribution. 
formula_1 has a symmetric trapezoidal probability distribution in this case.
Once the input quantities formula_2 have been characterized by appropriate probability distributions, and the measurement model has been developed, the probability distribution for the measurand formula_1 is fully specified in terms of this information. In particular, the expectation of formula_1 is used as the estimate of formula_1, and the standard deviation of formula_1 as the standard uncertainty associated with this estimate.
Often an interval containing formula_1 with a specified probability is required. Such an interval, a coverage interval, can be deduced from the probability distribution for formula_1. The specified probability is known as the coverage probability. For a given coverage probability, there is more than one coverage interval. The probabilistically symmetric coverage interval is an interval for which the probabilities (summing to one minus the coverage probability) of a value to the left and the right of the interval are equal. The shortest coverage interval is an interval for which the length is least over all coverage intervals having the same coverage probability.
Prior knowledge about the true value of the output quantity formula_1 can also be considered. For the domestic bathroom scale, the fact that the person's mass is positive, and that it is the mass of a person, rather than that of a motor car, that is being measured, both constitute prior knowledge about the possible values of the measurand in this example. Such additional information can be used to provide a probability distribution for formula_1 that can give a smaller standard deviation for formula_1 and hence a smaller standard uncertainty associated with the estimate of formula_1.
Type A and Type B evaluation of uncertainty.
Knowledge about an input quantity formula_20 is inferred from repeated measured values ("Type A evaluation of uncertainty"), or scientific judgement or other information concerning the possible values of the quantity ("Type B evaluation of uncertainty").
In Type A evaluations of measurement uncertainty, the assumption is often made that the distribution best describing an input quantity formula_43 given repeated measured values of it (obtained independently) is a Gaussian distribution.
formula_43 then has expectation equal to the average measured value and standard deviation equal to the standard deviation of the average.
When the uncertainty is evaluated from a small number of measured values (regarded as instances of a quantity characterized by a Gaussian distribution), the corresponding distribution can be taken as a formula_45-distribution.
Other considerations apply when the measured values are not obtained independently.
For a Type B evaluation of uncertainty, often the only available information is that formula_43 lies in a specified interval [formula_47].
In such a case, knowledge of the quantity can be characterized by a rectangular probability distribution with limits formula_48 and formula_49.
If different information were available, a probability distribution consistent with that information would be used.
Sensitivity coefficients.
Sensitivity coefficients formula_50 describe how the estimate formula_51 of formula_1 would be influenced by small changes in the estimates formula_13 of the input quantities formula_2.
For the measurement model formula_55, the sensitivity coefficient formula_56 equals the partial derivative of first order of formula_4 with respect to formula_20 evaluated at formula_59, formula_60, etc.
For a linear measurement model
with formula_2 independent, a change in formula_21 equal to formula_19 would give a change formula_65 in formula_51.
This statement would generally be approximate for measurement models formula_55.
The relative magnitudes of the terms formula_68 are useful in assessing the respective contributions from the input quantities to the standard uncertainty formula_69 associated with formula_51.
The standard uncertainty formula_69 associated with the estimate formula_51 of the output quantity formula_1 is not given by the sum of the formula_68, but these terms combined in quadrature, namely by [an expression that is generally approximate for measurement models formula_55]
which is known as the law of propagation of uncertainty.
When the input quantities formula_20 contain dependencies, the above formula is augmented by terms containing covariances, which may increase or decrease formula_69.
Uncertainty evaluation.
The main stages of uncertainty evaluation constitute formulation and calculation, the latter consisting of propagation and summarizing.
The formulation stage constitutes
The calculation stage consists of propagating the probability distributions for the input quantities through the measurement model to obtain the probability distribution for the output quantity formula_1, and summarizing by using this distribution to obtain
The propagation stage of uncertainty evaluation is known as the propagation of distributions, various approaches for which are available, including
For any particular uncertainty evaluation problem, approach 1), 2) or 3) (or some other approach) is used, 1) being generally approximate, 2) exact, and 3) providing a solution with a numerical accuracy that can be controlled.
Models with any number of output quantities.
When the measurement model is multivariate, that is, it has any number of output quantities, the above concepts can be extended. The output quantities are now described by a joint probability distribution, the coverage interval becomes a coverage region, the law of propagation of uncertainty has a natural generalization, and a calculation procedure that implements a multivariate Monte Carlo method is available.
Alternative perspective.
Most of this article represents the most common view of measurement uncertainty, which assumes 
that random variables are proper mathematical models for uncertain quantities and simple 
probability distributions are sufficient for representing all forms of measurement uncertainties.
In some situations, however, a mathematical interval rather than a probability 
distribution might be a better model of uncertainty. This may include situations involving periodic measurements, 
binned data values, censoring, detection limits, or
plus-minus ranges of measurements where no particular probability distribution seems justified or 
where one cannot assume that the errors among individual measurements are completely independent.
A more robust representation of measurement uncertainty in such cases can be fashioned from intervals.
An interval ["a","b"] is different from a rectangular or uniform probability distribution over the same range in that the 
latter suggests that the true value lies inside the right half of the range [("a" + "b")/2, "b"] with
probability one half, and within any subinterval of ["a","b"] with probability equal to the width of the subinterval divided by "b" – "a". The interval makes no such claims, except simply that the measurement lies somewhere within the interval. Distributions of such measurement intervals can be summarized as probability boxes and Dempster–Shafer structures over the real numbers, which incorporate both aleatoric and epistemic uncertainties.

</doc>
<doc id="18837" url="https://en.wikipedia.org/wiki?curid=18837" title="Median">
Median

In statistics and probability theory, a median is the number separating the higher half of a data sample, a population, or a probability distribution, from the lower half. The "median" of a finite list of numbers can be found by arranging all the observations from lowest value to highest value and picking the middle one (e.g., the median of {3, 3, 5, 9, 11} is 5). If there is an even number of observations, then there is no single middle value; the median is then usually defined to be the mean of the two middle values
(the median of {3, 5, 7, 9} is (5 + 7) / 2 = 6), which corresponds to interpreting the median as the fully trimmed mid-range. The median is of central importance in robust statistics, as it is the most resistant statistic, having a breakdown point of 50%: so long as no more than half the data are contaminated, the median will not give an arbitrarily large result.
A median is only defined on ordered one-dimensional data, and is independent of any distance metric. A geometric median, on the other hand, is defined in any number of dimensions.
In a sample of data, or a finite population, there may be no member of the sample whose value is identical to the median (in the case of an even sample size); if there is such a member, there may be more than one so that the median may not uniquely identify a sample member. Nonetheless, the value of the median is uniquely determined with the usual definition. A related concept, in which the outcome is forced to correspond to a member of the sample, is the medoid.
At most, half the population have values strictly less than the "median", and, at most, half have values strictly greater than the median. If each group contains less than half the population, then some of the population is exactly equal to the median. For example, if "a" < "b" < "c", then the median of the list {"a", "b", "c"} is "b", and, if "a" < "b" < "c" < "d", then the median of the list {"a", "b", "c", "d"} is the mean of "b" and "c"; i.e., it is ("b" + "c")/2.
The median can be used as a measure of location when a distribution is skewed, when end-values are not known, or when one requires reduced importance to be attached to outliers, e.g., because they may be measurement errors.
In terms of notation, some authors represent the median of a variable "x" either as "x͂" or as "μ"1/2 sometimes also "M". There is no widely accepted standard notation for the median, so the use of these or other symbols for the median needs to be explicitly defined when they are introduced.
The median is the 2nd quartile, 5th decile, and 50th percentile.
Measures of location and dispersion.
The median is one of a number of ways of summarising the typical values associated with members of a statistical population; thus, it is a possible location parameter. Since the median is the same as the "second quartile", its calculation is illustrated in the article on quartiles.
When the median is used as a location parameter in descriptive statistics, there are several choices for a measure of variability: the range, the interquartile range, the mean absolute deviation, and the median absolute deviation.
For practical purposes, different measures of location and dispersion are often compared on the basis of how well the corresponding population values can be estimated from a sample of data. The median, estimated using the sample median, has good properties in this regard. While it is not usually optimal if a given population distribution is assumed, its properties are always reasonably good. For example, a comparison of the efficiency of candidate estimators shows that the sample mean is more statistically efficient than the sample median when data are uncontaminated by data from heavy-tailed distributions or from mixtures of distributions, but less efficient otherwise, and that the efficiency of the sample median is higher than that for a wide range of distributions. More specifically, the median has a 64% efficiency compared to the minimum-variance mean (for large normal samples), which is to say the variance of the median will be ~50% greater than the variance of the mean—see Efficiency (statistics)#Asymptotic efficiency and references therein.
Probability distributions.
For any probability distribution on the real line R with cumulative distribution function "F", regardless of whether it is any kind of continuous probability distribution, in particular an absolutely continuous distribution (which has a probability density function), or a discrete probability distribution, a median is by definition any real number "m" that satisfies the inequalities
or, equivalently, the inequalities
in which a Lebesgue–Stieltjes integral is used. For an absolutely continuous probability distribution with probability density function "ƒ", the median satisfies
Any probability distribution on R has at least one median, but there may be more than one median. Where exactly one median exists, statisticians speak of "the median" correctly; even when the median is not unique, some statisticians speak of "the median" informally.
Medians of particular distributions.
The medians of certain types of distributions can be easily calculated from their parameters; furthermore, they exist even for some distributions lacking a well-defined mean, such as the Cauchy distribution:
Descriptive statistics.
The median is used primarily for skewed distributions, which it summarizes differently from the arithmetic mean. Consider the multiset { 1, 2, 2, 2, 3, 14 }. The median is 2 in this case, (as is the mode), and it might be seen as a better indication of central tendency (less susceptible to the exceptionally large value in data) than the arithmetic mean of 4.
Calculation of medians is a popular technique in summary statistics and summarizing statistical data, since it is simple to understand and easy to calculate, while also giving a measure that is more robust in the presence of outlier values than is the mean. The widely-cited empirical relationship between the relative locations of the mean and the median for skewed distributions is, however, not generally true. There are, however, various relationships for the "absolute" difference between them; see below.
Populations.
Optimality property.
The "mean absolute error" of a real variable "c" with respect to the random variable "X" is
Provided that the probability distribution of "X" is such that the above expectation exists, then "m" is a median of "X" if and only if "m" is a minimizer of the mean absolute error with respect to "X". In particular, "m" is a sample median if and only if "m" minimizes the arithmetic mean of the absolute deviations.
See also "k"-medians clustering.
Unimodal distributions.
It can be shown for a unimodal distribution that the median formula_5 and the mean formula_6 lie within (3/5)1/2 ≈ 0.7746 standard deviations of each other. In symbols,
where |.| is the absolute value.
A similar relation holds between the median and the mode: they lie within 31/2 ≈ 1.732 standard deviations of each other:
Inequality relating means and medians.
If the distribution has finite variance, then the distance between the median and the mean is bounded by one standard deviation.
This bound was proved by Mallows, who used Jensen's inequality twice, as follows. We have
The first and third inequalities come from Jensen's inequality applied to the absolute-value function and the square function, which are each convex. The second inequality comes from the fact that a median minimizes the absolute deviation function
This proof can easily be generalized to obtain a multivariate version of the inequality, as follows:
where "m" is a spatial median, that is, a minimizer of the function
formula_12 The spatial median is unique when the data-set's dimension is two or more. An alternative proof uses the one-sided Chebyshev inequality; it appears in .
Jensen's inequality for medians.
Jensen's inequality states that for any random variable "x" with a ﬁnite expectation "E"("x") and for any convex function "f"
It has been shown that if "x" is a real variable with a unique median "m" and "f" is a C function then
A C function is a real valued function, defined on the set of real numbers "R", with the property that for any real "t"
is a closed interval, a singleton or an empty set.
Medians for samples.
The sample median.
Efficient computation of the sample median.
Even though comparison-sorting "n" items requires operations, selection algorithms can compute the 'th-smallest of items with only operations. This includes the median, which is the 'th order statistic (or for an even number of samples, the average of the two middle order statistics).
Selection algorithms still have the downside of requiring memory, that is, they need to have the full sample (or a linear-sized portion of it) in memory. Because this, as well as the linear time requirement, can be prohibitive, several estimation procedures for the median have been developed. A simple one is the median of three rule, which estimates the median as the median of a three-element sample from the sample; this is commonly used as a subroutine in the quicksort sorting algorithm, which uses an estimate of its input's median. A more robust estimator is Tukey's "ninther", which is the median of three rule applied with limited recursion: if is the sample laid out as an array, and
then
This idea is generalized by the "remedian" estimator.
Easy explanation of the sample median.
In individual series (if number of observation is very low) first one must arrange all the observations in order. Then count("n") is the total number of observation in given data.
If "n" is odd then Median ("M") = value of (("n" + 1)/2)th item term.
If "n" is even then Median ("M") = value of [(("n")/2)th item term + (("n")/2 + 1)th item term ]/2
As an example, we will calculate the sample median for the following set of observations: 1, 5, 2, 8, 7.
Start by sorting the values: 1, 2, 5, 7, 8.
In this case, the median is 5 since it is the middle observation in the ordered list.
The median is the (("n" + 1)/2)th item, where "n" is the number of values. For example, for the list {1, 2, 5, 7, 8}, we have "n" = 5, so the median is the ((5 + 1)/2)th item.
As an example, we will calculate the sample median for the following set of observations: 1, 6, 2, 8, 7, 2.
Start by sorting the values: 1, 2, 2, 6, 7, 8.
In this case, the arithmetic mean of the two middlemost terms is (2 + 6)/2 = 4. Therefore, the median is 4 since it is the arithmetic mean of the middle observations in the ordered list.
We also use this formula MEDIAN = {("n" + 1 )/2}th item . "n" = number of values
As above example 1, 2, 2, 6, 7, 8
"n" = 6 Median = {(6 + 1)/2}th item = 3.5th item. In this case, the median is average of the 3rd number and the next one (the fourth number). The median is (2 + 6)/2 which is 4.
Sampling distribution.
The distribution of both the sample mean and the sample median were determined by Laplace. The distribution of the sample median from a population with a density function formula_16 is asymptotically normal with mean formula_17 and variance
where formula_17 is the median value of distribution and formula_20 is the sample size. In practice this may be difficult to estimate as the density function is usually unknown.
These results have also been extended. It is now known for the formula_21-th quantile that the distribution of the sample formula_21-th quantile is asymptotically normal around the formula_21-th quantile with variance equal to
where formula_25 is the value of the distribution density at the formula_21-th quantile.
The value of formula_27—the asymptotic value of formula_28 where formula_29 is the population median—has been studied by several authors. The standard 'delete one' jackknife method produces inconsistent results. An alternative—the 'delete k' method—where formula_30 grows with the sample size has been shown to be asymptotically consistent. This method may be computationally expensive for large data sets. A bootstrap estimate is known to be consistent, but converges very slowly (order of formula_31). Other methods have been proposed but their behavior may differ between large and small samples.
The efficiency of the sample median, measured as the ratio of the variance of the mean to the variance of the median, depends on the sample size and on the underlying population distribution. For a sample of size formula_32 from the normal distribution, the ratio is
For large samples (as formula_20 tends to infinity) this ratio tends to formula_35
Other estimators.
For univariate distributions that are "symmetric" about one median, the Hodges–Lehmann estimator is a robust and highly efficient estimator of the population median.
If data are represented by a statistical model specifying a particular family of probability distributions, then estimates of the median can be obtained by fitting that family of probability distributions to the data and calculating the theoretical median of the fitted distribution. Pareto interpolation is an application of this when the population is assumed to have a Pareto distribution.
Coefficient of dispersion.
The coefficient of dispersion (CD) is defined as the ratio of the average absolute deviation from the median to the median of the data. It is a statistical measure used by the states of Iowa, New York and South Dakota in estimating dues taxes. In symbols
where "n" is the sample size, "m" is the sample median and "x" is a variate. The sum is taken over the whole sample.
Confidence intervals for a two sample test where the sample sizes are large have been derived by Bonett and Seier This test assumes that both samples have the same median but differ in the dispersion around it. The confidence interval (CI) is bounded inferiorly by
where "t"j is the mean absolute deviation of the "j"th sample, var() is the variance and "zα" is the value from the normal distribution for the chosen value of "α": for "α" = 0.05, "zα" = 1.96. The following formulae are used in the derivation of these confidence intervals
where "r" is the Pearson correlation coefficient between the squared deviation scores
"a" and "b" here are constants equal to 1 and 2, "x" is a variate and "s" is the standard deviation of the sample.
Multivariate median.
Previously, this article discussed the concept of a univariate median for a one-dimensional object (population, sample). When the dimension is two or higher, there are multiple concepts that extend the definition of the univariate median; each such multivariate median agrees with the univariate median when the dimension is exactly one. In higher dimensions, however, there are several multivariate medians.
Marginal median.
The marginal median is defined for vectors defined with respect to a fixed set of coordinates. A marginal median is defined to be the vector whose components are univariate medians. The marginal median is easy to compute, and its properties were studied by Puri and Sen.
Spatial median (L1 median).
In a normed vector space of dimension two or greater, the "spatial median" minimizes the expected distance
where "X" and "a" are vectors, if this expectation has a finite minimum; another definition is better suited for general probability-distributions. The spatial median is unique when the data-set's dimension is two or more. It is a robust and highly efficient estimator of a central tendency of a population.
The Geometric median is the corresponding estimator based on the sample statistics of a finite set of points, rather than the population statistics. It is the point minimizing the arithmetic average of Euclidean distances to the given sample points, instead of the expectation. Note that the arithmetic average and sum are interchangeable since they differ by a fixed constant which does not alter the location of the minimum.
Other multivariate medians.
An alternative generalization of the spatial median in higher dimensions that does not relate to a particular metric is the centerpoint.
Other median-related concepts.
Pseudo-median.
For univariate distributions that are "symmetric" about one median, the Hodges–Lehmann estimator is a robust and highly efficient estimator of the population median; for non-symmetric distributions, the Hodges–Lehmann estimator is a robust and highly efficient estimator of the population "pseudo-median", which is the median of a symmetrized distribution and which is close to the population median. The Hodges–Lehmann estimator has been generalized to multivariate distributions.
Variants of regression.
The Theil–Sen estimator is a method for robust linear regression based on finding medians of slopes.
Median filter.
In the context of image processing of monochrome raster images there is a type of noise, known as the salt and pepper noise, when each pixel independently becomes black (with some small probability) or white (with some small probability), and is unchanged otherwise (with the probability close to 1). An image constructed of median values of neighborhoods (like 3×3 square) can effectively reduce noise in this case.
Cluster analysis.
In cluster analysis, the k-medians clustering algorithm provides a way of defining clusters, in which the criterion of maximising the distance between cluster-means that is used in k-means clustering, is replaced by maximising the distance between cluster-medians.
Median–median line.
This is a method of robust regression. The idea dates back to Wald in 1940 who suggested dividing a set of bivariate data into two halves depending on the value of the independent parameter formula_43: a left half with values less than the median and a right half with values greater than the median. He suggested taking the means of the dependent formula_44 and independent formula_43 variables of the left and the right halves and estimating the slope of the line joining these two points. The line could then be adjusted to fit the majority of the points in the data set.
Nair and Shrivastava in 1942 suggested a similar idea but instead advocated dividing the sample into three equal parts before calculating the means of the subsamples. Brown and Mood in 1951 proposed the idea of using the medians of two subsamples rather the means. Tukey combined these ideas and recommended dividing the sample into three equal size subsamples and estimating the line based on the medians of the subsamples.
Median-unbiased estimators.
Any "mean"-unbiased estimator minimizes the risk (expected loss) with respect to the squared-error loss function, as observed by Gauss. A "median"-unbiased estimator minimizes the risk with respect to the absolute-deviation loss function, as observed by Laplace. Other loss functions are used in statistical theory, particularly in robust statistics.
The theory of median-unbiased estimators was revived by George W. Brown in 1947:
Further properties of median-unbiased estimators have been reported. In particular, median-unbiased estimators exist in cases where mean-unbiased and maximum-likelihood estimators do not exist. Median-unbiased estimators are invariant under one-to-one transformations.
History.
The idea of the median appeared in the 13th century in the Talmud (further for possible older mentions)
The idea of the median also appeared later in Edward Wright's book on navigation ("Certaine Errors in Navigation") in 1599 in a section concerning the determination of location with a compass. Wright felt that this value was the most likely to be the correct value in a series of observations.
In 1757, Roger Joseph Boscovich developed a regression method based on the L1 norm and therefore implicitly on the median.
In 1774, Laplace suggested the median be used as the standard estimator of the value of a posterior pdf. The specific criteria was to minimize the expected magnitude of the error; formula_46 where formula_47 is the estimate and formula_48 is the true value. Laplaces's criterion was generally rejected for 150 years in favor of the least squares method of Gauss and Legendre which minimizes formula_49 to obtain the mean. The distribution of both the sample mean and the sample median were determined by Laplace in the early 1800s.
Antoine Augustin Cournot in 1843 was the first to use the term "median" ("valeur médiane") for the value that divides a probability distribution into two equal halves. Gustav Theodor Fechner used the median ("Centralwerth") in sociological and psychological phenomena. It had earlier been used only in astronomy and related fields. Gustav Fechner popularized the median into the formal analysis of data, although it had been used previously by Laplace.
Francis Galton used the English term "median" in 1881, having earlier used the terms "middle-most value" in 1869 and the "medium" in 1880.

</doc>
<doc id="51117" url="https://en.wikipedia.org/wiki?curid=51117" title="Meissner effect">
Meissner effect

The Meissner effect is the expulsion of a magnetic field from a superconductor during its transition to the superconducting state. The German physicists Walther Meissner and Robert Ochsenfeld discovered this phenomenon in 1933 by measuring the magnetic field distribution outside superconducting tin and lead samples. The samples, in the presence of an applied magnetic field, were cooled below their superconducting transition temperature. Below the transition temperature the samples cancelled nearly all interior magnetic fields. They detected this effect only indirectly because the magnetic flux is conserved by a superconductor: when the interior field decreases, the exterior field increases. The experiment demonstrated for the first time that superconductors were more than just perfect conductors and provided a uniquely defining property of the superconducting state.
A superconductor with little or no magnetic field within it is said to be in the Meissner state. The Meissner state breaks down when the applied magnetic field is too large. Superconductors can be divided into two classes according to how this breakdown occurs. In Type I superconductors, superconductivity is abruptly destroyed when the strength of the applied field rises above a critical value "Hc". Depending on the geometry of the sample, one may obtain an intermediate state consisting of a baroque pattern of regions of normal material carrying a magnetic field mixed with regions of superconducting material containing no field. In Type II superconductors, raising the applied field past a critical value "H""c"1 leads to a mixed state (also known as the vortex state) in which an increasing amount of magnetic flux penetrates the material, but there remains no resistance to the flow of electric current as long as the current is not too large. At a second critical field strength "H""c"2, superconductivity is destroyed. The mixed state is actually caused by vortices in the electronic superfluid, sometimes called fluxons because the flux carried by these vortices is quantized. Most pure elemental superconductors, except niobium and carbon nanotubes, are Type I, while almost all impure and compound superconductors are Type II.
Explanation.
The Meissner effect was given a phenomenological explanation by the brothers Fritz and Heinz London, who showed that the electromagnetic free energy in a superconductor is minimized provided
where H is the magnetic field and λ is the London penetration depth.
This equation, which is known as the London equation, predicts that the magnetic field in a superconductor decays exponentially from whatever value it possesses at the surface.
In a weak applied field, a superconductor "expels" nearly all magnetic flux. It does this by setting up electric currents near its surface. The magnetic field of these surface currents cancels the applied magnetic field within the bulk of the superconductor. As the field expulsion, or cancellation, does not change with time, the currents producing this effect (called persistent currents) do not decay with time. Therefore, the conductivity can be thought of as infinite: a superconductor.
Near the surface, within the London penetration depth, the magnetic field is not completely cancelled. Each superconducting material has its own characteristic penetration depth.
Any perfect conductor will prevent any change to magnetic flux passing through its surface due to ordinary electromagnetic induction at zero resistance. The Meissner effect is distinct from this: when an ordinary conductor is cooled so that it makes the transition to a superconducting state in the presence of a constant applied magnetic field, the magnetic flux is expelled during the transition. This effect cannot be explained by infinite conductivity alone. Its explanation is more complex and was first given in the London equations by the brothers Fritz and Heinz London. It should thus be noted that the placement and subsequent levitation of a magnet above an already superconducting material does not demonstrate the Meissner effect, while an initially stationary magnet later being repelled by a superconductor as it is cooled through its critical temperature does.
Perfect diamagnetism.
Superconductors in the Meissner state exhibit perfect diamagnetism, or superdiamagnetism, meaning that the total magnetic field is very close to zero deep inside them (many penetration depths from the surface). This means that their magnetic susceptibility, formula_2 = −1. Diamagnetics are defined by the generation of a spontaneous magnetization of a material which directly opposes the direction of an applied field. However, the fundamental origins of diamagnetism in superconductors and normal materials are very different. In normal materials diamagnetism arises as a direct result of the orbital spin of electrons about the nuclei of an atom induced electromagnetically by the application of an applied field. In superconductors the illusion of perfect diamagnetism arises from persistent screening currents which flow to oppose the applied field (the Meissner effect); not solely the orbital spin.
Consequences.
The discovery of the Meissner effect led to the phenomenological theory of superconductivity by Fritz and Heinz London in 1935. This theory explained resistanceless transport and the Meissner effect, and allowed the first theoretical predictions for superconductivity to be made. However, this theory only explained experimental observations—it did not allow the microscopic origins of the superconducting properties to be identified. This was done successfully by the BCS theory in 1957, from which the penetration depth and the Meissner effect result. However, some physicists argue that BCS theory does not explain the Meissner effect.
Paradigm for the Higgs mechanism.
The Meissner effect of superconductivity serves as an important paradigm for the generation mechanism of a mass "M" (i.e. a reciprocal "range", formula_3 where "h" is Planck constant and "c" is speed of light) for a gauge field. In fact, this analogy is an abelian example for the Higgs mechanism, through which in high-energy physics the masses of the electroweak gauge particles, and are generated. The length formula_4 is identical with the London penetration depth in the theory of superconductivity.

</doc>
<doc id="19870" url="https://en.wikipedia.org/wiki?curid=19870" title="Meson">
Meson

In particle physics, mesons ( or ) are hadronic subatomic particles composed of one quark and one antiquark, bound together by the strong interaction. Because mesons are composed of sub-particles, they have a physical size, with a diameter of roughly one fermi, which is about the size of a proton or neutron. All mesons are unstable, with the longest-lived lasting for only a few hundredths of a microsecond. Charged mesons decay (sometimes through intermediate particles) to form electrons and neutrinos. Uncharged mesons may decay to photons.
Mesons are not produced by radioactive decay, but appear in nature only as short-lived products of very high-energy interactions in matter, between particles made of quarks. In cosmic ray interactions, for example, such particles are ordinary protons and neutrons. Mesons are also frequently produced artificially in high-energy particle accelerators that collide protons, anti-protons, or other particles.
In nature, the importance of lighter mesons is that they are the associated quantum-field particles that transmit the nuclear force, in the same way that photons are the particles that transmit the electromagnetic force. The higher energy (more massive) mesons were created momentarily in the Big Bang, but are not thought to play a role in nature today. However, such particles are regularly created in experiments, in order to understand the nature of the heavier types of quark that compose the heavier mesons.
Mesons are part of the hadron particle family, and are defined simply as particles composed of two quarks. The other members of the hadron family are the baryons: subatomic particles composed of three quarks rather than two. Some experiments show evidence of exotic mesons, which don't have the conventional valence quark content of one quark and one antiquark.
Because quarks have a spin of , the difference in quark-number between mesons and baryons results in conventional two-quark mesons being bosons, whereas baryons are fermions.
Each type of meson has a corresponding antiparticle (antimeson) in which quarks are replaced by their corresponding antiquarks and vice versa. For example, a positive pion () is made of one up quark and one down antiquark; and its corresponding antiparticle, the negative pion (), is made of one up antiquark and one down quark.
Because mesons are composed of quarks, they participate in both the weak and strong interactions. Mesons with net electric charge also participate in the electromagnetic interaction. They are classified according to their quark content, total angular momentum, parity and various other properties, such as C-parity and G-parity. Although no meson is stable, those of lower mass are nonetheless more stable than the most massive mesons, and are easier to observe and study in particle accelerators or in cosmic ray experiments. They are also typically less massive than baryons, meaning that they are more easily produced in experiments, and thus exhibit certain higher energy phenomena more readily than baryons composed of the same quarks would. For example, the charm quark was first seen in the J/Psi meson () in 1974, and the bottom quark in the upsilon meson () in 1977.
History.
From theoretical considerations, in 1934 Hideki Yukawa predicted the existence and the approximate mass of the "meson" as the carrier of the nuclear force that holds atomic nuclei together. If there were no nuclear force, all nuclei with two or more protons would fly apart because of the electromagnetic repulsion. Yukawa called his carrier particle the meson, from μέσος "mesos", the Greek word for "intermediate," because its predicted mass was between that of the electron and that of the proton, which has about 1,836 times the mass of the electron. Yukawa had originally named his particle the "mesotron", but he was corrected by the physicist Werner Heisenberg (whose father was a professor of Greek at the University of Munich). Heisenberg pointed out that there is no "tr" in the Greek word "mesos".
The first candidate for Yukawa's meson, now known in modern terminology as the muon, was discovered in 1936 by Carl David Anderson and others in the decay products of cosmic ray interactions. The mu meson had about the right mass to be Yukawa's carrier of the strong nuclear force, but over the course of the next decade, it became evident that it was not the right particle. It was eventually found that the "mu meson" did not participate in the strong nuclear interaction at all, but rather behaved like a heavy version of the electron, and was eventually classed as a lepton like the electron, rather than a meson. Physicists in making this choice decided that properties other than particle mass should control their classification.
There were years of delays in the subatomic particle research during World War II in 1939–45, with most physicists working in applied projects for wartime necessities. When the war ended in August 1945, many physicists gradually returned to peacetime research. The first true meson to be discovered was what would later be called the "pi meson" (or pion). This discovery was made in 1947, by Cecil Powell, César Lattes, and Giuseppe Occhialini, who were investigating cosmic ray products at the University of Bristol in England, based on photographic films placed in the Andes mountains. Some mesons in these films had about the same mass as the already-known meson, yet seemed to decay into it, leading physicist Robert Marshak to hypothesize in 1947 that it was actually a new and different meson. Over the next few years, more experiments showed that the pion was indeed involved in strong interactions. The pion (as a virtual particle) is the primary force carrier for the nuclear force in atomic nuclei. Other mesons, such as the rho mesons are involved in mediating this force as well, but to lesser extents. Following the discovery of the pion, Yukawa was awarded the 1949 Nobel Prize in Physics for his predictions.
The word "meson" has at times been used to mean "any" force carrier, such as the "Z0 meson", which is involved in mediating the weak interaction. However, this spurious usage has fallen out of favor. Mesons are now defined as particles composed of pairs of quarks and antiquarks.
Overview.
Spin, orbital angular momentum, and total angular momentum.
Spin (quantum number S) is a vector quantity that represents the "intrinsic" angular momentum of a particle. It comes in increments of  ħ. The ħ is often dropped because it is the "fundamental" unit of spin, and it is implied that "spin 1" means "spin 1 ħ". (In some systems of natural units, ħ is chosen to be 1, and therefore does not appear in equations).
Quarks are fermions—specifically in this case, particles having spin ("S" = ). Because spin projections vary in increments of 1 (that is 1 ħ), a single quark has a spin vector of length , and has two spin projections ("S"z = + and "S"z = −). Two quarks can have their spins aligned, in which case the two spin vectors add to make a vector of length "S" = 1 and three spin projections ("S"z = +1, "S"z = 0, and "S"z = −1), called the spin-1 triplet. If two quarks have unaligned spins, the spin vectors add up to make a vector of length S = 0 and only one spin projection ("S"z = 0), called the spin-0 singlet. Because mesons are made of one quark and one antiquark, they can be found in triplet and singlet spin states.
There is another quantity of quantized angular momentum, called the orbital angular momentum (quantum number "L"), that comes in increments of 1 ħ, which represent the angular momentum due to quarks orbiting around each other. The total angular momentum (quantum number "J") of a particle is therefore the combination of intrinsic angular momentum (spin) and orbital angular momentum. It can take any value from to , in increments of 1.
Particle physicists are most interested in mesons with no orbital angular momentum ("L" = 0), therefore the two groups of mesons most studied are the "S" = 1; "L" = 0 and "S" = 0; "L" = 0, which corresponds to "J" = 1 and "J" = 0, although they are not the only ones. It is also possible to obtain "J" = 1 particles from "S" = 0 and "L" = 1. How to distinguish between the "S" = 1, "L" = 0 and "S" = 0, "L" = 1 mesons is an active area of research in meson spectroscopy.
Parity.
If the universe were reflected in a mirror, most of the laws of physics would be identical—things would behave the same way regardless of what we call "left" and what we call "right". This concept of mirror reflection is called parity ("P"). Gravity, the electromagnetic force, and the strong interaction all behave in the same way regardless of whether or not the universe is reflected in a mirror, and thus are said to conserve parity (P-symmetry). However, the weak interaction does" "distinguish "left" from "right", a phenomenon called parity violation (P-violation).
Based on this, one might think that, if the wavefunction for each particle (more precisely, the quantum field for each particle type) were simultaneously mirror-reversed, then the new set of wavefunctions would perfectly satisfy the laws of physics (apart from the weak interaction). It turns out that this is not quite true: In order for the equations to be satisfied, the wavefunctions of certain types of particles have to be multiplied by −1, in addition to being mirror-reversed. Such particle types are said to have "negative" or "odd" parity ("P" = −1, or alternatively "P" = –), whereas the other particles are said to have "positive" or "even" parity ("P" = +1, or alternatively "P" = +).
For mesons, the parity is related to the orbital angular momentum by the relation:
where the "L" is a result of the parity of the corresponding spherical harmonic of the wavefunction. The '+1' in the exponent comes from the fact that, according to the Dirac equation, a quark and an antiquark have opposite intrinsic parities. Therefore, the intrinsic parity of a meson is the product of the intrinsic parities of the quark (+1) and antiquark (−1). As these are different, their product is −1, and so it contributes a +1 in the exponent.
As a consequence, mesons with no orbital angular momentum ("L" = 0) all have odd parity ("P" = −1).
C-parity.
C-parity is only defined for mesons that are their own antiparticle (i.e. neutral mesons). It represents whether or not the wavefunction of the meson remains the same under the interchange of their quark with their antiquark. If
then, the meson is "C even" (C = +1). On the other hand, if
then the meson is "C odd" (C = −1).
C-parity rarely is studied on its own, but more commonly in combination with P-parity into CP-parity. CP-parity was thought to be conserved, but was later found to be violated in weak interactions.
G-parity.
G parity is a generalization of the C-parity. Instead of simply comparing the wavefunction after exchanging quarks and antiquarks, it compares the wavefunction after exchanging the meson for the corresponding antimeson, regardless of quark content. In the case of neutral meson, G-parity is equivalent to C-parity because neutral mesons are their own antiparticles.
If
then, the meson is "G even" (G = +1). On the other hand, if
then the meson is "G odd" (G = −1).
Isospin and charge.
The concept of isospin was first proposed by Werner Heisenberg in 1932 to explain the similarities between protons and neutrons under the strong interaction. Although they had different electric charges, their masses were so similar that physicists believed that they were actually the same particle. The different electric charges were explained as being the result of some unknown excitation similar to spin. This unknown excitation was later dubbed "isospin" by Eugene Wigner in 1937. When the first mesons were discovered, they too were seen through the eyes of isospin and so the three pions were believed to be the same particle, but in different isospin states.
This belief lasted until Murray Gell-Mann proposed the quark model in 1964 (containing originally only the u, d, and s quarks). The success of the isospin model is now understood to be the result of the similar masses of the u and d quarks. Because the u and d quarks have similar masses, particles made of the same number of them also have similar masses. The exact specific u and d quark composition determines the charge, because u quarks carry charge + whereas d quarks carry charge −. For example the three pions all have different charges ( (), (a quantum superposition of and states), ()), but have similar masses (~) as they are each made of a same number of total of up and down quarks and antiquarks. Under the isospin model, they were considered to be a single particle in different charged states.
The mathematics of isospin was modeled after that of spin. Isospin projections varied in increments of 1 just like those of spin, and to each projection was associated a "charged state". Because the "pion particle" had three "charged states", it was said to be of isospin "I" = 1. Its "charged states" , , and , corresponded to the isospin projections "I"3 = +1, "I"3 = 0, and "I"3 = −1 respectively. Another example is the "rho particle", also with three charged states. Its "charged states" , , and , corresponded to the isospin projections "I"3 = +1, "I"3 = 0, and "I"3 = −1 respectively. It was later noted that the isospin projections were related to the up and down quark content of particles by the relation
where the "n"'s are the number of up and down quarks and antiquarks.
In the "isospin picture", the three pions and three rhos were thought to be the different states of two particles. However, in the quark model, the rhos are excited states of pions. Isospin, although conveying an inaccurate picture of things, is still used to classify hadrons, leading to unnatural and often confusing nomenclature. Because mesons are hadrons, the isospin classification is also used, with "I"3 = + for up quarks and down antiquarks, and "I"3 = − for up antiquarks and down quarks.
Flavour quantum numbers.
The strangeness quantum number "S" (not to be confused with spin) was noticed to go up and down along with particle mass. The higher the mass, the lower the strangeness (the more s quarks). Particles could be described with isospin projections (related to charge) and strangeness (mass) (see the uds nonet figures). As other quarks were discovered, new quantum numbers were made to have similar description of udc and udb nonets. Because only the u and d mass are similar, this description of particle mass and charge in terms of isospin and flavour quantum numbers only works well for the nonets made of one u, one d and one other quark and breaks down for the other nonets (for example ucb nonet). If the quarks all had the same mass, their behaviour would be called "symmetric", because they would all behave in exactly the same way with respect to the strong interaction. However, as quarks do not have the same mass, they do not interact in the same way (exactly like an electron placed in an electric field will accelerate more than a proton placed in the same field because of its lighter mass), and the symmetry is said to be broken.
It was noted that charge ("Q") was related to the isospin projection ("I"3), the baryon number ("B") and flavour quantum numbers ("S", "C", "B"′, "T") by the Gell-Mann–Nishijima formula:
where "S", "C", "B"′, and "T" represent the strangeness, charm, bottomness and topness flavour quantum numbers respectively. They are related to the number of strange, charm, bottom, and top quarks and antiquark according to the relations:
meaning that the Gell-Mann–Nishijima formula is equivalent to the expression of charge in terms of quark content:
Classification.
Mesons are classified into groups according to their isospin ("I"), total angular momentum ("J"), parity ("P"), G-parity ("G") or C-parity ("C") when applicable, and quark (q) content. The rules for classification are defined by the Particle Data Group, and are rather convoluted. The rules are presented below, in table form for simplicity.
Types of meson.
Mesons are classified into types according to their spin configurations. Some specific configurations are given special names based on the mathematical properties of their spin configuration.
Nomenclature.
Flavourless mesons.
Flavourless mesons are mesons made of pair of quark and antiquarks of the same flavour (all their flavour quantum numbers are zero: "S" = 0, "C" = 0, "B"′ = 0, "T" = 0). The rules for flavourless mesons are:
† The C parity is only relevant to neutral mesons.<br>
†† For "J""PC"=1−−, the ψ is called the 
In addition:
Flavoured mesons.
Flavoured mesons are mesons made of pair of quark and antiquarks of different flavours. The rules are simpler in this case: the main symbol depends on the heavier quark, the superscript depends on the charge, and the subscript (if any) depends on the lighter quark. In table form, they are:
In addition:
Exotic mesons.
There is experimental evidence for particles that are hadrons (i.e., are composed of quarks) and are color-neutral with zero baryon number, and thus by conventional definition are mesons. Yet, these particles do not consist of a single quark-antiquark pair, as all the other conventional mesons discussed above do. A tentative category for these particles is exotic mesons.
There are at least five exotic meson resonances that have been experimentally confirmed to exist by two or more independent experiments. The most statistically significant of these is the Z(4430), discovered by the Belle experiment in 2007 and confirmed by LHCb in 2014. It is a candidate for being a tetraquark: a particle composed of two quarks and two antiquarks. See the main article above for other particle resonances that are candidates for being exotic mesons.

</doc>
<doc id="1561467" url="https://en.wikipedia.org/wiki?curid=1561467" title="Metric (mathematics)">
Metric (mathematics)

In mathematics, a metric or distance function is a function that defines a distance between each pair of elements of a set. A set with a metric is called a metric space. A metric induces a topology on a set, but not all topologies can be generated by a metric. A topological space whose topology can be described by a metric is called metrizable.
In differential geometry, the word "metric" may refer to a bilinear form that may be defined from the tangent vectors of a differentiable manifold onto a scalar, allowing distances along curves to be determined through integration. It is more properly termed a metric tensor.
Definition.
A metric on a set "X" is a function (called the "distance function" or simply distance)
where [0,∞) is the set of non-negative real numbers (because distance can't be negative so we can't use R), and for all "x", "y", "z" in "X", the following conditions are satisfied:
Conditions 1 and 2 together define a "positive-definite function". 
The first condition is implied by the others.
A metric is called an ultrametric if it satisfies the following stronger version of the "triangle inequality" where points can never fall 'between' other points:
for all "x", "y", "z" in "X".
A metric "d" on "X" is called intrinsic if any two points "x" and "y" in "X" can be joined by a curve with length arbitrarily close to "d"("x", "y").
For sets on which an addition + : "X" × "X" → "X" is defined,
"d" is called a translation invariant metric if
for all "x", "y" and "a" in "X".
Notes.
These conditions express intuitive notions about the concept of distance. For example, that the distance between distinct points is positive and the distance from "x" to "y" is the same as the distance from "y" to "x". The triangle inequality means that the distance from "x" to "z" via "y" is at least as great as from "x" to "z" directly. Euclid in his work stated that the shortest distance between two points is a line; that was the triangle inequality for his geometry.
If a modification of the triangle inequality
is used in the definition then property 1 follows straight from property 4*. Properties 2 and 4* give property 3 which in turn gives property 4.
Equivalence of metrics.
For a given set "X", two metrics "d"1 and "d"2 are called topologically equivalent (uniformly equivalent) if the identity mapping
is a homeomorphism (uniform isomorphism).
For example, if formula_5 is a metric, then formula_6 and formula_7 are metrics equivalent to formula_8
See also notions of metric space equivalence.
Metrics on vector spaces.
Norms on vector spaces are equivalent to certain metrics, namely homogeneous, translation-invariant ones. In other words, every norm determines a metric, and some metrics determine a norm.
Given a normed vector space formula_9 we can define a metric on "X" by
The metric "d" is said to be induced by the norm formula_11.
Conversely if a metric "d" on a vector space "X" satisfies the properties
then we can define a norm on "X" by
Similarly, a seminorm induces a pseudometric (see below), and a homogeneous, translation invariant pseudometric induces a seminorm.
Metrics on multisets.
We can generalize the notion of a metric from a distance between two elements to a distance between two nonempty finite multisets of elements. A multiset is a generalization of the notion of a set such that an element can occur more than once. Define formula_15 if
formula_16 is the multiset consisting of the elements of the multisets formula_17 and formula_18, that is, if formula_19 occurs once in formula_17 and once in formula_18 then it occurs twice in formula_16. 
A distance function
formula_5 on the set of nonempty finite multisets is a metric if
Note that the familiar metric between two elements results if the multiset formula_17 has two elements in 1 and 2 and the multisets formula_31 have one element each in 3. For instance if formula_17 consists of two occurrences of formula_19, then formula_24 according to 1.
A simple example is the set of all nonempty finite multisets formula_17 of integers with formula_36. More complex examples are information distance in multisets; and normalized compression distance (NCD) in multisets.
Generalized metrics.
There are numerous ways of relaxing the axioms of metrics, giving rise to various notions of generalized metric spaces. These generalizations can also be combined. The terminology used to describe them is not completely standardized. Most notably, in functional analysis pseudometrics often come from seminorms on vector spaces, and so it is natural to call them "semimetrics". This conflicts with the use of the term in topology.
Extended metrics.
Some authors allow the distance function "d" to attain the value ∞, i.e. distances are non-negative numbers on the extended real number line. 
Such a function is called an "extended metric" or "∞-metric". 
Every extended metric can be transformed to a finite metric such that the metric spaces are equivalent as far as notions of topology (such as continuity or convergence) are concerned. This can be done using a subadditive monotonically increasing bounded function which is zero at zero, e.g. "d"′("x", "y") = "d"("x", "y") / (1 + "d"("x", "y")) or "d"′′("x", "y") = min(1, "d"("x", "y")).
The requirement that the metric take values in [0,∞) can even be relaxed to consider metrics with values in other directed sets. The reformulation of the axioms in this case leads to the construction of uniform spaces: topological spaces with an abstract structure enabling one to compare the local topologies of different points.
Pseudometrics.
A pseudometric on "X" is a function "d" : "X" × "X" → R which satisfies the axioms for a metric, except that instead of the second (identity of indiscernibles) only "d"("x","x")=0 for all "x" is required. In other words, the axioms for a pseudometric are:
In some contexts, pseudometrics are referred to as "semimetrics" because of their relation to seminorms.
Quasimetrics.
Occasionally, a quasimetric is defined as a function that satisfies all axioms for a metric with the possible exception of symmetry:
Quasimetrics are common in real life. For example, given a set "X" of mountain villages, the typical walking times between elements of "X" form a quasimetric because travel up hill takes longer than travel down hill. Another example is a taxicab geometry topology having one-way streets, where a path from point "A" to point "B" comprises a different set of streets than a path from "B" to "A". Nevertheless, this notion is rarely used in mathematics, and its name is not entirely standardized.
A quasimetric on the reals can be defined by setting
The topological space underlying this quasimetric space is the Sorgenfrey line. This space describes the process of filing down a metal stick: it is easy to reduce its size, but it is difficult or impossible to grow it.
If "d" is a quasimetric on "X", a metric "d'" on "X" can be formed by taking
Semimetrics.
A semimetric on "X" is a function "d" : "X" × "X" → R that satisfies the first three axioms, but not necessarily the triangle inequality:
Some authors work with a weaker form of the triangle inequality, such as:
The ρ-inframetric inequality implies the ρ-relaxed triangle inequality (assuming the first axiom), and the ρ-relaxed triangle inequality implies the 2ρ-inframetric inequality. Semimetrics satisfying these equivalent conditions have sometimes been referred to as "quasimetrics", "nearmetrics" or inframetrics.
The ρ-inframetric inequalities were introduced to model round-trip delay times in the internet. The triangle inequality implies the 2-inframetric inequality, and the ultrametric inequality is exactly the 1-inframetric inequality.
Premetrics.
Relaxing the last three axioms leads to the notion of a premetric, i.e. a function satisfying the following conditions:
This is not a standard term. Sometimes it is used to refer to other generalizations of metrics such as pseudosemimetrics or pseudometrics; in translations of Russian books it sometimes appears as "prametric".
Any premetric gives rise to a topology as follows. For a positive real "r", the "open" "r"-ball centred at a point "p" is defined as
A set is called "open" if for any point "p" in the set there is an "open" "r"-ball centred at "p" which is contained in the set. Every premetric space is a topological space, and in fact a sequential space.
In general, the "open" "r"-balls themselves need not be open sets with respect to this topology. 
As for metrics, the distance between two sets "A" and "B", is defined as
This defines a premetric on the power set of a premetric space. If we start with a (pseudosemi-)metric space, we get a pseudosemimetric, i.e. a symmetric premetric.
Any premetric gives rise to a preclosure operator "cl" as follows:
Pseudoquasimetrics.
The prefixes "pseudo-", "quasi-" and "semi-" can also be combined, e.g., a pseudoquasimetric (sometimes called hemimetric) relaxes both the indiscernibility axiom and the symmetry axiom and is simply a premetric satisfying the triangle inequality. For pseudoquasimetric spaces the open "r"-balls form a basis of open sets. A very basic example of a pseudoquasimetric space is the set {0,1} with the premetric given by "d"(0,1) = 1 and "d"(1,0) = 0. The associated topological space is the Sierpiński space.
Sets equipped with an extended pseudoquasimetric were studied by William Lawvere as "generalized metric spaces". From a categorical point of view, the extended pseudometric spaces and the extended pseudoquasimetric spaces, along with their corresponding nonexpansive maps, are the best behaved of the metric space categories. One can take arbitrary products and coproducts and form quotient objects within the given category. If one drops "extended", one can only take finite products and coproducts. If one drops "pseudo", one cannot take quotients. Approach spaces are a generalization of metric spaces that maintains these good categorical properties.
Important cases of generalized metrics.
In differential geometry, one considers a metric tensor, which can be thought of as an "infinitesimal" quadratic metric function. This is defined as a nondegenerate symmetric bilinear form on the tangent space of a manifold with an appropriate differentiability requirement. While these are not metric functions as defined in this article, they induce what is called a pseudo-semimetric function by integration of its square root along a path through the manifold. If one imposes the positive-definiteness requirement of an inner product on the metric tensor, this restricts to the case of a Riemannian manifold, and the path integration yields a metric.
In general relativity the related concept is a metric tensor (general relativity) which expresses the structure of a pseudo-Riemannian manifold. Though the term "metric" is used in cosmology, the fundamental idea is different because there are non-zero null vectors in the tangent space of these manifolds. This generalized view of "metrics", in which zero distance does "not" imply identity, has crept into some mathematical writing too:

</doc>
<doc id="463225" url="https://en.wikipedia.org/wiki?curid=463225" title="Metric signature">
Metric signature

The signature of a metric tensor "g" (or equivalently, a real quadratic form thought of as a real symmetric bilinear form on a finite-dimensional vector space) is the number (counted with multiplicity) of positive, negative and zero eigenvalues of the real symmetric matrix of the metric tensor with respect to a basis. Alternatively, it can be defined as the dimensions of a maximal positive, negative and null subspace. By Sylvester's law of inertia these numbers do not depend on the choice of basis. The signature thus classifies the metric up to a choice of basis. The signature is often denoted by a pair of integers implying "r" = 0 or as an explicit list of signs of eigenvalues such as or for the signature resp. .
The signature is said to be indefinite or mixed if both "p" and "q" are nonzero, and degenerate if "r" is nonzero. A Riemannian metric is a metric with a (positive) definite signature. A Lorentzian metric is one with signature , or .
There is another notion of signature of a nondegenerate metric tensor given by a single number "s" defined as , where "p" and "q" are as above, which is equivalent to the above definition when the dimension "n" = "p" + "q" is given or implicit. For example, "s" = 1 − 3 = −2 for and "s" = 3 − 1 = +2 for .
Definition.
The signature of a metric tensor is defined as the signature of the corresponding quadratic form. It is the number of positive, negative and zero eigenvalues of any matrix (i.e. in any basis for the underlying vector space) representing the form, counted with their algebraic multiplicity. Usually, is required, which is the same as saying a metric tensor must be nondegenerate, i.e. no nonzero vector is orthogonal to all vectors.
By Sylvester's law of inertia, the numbers are basis independent.
Properties.
Signature and dimension.
By the spectral theorem a symmetric "n" × "n" matrix over the reals is always diagonalizable, and has therefore exactly "n" real eigenvalues (counted with algebraic multiplicity). Thus .
Sylvester's law of inertia: independence of basis choice and existence of orthonormal basis.
According to Sylvester's law of inertia, the signature of the scalar product (a.k.a. real symmetric bilinear form), "g" does not depend on the choice of basis. Moreover, for every metric "g" of signature there exists a basis such that 
 for , for and otherwise. It follows that there exists an isometry if and only if the signatures of "g"1 and "g"2 are equal. Likewise the signature is equal for two congruent matrices and classifies a matrix up to congruency. Equivalently, the signature is constant on the orbits of the general linear group GL("V") on the space of symmetric rank 2 contravariant tensors "S"2"V"∗ and classifies each orbit.
Geometrical interpretation of the indices.
The number "p" (resp. "q") is the maximal dimension of a vector subspace on which the scalar product "g" is positive-definite (resp. negative-definite), and "r" is the dimension of the radical of the scalar product "g" or the null subspace of symmetric matrix of the scalar product. Thus a nondegenerate scalar product has signature , with . The special cases and correspond to positive-definite and negative-definite scalar products which can be transformed into each other by negation.
Examples.
Matrices.
The signature of the identity matrix is . The signature of a diagonal matrix is the number of positive, negative and zero numbers on its main diagonal.
The following matrices have both the same signature , therefore they are congruent because of Sylvester's law of inertia:
Scalar products.
The standard scalar product defined on formula_2 has the signature . A scalar product has this signature if and only if it is a positive definite scalar product.
A negative definite scalar product has the signature . A positive semi-definite scalar product has a signature , where .
The Minkowski space is formula_3 and has a scalar product defined by the matrix
and has signature , which is known as space-positive.
Sometimes it is used with the opposite signs, thus obtaining the signature , which is known as time-positive.
How to compute the signature.
There are some methods for computing the signature of a matrix.
Signature in physics.
In mathematics, the usual convention for any Riemannian manifold is to use a positive-definite metric tensor (meaning that after diagonalization, elements on the diagonal are all positive).
In theoretical physics, spacetime is modeled by a pseudo-Riemannian manifold. The signature counts how many time-like or space-like characters are in the spacetime, in the sense defined by special relativity: as used in particle physics, the metric is positive definite on the time-like subspace, and negative definite on the space-like subspace.
In the specific case of the Minkowski metric,
the metric signature is , since it is positive definite in the time direction, and negative definite in the three spatial directions "x", "y" and "z".
Signature change.
If a metric is regular everywhere then the signature of the metric is constant. However if one allows for metrics that are degenerate or discontinuous on some hypersurfaces, then signature of the metric may change at these surfaces. Such signature changing metrics may possibly have applications in cosmology and quantum gravity.

</doc>
<doc id="195795" url="https://en.wikipedia.org/wiki?curid=195795" title="Metric tensor">
Metric tensor

In the mathematical field of differential geometry, a metric tensor is a type of function which takes as input a pair of tangent vectors "v" and "w" at a point of a surface (or higher dimensional differentiable manifold) and produces a real number scalar in a way that generalizes many of the familiar properties of the dot product of vectors in Euclidean space. In the same way as a dot product, metric tensors are used to define the length of and angle between tangent vectors.
A metric tensor is called "positive-definite" if it assigns a positive value to every nonzero vector. A manifold equipped with a positive-definite metric tensor is known as a Riemannian manifold. Through integration, the metric tensor allows one to define and compute the length of curves on the manifold. On a Riemannian manifold, the curve connecting two points that (locally) has the smallest length is called a geodesic, and its length is the distance that a passenger in the manifold needs to traverse to go from one point to the other. Equipped with this notion of length, a Riemannian manifold is a metric space, meaning that it has a distance function whose value at a pair of points "p" and "q" is the distance from "p" to "q". Conversely, the metric tensor itself is the derivative of the distance function (taken in a suitable manner). Thus the metric tensor gives the "infinitesimal" distance on the manifold.
While the notion of a metric tensor was known in some sense to mathematicians such as Carl Gauss from the early 19th century, it was not until the early 20th century that its properties as a tensor were understood by, in particular, Gregorio Ricci-Curbastro and Tullio Levi-Civita, who first codified the notion of a tensor. The metric tensor is an example of a tensor field.
The components of a metric tensor in a coordinate basis take on the form of a symmetric matrix whose entries transform covariantly under changes to the coordinate system. Thus a metric tensor is a covariant symmetric tensor. From the coordinate-independent point of view, a metric tensor is defined to be a nondegenerate symmetric bilinear form on each tangent space that varies smoothly from point to point.
Introduction.
Carl Friedrich Gauss in his 1827 "Disquisitiones generales circa superficies curvas" ("General investigations of curved surfaces") considered a surface parametrically, with the Cartesian coordinates "x", "y", and "z" of points on the surface depending on two auxiliary variables "u" and "v". Thus a parametric surface is (in today's terms) a vector valued function
depending on an ordered pair of real variables ("u","v"), and defined in an open set "D" in the "uv"-plane. One of the chief aims of Gauss' investigations was to deduce those features of the surface which could be described by a function which would remain unchanged if the surface underwent a transformation in space (such as bending the surface without stretching it), or a change in the particular parametric form of the same geometrical surface.
One natural such invariant quantity is the length of a curve drawn along the surface. Another is the angle between a pair of curves drawn along the surface and meeting at a common point. A third such quantity is the area of a piece of the surface. The study of these invariants of a surface led Gauss to introduce the predecessor of the modern notion of the metric tensor.
Arclength.
If the variables "u" and "v" are taken to depend on a third variable, "t", taking values in an interval , then formula_2 will trace out a parametric curve in parametric surface "M". The arclength of that curve is given by the integral
where formula_4 represents the Euclidean norm. Here the chain rule has been applied, and the subscripts denote partial derivatives (formula_5, formula_6). The integrand is the restriction to the curve of the square root of the (quadratic) differential
where
The quantity "ds" in () is called the line element, while "ds"2 is called the first fundamental form of "M". Intuitively, it represents the principal part of the square of the displacement undergone by formula_7 when "u" is increased by "du" units, and "v" is increased by "dv" units.
Using matrix notation, the first fundamental form becomes
Coordinate transformations.
Suppose now that a different parameterization is selected, by allowing "u" and "v" to depend on another pair of variables "u"′ and "v"′. Then the analog of () for the new variables is
The chain rule relates "E"′, "F"′, and "G"′ to "E", "F", and "G" via the matrix equation
where the superscript T denotes the matrix transpose. The matrix with the coefficients "E", "F", and "G" arranged in this way therefore transforms by the Jacobian matrix of the coordinate change
A matrix which transforms in this way is one kind of what is called a tensor. The matrix
with the transformation law () is known as the metric tensor of the surface.
Invariance of arclength under coordinate transformations.
 first observed the significance of a system of coefficients "E", "F", and "G", that transformed in this way on passing from one system of coordinates to another. The upshot is that the first fundamental form () is "invariant" under changes in the coordinate system, and that this follows exclusively from the transformation properties of "E", "F", and "G". Indeed, by the chain rule,
so that
Length and angle.
Another interpretation of the metric tensor, also considered by Gauss, is that it provides a way in which to compute the length of tangent vectors to the surface, as well as the angle between two tangent vectors. In contemporary terms, the metric tensor allows one to compute the dot product of tangent vectors in a manner independent of the parametric description of the surface. Any tangent vector at a point of the parametric surface "M" can be written in the form
for suitable real numbers "p"1 and "p"2. If two tangent vectors are given
then using the bilinearity of the dot product,
This is plainly a function of the four variables "a"1, "b"1, "a"2, and "b"2. It is more profitably viewed, however, as a function that takes a pair of arguments a = ["a"1 "a"2] and b = ["b"1 "b"2] which are vectors in the "uv"-plane. That is, put
This is a symmetric function in a and b, meaning that
It is also bilinear, meaning that it is linear in each variable a and b separately. That is,
for any vectors a, a′, b, and b′ in the "uv" plane, and any real numbers "μ" and "λ".
In particular, the length of a tangent vector a is given by
and the angle θ between two vectors a and b is calculated by
Area.
The surface area is another numerical quantity which should depend only on the surface itself, and not on how it is parameterized. If the surface "M" is parameterized by the function formula_23 over the domain "D" in the "uv"-plane, then the surface area of "M" is given by the integral
where × denotes the cross product, and the absolute value denotes the length of a vector in Euclidean space. By Lagrange's identity for the cross product, the integral can be written
where det is the determinant.
Definition.
Let "M" be a smooth manifold of dimension "n"; for instance a surface (in the case "n" = 2) or hypersurface in the Cartesian space R"n"+1. At each point "p" ∈ "M" there is a vector space T"p""M", called the tangent space, consisting of all tangent vectors to the manifold at the point "p". A metric at "p" is a function "g""p"("X""p","Y""p") which takes as inputs a pair of tangent vectors "X""p" and "Y""p" at "p", and produces as an output a real number (scalar), so that the following conditions are satisfied:
A metric tensor "g" on "M" assigns to each point "p" of "M" a metric "g""p" in the tangent space at "p" in a way that varies smoothly with "p". More precisely, given any open subset "U" of manifold "M" and any (smooth) vector fields "X" and "Y" on "U", the real function
is a smooth function of "p".
Components of the metric.
The components of the metric in any basis of vector fields, or frame, f = ("X"1, ..., "X""n") are given by
The "n"2 functions "g""ij"[f] form the entries of an "n"×"n" symmetric matrix, "G"[f]. If
are two vectors at "p" ∈ "U", then the value of the metric applied to "v" and "w" is determined by the coefficients () by bilinearity:
Denoting the matrix ("g""ij"[f]) by "G"[f] and arranging the components of the vectors "v" and "w" into column vectors v[f] and w[f],
where v[f]T and w[f]T denote the transpose of the vectors v[f] and w[f], respectively. Under a change of basis of the form
for some invertible matrix , the matrix of components of the metric changes by "A" as well. That is,
or, in terms of the entries of this matrix,
For this reason, the system of quantities "g""ij"[f] is said to transform covariantly with respect to changes in the frame f.
Metric in coordinates.
A system of "n" real valued functions , giving a local coordinate system on an open set "U" in "M", determines a basis of vector fields on "U"
The metric "g" has components relative to this frame given by
Relative to a new system of local coordinates, say
the metric tensor will determine a different matrix of coefficients,
This new system of functions is related to the original "g""ij"(f) by means of the chain rule
so that
Or, in terms of the matrices "G"[f] = ("g""ij"[f]) and "G"[f′] = ("g""ij"[f′]),
where "Dy" denotes the Jacobian matrix of the coordinate change.
Signature of a metric.
Associated to any metric tensor is the quadratic form defined in each tangent space by
If "q""m" is positive for all non-zero "X""m", then the metric is positive definite at "m". If the metric is positive definite at every , then "g" is called a Riemannian metric. More generally, if the quadratic forms "q""m" have constant signature independent of "m", then the signature of "g" is this signature, and "g" is called a pseudo-Riemannian metric. If "M" is connected, then the signature of "q"m does not depend on "m".
By Sylvester's law of inertia, a basis of tangent vectors "X""i" can be chosen locally so that the quadratic form diagonalizes in the following manner
for some "p" between 1 and "n". Any two such expressions of "q" (at the same point "m" of "M") will have the same number "p" of positive signs. The signature of "g" is the pair of integers , signifying that there are "p" positive signs and negative signs in any such expression. Equivalently, the metric has signature if the matrix "g""ij" of the metric has "p" positive and negative eigenvalues.
Certain metric signatures which arise frequently in applications are:
Inverse metric.
Let be a basis of vector fields, and as above let "G"[f] be the matrix of coefficients
One can consider the inverse matrix "G"[f]−1, which is identified with the inverse metric (or "conjugate" or "dual metric"). The inverse metric satisfies a transformation law when the frame f is changed by a matrix "A" via
The inverse metric transforms "contravariantly", or with respect to the inverse of the change of basis matrix "A". Whereas the metric itself provides a way to measure the length of (or angle between) vector fields, the inverse metric supplies a means of measuring the length of (or angle between) covector fields; that is, fields of linear functionals.
To see this, suppose that "α" is a covector field. To wit, for each point "p", "α" determines a function "α""p" defined on tangent vectors at "p" so that the following linearity condition holds for all tangent vectors "X""p" and "Y""p", and all real numbers "a" and "b":
As "p" varies, "α" is assumed to be a smooth function in the sense that
is a smooth function of "p" for any smooth vector field "X".
Any covector field "α" has components in the basis of vector fields f. These are determined by
Denote the row vector of these components by
Under a change of f by a matrix "A", "α"[f] changes by the rule
That is, the row vector of components "α"[f] transforms as a "covariant" vector.
For a pair "α" and "β" of covector fields, define the inverse metric applied to these two covectors by
The resulting definition, although it involves the choice of basis f, does not actually depend on f in an essential way. Indeed, changing basis to f"A" gives
So that the right-hand side of equation () is unaffected by changing the basis f to any other basis f"A" whatsoever. Consequently, the equation may be assigned a meaning independently of the choice of basis. The entries of the matrix "G"[f] are denoted by "g""ij", where the indices "i" and "j" have been raised to indicate the transformation law ().
Raising and lowering indices.
In a basis of vector fields , any smooth tangent vector field "X" can be written in the form
for some uniquely determined smooth functions . Upon changing the basis f by a nonsingular matrix "A", the coefficients "v""i" change in such a way that equation () remains true. That is,
Consequently, . In other words, the components of a vector transform "contravariantly" (with respect to the inverse) under a change of basis by the nonsingular matrix "A". The contravariance of the components of "v"[f] is notationally designated by placing the indices of "v""i"[f] in the upper position.
A frame also allows covectors to be expressed in terms of their components. For the basis of vector fields define the dual basis to be the linear functionals such that
That is, , the Kronecker delta. Let
Under a change of basis for a nonsingular matrix "A", "θ"[f] transforms via
Any linear functional α on tangent vectors can be expanded in terms of the dual basis θ
where "a"[f] denotes the row vector . The components "a""i" transform when the basis f is replaced by f"A" in such a way that equation () continues to hold. That is,
whence, because , it follows that . That is, the components "a" transform "covariantly" (by the matrix "A" rather than its inverse). The covariance of the components of "a"[f] is notationally designated by placing the indices of "a""i"[f] in the lower position.
Now, the metric tensor gives a means to identify vectors and covectors as follows. Holding "X""p" fixed, the function
of tangent vector "Y""p" defines a linear functional on the tangent space at "p". This operation takes a vector "X""p" at a point "p" and produces a covector . In a basis of vector fields f, if a vector field "X" has components "v"[f], then the components of the covector field in the dual basis are given by the entries of the row vector
Under a change of basis , the right-hand side of this equation transforms via
so that : "a" transforms covariantly. The operation of associating to the (contravariant) components of a vector field the (covariant) components of the covector field , where
is called lowering the index.
To "raise the index", one applies the same construction but with the inverse metric instead of the metric. If are the components of a covector in the dual basis "θ"[f], then the column vector
has components which transform contravariantly:
Consequently, the quantity does not depend on the choice of basis f in an essential way, and thus defines a vector field on "M". The operation () associating to the (covariant) components of a covector "a"[f] the (contravariant) components of a vector "v"[f] given is called raising the index. In components, () is
Induced metric.
Let "U" be an open set in R"n", and let "φ" be a continuously differentiable function from "U" into the Euclidean space R"m", where . The mapping "φ" is called an immersion if its differential is injective at every point of "U". The image of "φ" is called an immersed submanifold.
Suppose that "φ" is an immersion onto the submanifold . The usual Euclidean dot product in R"m" is a metric which, when restricted to vectors tangent to "M", gives a means for taking the dot product of these tangent vectors. This is called the induced metric.
Suppose that "v" is a tangent vector at a point of "U", say
where ei are the standard coordinate vectors in R"n". When "φ" is applied to "U", the vector "v" goes over to the vector tangent to "M" given by
(This is called the pushforward of "v" along "φ".) Given two such vectors, "v" and "w", the induced metric is defined by
It follows from a straightforward calculation that the matrix of the induced metric in the basis of coordinate vector fields e is given by
where "Dφ" is the Jacobian matrix:
Intrinsic definitions of a metric.
The notion of a metric can be defined intrinsically using the language of fiber bundles and vector bundles. In these terms, a metric tensor is a function
from the fiber product of the tangent bundle of "M" with itself to R such that the restriction of "g" to each fiber is a nondegenerate bilinear mapping
The mapping () is required to be continuous, and often continuously differentiable, smooth, or real analytic, depending on the case of interest, and whether "M" can support such a structure.
Metric as a section of a bundle.
By the universal property of the tensor product, any bilinear mapping () gives rise naturally to a section "g"⊗ of the dual of the tensor product bundle of T"M" with itself
The section "g"⊗ is defined on simple elements of T"M"⊗T"M" by
and is defined on arbitrary elements of T"M"⊗T"M" by extending linearly to linear combinations of simple elements. The original bilinear form "g" is symmetric if and only if
where
is the braiding map.
Since "M" is finite-dimensional, there is a natural isomorphism
so that "g"⊗ is regarded also as a section of the bundle T*"M"⊗T*"M" of the cotangent bundle T*"M" with itself. Since "g" is symmetric as a bilinear mapping, it follows that "g"⊗ is a symmetric tensor.
Metric in a vector bundle.
More generally, one may speak of a metric in a vector bundle. If "E" is a vector bundle over a manifold "M", then a metric is a mapping
from the fiber product of "E" to R which is bilinear in each fiber:
Using duality as above, a metric is often identified with a section of the tensor product bundle formula_77, (See metric (vector bundle).)
Tangent-cotangent isomorphism.
The metric tensor gives a natural isomorphism from the tangent bundle to the cotangent bundle, sometimes called the musical isomorphism. This isomorphism is obtained by setting, for each tangent vector "X""p" ∈ T"p""M",
the linear functional on T"p""M" which sends a tangent vector "Y""p" at "p" to "g""p"("X""p","Y""p"). That is, in terms of the pairing [−,−] between T"p""M" and its dual space "T""p"*"M",
for all tangent vectors "X""p" and "Y""p". The mapping "S""g" is a linear transformation from T"p""M" to T"p"*"M". It follows from the definition of non-degeneracy that the kernel of "S""g" is reduced to zero, and so by the rank-nullity theorem, "S""g" is a linear isomorphism. Furthermore, "S""g" is a symmetric linear transformation in the sense that
for all tangent vectors "X""p" and "Y""p".
Conversely, any linear isomorphism "S" : T"p""M" → T"p"*"M" defines a non-degenerate bilinear form on T"p""M" by means of
This bilinear form is symmetric if and only if "S" is symmetric. There is thus a natural one-to-one correspondence between symmetric bilinear forms on T"p""M" and symmetric linear isomorphisms of T"p""M" to the dual T"p"*"M".
As "p" varies over "M", "S""g" defines a section of the bundle Hom(T"M",T*"M") of vector bundle isomorphisms of the tangent bundle to the cotangent bundle. This section has the same smoothness as "g": it is continuous, differentiable, smooth, or real-analytic according as "g". The mapping "S""g", which associates to every vector field on "M" a covector field on "M" gives an abstract formulation of "lowering the index" on a vector field. The inverse of "S""g" is a mapping T*"M" → T"M" which, analogously, gives an abstract formulation of "raising the index" on a covector field.
The inverse "S""g"−1 defines a linear mapping
which is nonsingular and symmetric in the sense that
for all covectors α, β. Such a nonsingular symmetric mapping gives rise (by the tensor-hom adjunction) to a map
or by the double dual isomorphism to a section of the tensor product
Arclength and the line element.
Suppose that "g" is a Riemannian metric on "M". In a local coordinate system "x""i", "i" = 1,2...,"n", the metric tensor appears as a matrix, denoted here by G, whose entries are the components "g""ij" of the metric tensor relative to the coordinate vector fields.
Let formula_86 be a piecewise differentiable parametric curve in "M", for "a" ≤"t" ≤ "b". The arclength of the curve is defined by
In connection with this geometrical application, the quadratic differential form
is called the first fundamental form associated to the metric, while "ds" is the line element. When "ds"2 is pulled back to the image of a curve in "M", it represents the square of the differential with respect to arclength.
For a pseudo-Riemannian metric, the length formula above is not always defined, because the term under the square root may become negative. We generally only define the length of a curve when the quantity under the square root is always of one sign or the other. In this case, define
Note that, while these formulas use coordinate expressions, they are in fact independent of the coordinates chosen; they depend only on the metric, and the curve along which the formula is integrated.
The energy, variational principles and geodesics.
Given a segment of a curve, another frequently defined quantity is the (kinetic) energy of the curve:
This usage comes from physics, specifically, classical mechanics, where the integral "E" can be seen to directly correspond to the kinetic energy of a point particle moving on the surface of a manifold. Thus, for example, in Jacobi's formulation of Maupertuis principle, the metric tensor can be seen to correspond to the mass tensor of a moving particle.
In many cases, whenever a calculation calls for the length to be used, a similar calculation using the energy may be done as well. This often leads to simpler formulas by avoiding the need for the square-root. Thus, for example, the geodesic equations may be obtained by applying variational principles to either the length or the energy. In the latter case, the geodesic equations are seen to arise from the principle of least action: they describe the motion of a "free particle" (a particle feeling no forces) that is confined to move on the manifold, but otherwise moves freely, with constant momentum, within the manifold.
Canonical measure and volume form.
In analogy with the case of surfaces, a metric tensor on an "n"-dimensional paracompact manifold "M" gives rise to a natural way to measure the "n"-dimensional volume of subsets of the manifold. The resulting natural positive Borel measure allows one to develop a theory of integrating functions on the manifold by means of the associated Lebesgue integral.
A measure can be defined, by the Riesz representation theorem, by giving a positive linear functional Λ on the space "C"0("M") of compactly supported continuous functions on "M". More precisely, if "M" is a manifold with a (pseudo-)Riemannian metric tensor "g", then there is a unique positive Borel measure μ"g" such that for any coordinate chart ("U",φ),
for all "ƒ" supported in "U". Here det "g" is the determinant of the matrix formed by the components of the metric tensor in the coordinate chart. That Λ is well-defined on functions supported in coordinate neighborhoods is justified by Jacobian change of variables. It extends to a unique positive linear functional on "C"0("M") by means of a partition of unity.
If "M" is in addition oriented, then it is possible to define a natural volume form from the metric tensor. In a positively oriented coordinate system ("x""1"...,"x""n") the volume form is represented as
where the "dx""i" are the coordinate differentials and the wedge ∧ denotes the exterior product in the algebra of differential forms. The volume form also gives a way to integrate functions on the manifold, and this geometric integral agrees with the integral obtained by the canonical Borel measure.
Examples.
The "Euclidean metric".
The most familiar example is that of elementary Euclidean geometry: the two-dimensional Euclidean metric tensor. In the usual formula_93-formula_94 coordinates, we can write
The length of a curve reduces to the formula:
The Euclidean metric in some other common coordinate systems can be written as follows.
Polar coordinates: formula_97
So
by trigonometric identities.
In general, in a Cartesian coordinate system "x""i" on a Euclidean space, the partial derivatives formula_102 are orthonormal with respect to the Euclidean metric. Thus the metric tensor is the Kronecker delta δ"ij" in this coordinate system. The metric tensor with respect to arbitrary (possibly curvilinear) coordinates formula_103 is given by:
The round metric on a sphere.
The unit sphere in R3 comes equipped with a natural metric induced from the ambient Euclidean metric. In standard spherical coordinates formula_105, with formula_106 the co-latitude, the angle measured from the z axis, and formula_107 the angle from the x axis in the xy plane, the metric takes the form
This is usually written in the form
"Lorentzian metrics from relativity".
In flat Minkowski space (special relativity), with coordinates formula_110 the metric is
For a curve with—for example—constant time coordinate, the length formula with this metric reduces to the usual length formula. For a timelike curve, the length formula gives the proper time along the curve.
In this case, the spacetime interval is written as
The Schwarzschild metric describes the spacetime around a spherically symmetric body, such as a planet, or a black hole. With coordinates formula_113, we can write the metric as
where "G" (inside the matrix) is the gravitational constant and "M" represents the total mass-energy content of the central object.

</doc>
<doc id="2145168" url="https://en.wikipedia.org/wiki?curid=2145168" title="Metric tensor (general relativity)">
Metric tensor (general relativity)

In general relativity, the metric tensor (or simply, the metric) is the fundamental object of study. It may loosely be thought of as a generalization of the gravitational potential familiar from Newtonian gravitation. The metric captures all the geometric and causal structure of spacetime, being used to define notions such as time, distance, volume, curvature, angle, and separating the future and the past.
Notation and conventions.
Throughout this article we work with a metric signature that is mostly positive (); see sign convention. The gravitation constant "G" will be kept explicit. The summation convention, where repeated indices are automatically summed over, is employed.
Definition.
Mathematically, spacetime is represented by a 4-dimensional differentiable manifold "M" and the metric is given as a covariant, second-order, symmetric tensor on "M", conventionally denoted by "g". Moreover, the metric is required to be nondegenerate with signature (-+++). A manifold "M" equipped with such a metric is called a Lorentzian manifold.
Explicitly, the metric is a symmetric bilinear form on each tangent space of "M" which varies in a smooth (or differentiable) manner from point to point. Given two tangent vectors "u" and "v" at a point "x" in "M", the metric can be evaluated on "u" and "v" to give a real number:
This can be thought of as a generalization of the dot product in ordinary Euclidean space. This analogy is not exact, however. Unlike Euclidean space — where the dot product is positive definite — the metric gives each tangent space the structure of Minkowski space.
Local coordinates and matrix representations.
Physicists usually work in local coordinates (i.e. coordinates defined on some local patch of "M"). In local coordinates formula_2 (where formula_3 is an index which runs from 0 to 3) the metric can be written in the form
The factors formula_5 are one-form gradients of the scalar coordinate fields formula_2. The metric is thus a linear combination of tensor products of one-form gradients of coordinates. The coefficients formula_7 are a set of 16 real-valued functions (since the tensor "g" is actually a "tensor field", which is defined at all points of a spacetime manifold). In order for the metric to be symmetric we must have
giving 10 independent coefficients. If we denote the symmetric tensor product by juxtaposition (so that formula_9) we can write the metric in the form
If the local coordinates are specified, or understood from context, the metric can be written as a symmetric matrix with entries formula_7. The nondegeneracy of formula_12 means that this matrix is non-singular (i.e. has non-vanishing determinant), while the Lorentzian signature of "g" implies that the matrix has one negative and three positive eigenvalues. Note that physicists often refer to this matrix or the coordinates formula_7 themselves as the metric (see, however, abstract index notation).
With the quantity formula_5 being regarded an infinitesimal coordinate displacement, the metric determines the invariant square of an infinitesimal line element, often referred to as an "interval". The interval is often denoted
The interval formula_16 imparts information about the causal structure of spacetime. When formula_17, the interval is timelike and the square root of the absolute value of "ds"2 is an incremental proper time. Only timelike intervals can be physically traversed by a massive object. When formula_18, the interval is lightlike, and can only be traversed by light. When formula_19, the interval is spacelike and the square root of "ds"2 acts as an incremental proper length. Spacelike intervals cannot be traversed, since they connect events that are outside each other's light cones. Events can be causally related only if they are within each other's light cones.
The components of the metric depend on the choice of local coordinate system. Under a change of coordinates formula_20, the metric components transform as
Examples.
Flat spacetime.
The simplest example of a Lorentzian manifold is flat spacetime, which can be given as R4 with coordinates formula_22 and the metric
Note that these coordinates actually cover all of R4. The flat space metric (or Minkowski metric) is often denoted by the symbol "η" and is the metric used in special relativity. In the above coordinates, the matrix representation of "η" is
In spherical coordinates formula_25, the flat space metric takes the form
where
is the standard metric on the 2-sphere.
Schwarzschild metric.
Besides the flat space metric the most important metric in general relativity is the Schwarzschild metric which can be given in one set of local coordinates by 
where, again, formula_29 is the standard metric on the 2-sphere. Here "G" is the gravitation constant and "M" is a constant with the dimensions of mass. Its derivation can be found here. The Schwarzschild metric approaches the Minkowski metric as "M" approaches zero (except at the origin where it is undefined). Similarly, when "r" goes to infinity, the Schwarzschild metric approaches the Minkowski metric.
Other metrics.
Other notable metrics are:
Some of them are without the event horizon or can be without the gravitational singularity.
Volume.
The metric "g" induces a natural volume form (up to a sign), which can be used to integrate over a region of a manifold. Given local coordinates formula_2 for the manifold, the volume form can be written
where det["g""μν"] is the determinant of the matrix of components of the metric tensor for the given coordinate system.
Curvature.
The metric "g" completely determines the curvature of spacetime. According to the fundamental theorem of Riemannian geometry, there is a unique connection ∇ on any semi-Riemannian manifold that is compatible with the metric and torsion-free. This connection is called the Levi-Civita connection. The Christoffel symbols of this connection are given in terms of partial derivatives of the metric in local coordinates formula_2 by the formula
The curvature of spacetime is then given by the Riemann curvature tensor which is defined in terms of the Levi-Civita connection ∇. In local coordinates this tensor is given by:
The curvature is then expressible purely in terms of the metric formula_35 and its derivatives.
Einstein's equations.
One of the core ideas of general relativity is that the metric (and the associated geometry of spacetime) is determined by the matter and energy content of spacetime. Einstein's field equations:
where the Ricci curvature tensor
and the scalar curvature
relate the metric (and the associated curvature tensors) to the stress–energy tensor formula_39. This tensor equation is a complicated set of nonlinear partial differential equations for the metric components. Exact solutions of Einstein's field equations are very difficult to find.
References.
See general relativity resources for a list of references.

</doc>
<doc id="32708967" url="https://en.wikipedia.org/wiki?curid=32708967" title="Minimal coupling">
Minimal coupling

In physics, minimal coupling refers to a coupling between fields which involves only the charge distribution and not higher multipole moments of the charge distribution. This minimal coupling is in contrast to, for example, Pauli coupling, which includes the magnetic moment of an electron directly in the Lagrangian.
In electrodynamics, minimal coupling is adequate to account for all electromagnetic interactions. Higher moments of particles are consequences of minimal coupling and non-zero spin.
Mathematically, minimal coupling is achieved by subtracting the charge (formula_1) times the four-potential (formula_2) from the four-momentum (formula_3) in the Lagrangian or Hamiltonian:
See the Hamiltonian mechanics article for a full derivation and examples.
Inflation.
In studies of cosmological inflation, "minimal coupling" of a scalar field usually refers to minimal coupling to gravity. This means that the action for the field formula_5 (called the inflaton in the context of inflation) is not coupled to the scalar curvature. Its only coupling to gravity is the coupling to the Lorentz invariant measure formula_6 constructed from the metric (in Planck units):
where formula_8.

</doc>
<doc id="606970" url="https://en.wikipedia.org/wiki?curid=606970" title="Minimal Supersymmetric Standard Model">
Minimal Supersymmetric Standard Model

The Minimal Supersymmetric Standard Model (MSSM) is an extension to the Standard Model that realizes supersymmetry. MSSM is the minimal supersymmetrical model as it considers only "the [minimum] number of new particle states and new interactions consistent with phenomenology". Supersymmetry pairs bosons with fermions; therefore every Standard Model particle has a partner that has yet to be discovered. If the superparticles are found, it may be analogous to discovering dark matter and depending on the details of what might be found, it could provide evidence for grand unification and might even, in principle, provide hints as to whether string theory describes nature. The failure to find evidence for supersymmetry using the Large Hadron Collider since 2010 has led to suggestions that the theory should be abandoned.
Background.
The MSSM was originally proposed in 1981 to stabilize the weak scale, solving the hierarchy problem. The Higgs boson mass of the Standard Model is unstable to quantum corrections and the theory predicts that weak scale should be much weaker than what is observed to be. In the MSSM, the Higgs boson has a fermionic superpartner, the Higgsino, that has the same mass as it would if supersymmetry were an exact symmetry. Because fermion masses are radiatively stable, the Higgs mass inherits this stability. However, in MSSM there is a need for more than one Higgs field, as described below.
The only unambiguous way to claim discovery of supersymmetry is to produce superparticles in the laboratory. Because superparticles are expected to be 100 to 1000 times heavier than the proton, it requires a huge amount of energy to make these particles that can only be achieved at particle accelerators. The Tevatron was actively looking for evidence of the production of supersymmetric particles before it was shut down on 30 September 2011. Most physicists believe that supersymmetry must be discovered at the LHC if it is responsible for stabilizing the weak scale. There are five classes of particle that superpartners of the Standard Model fall into: squarks, gluinos, charginos, neutralinos, and sleptons. These superparticles have their interactions and subsequent decays described by the MSSM and each has characteristic signatures.
The MSSM imposes R-parity to explain the stability of the proton. It adds supersymmetry breaking by introducing explicit soft supersymmetry breaking operators into the Lagrangian that is communicated to it by some unknown (and unspecified) dynamics. This means that there are 120 new parameters in the MSSM. Most of these parameters lead to unnacceptable phenomenology such as large flavor changing neutral currents or large electric dipole moments for the neutron and electron. To avoid these problems, the MSSM takes all of the soft supersymmetry breaking to be diagonal in flavor space and for all of the new CP violating phases to vanish.
Theoretical motivations.
There are three principal motivations for the MSSM over other theoretical extensions of the Standard Model, namely:
These motivations come out without much effort and they are the primary reasons why the MSSM is the leading candidate for a new theory to be discovered at collider experiments such as the Tevatron or the LHC.
Naturalness.
The original motivation for proposing the MSSM was to stabilize the Higgs mass to radiative corrections that are quadratically divergent in the Standard Model (hierarchy problem). In supersymmetric models, scalars are related to fermions and have the same mass. Since fermion masses are logarithmically divergent, scalar masses inherit the same radiative stability. The Higgs vacuum expectation value is related to the negative scalar mass in the Lagrangian. In order for the radiative corrections to the Higgs mass to not be dramatically larger than the actual value, the mass of the superpartners of the Standard Model should not be significantly heavier than the Higgs VEV—roughly 100 GeV. In 2012, the Higgs particle was discovered at the LHC, and its mass was found to be 125-126 GeV.
Gauge-coupling unification.
If the superpartners of the Standard Model are near the TeV scale, then measured gauge couplings of the three gauge groups unify at high energies. 
The beta-functions for the MSSM gauge couplings are given by
where formula_1 is measured in SU(5) normalization—a factor of formula_2 different
than the Standard Model's normalization and predicted by Georgi–Glashow SU(5) .
The condition for gauge coupling unification at one loop is whether the following expression is satisfied
formula_3.
Remarkably, this is precisely satisfied to experimental errors in the values of formula_4. There are two loop corrections and both TeV-scale and GUT-scale threshold corrections that alter this condition on gauge coupling unification, and the results of more extensive calculations reveal that gauge coupling unification occurs to an accuracy of 1%, though this is about 3 standard deviations from the theoretical expectations.
This prediction is generally considered as indirect evidence for both the MSSM and SUSY GUTs. It should be noted that gauge coupling unification does not necessarily imply grand unification and there exist other mechanisms to reproduce gauge coupling unification. However, if superpartners are found in the near future, the apparent success of gauge coupling unification would suggest that a supersymmetric grand unified theory is a promising candidate for high scale physics.
Dark matter.
If R-parity is preserved, then the lightest superparticle (LSP) of the MSSM is stable and is a Weakly interacting massive particle (WIMP) — i.e. it does not have electromagnetic or strong interactions. This makes the LSP a good dark matter candidate and falls into the category of cold dark matter (CDM) particle.
Predictions of the MSSM regarding hadron colliders.
The Tevatron and LHC have active experimental programs searching for supersymmetric particles. Since both of these machines are hadron colliders — proton antiproton for the Tevatron and proton proton for the LHC — they search best for strongly interacting particles. Therefore, most experimental signature involve production of squarks or gluinos. Since the MSSM has R-parity, the lightest supersymmetric particle is stable and after the squarks and gluinos decay each decay chain will contain one LSP that will leave the detector unseen. This leads to the generic prediction that the MSSM will produce a 'missing energy' signal from these particles leaving the detector.
Neutralinos.
There are four neutralinos that are fermions and are electrically neutral, the lightest of which is typically stable. They are typically labeled , , , (although sometimes formula_5 is used instead). These four states are mixtures of the Bino and the neutral Wino (which are the neutral electroweak Gauginos), and the neutral Higgsinos. As the neutralinos are Majorana fermions, each of them is identical with its antiparticle. Because these particles only interact with the weak vector bosons, they are not directly produced at hadron colliders in copious numbers. They primarily appear as particles in cascade decays of heavier particles usually originating from colored supersymmetric particles such as squarks or gluinos.
In R-parity conserving models, the lightest neutralino is stable and all supersymmetric cascades decays end up decaying into this particle which leaves the detector unseen and its existence can only be inferred by looking for unbalanced momentum in a detector.
The heavier neutralinos typically decay through a to a lighter neutralino or through a to chargino. Thus a typical decay is
The mass splittings between the different Neutralinos will dictate which patterns of decays are allowed.
Charginos.
There are two Charginos that are fermions and are electrically charged. They are typically labeled and (although sometimes formula_6 and formula_7 is used instead). The heavier chargino can decay through to the lighter chargino. Both can decay through a to neutralino.
Squarks.
The squarks are the scalar superpartners of the quarks and there is one version for each Standard Model quark. Due to phenomenological constraints from flavor changing neutral currents, typically the lighter two generations of squarks have to be nearly the same in mass and therefore are not given distinct names. The superpartners of the top and bottom quark can be split from the lighter squarks and are called "stop" and "sbottom".
On the other way, there may be a remarkable left-right mixing of the stops formula_8 and of the sbottoms formula_9 because of the high masses of the partner quarks top and bottom:
Same holds for bottom formula_9 with its own parameters formula_13 and formula_14.
Squarks can be produced through strong interactions and therefore are easily produced at hadron colliders. They decay to quarks and neutralinos or charginos which further decay. In R-parity conserving scenarios, squarks are pair produced and therefore a typical signal is
Gluinos.
Gluinos are Majorana fermionic partners of the gluon which means that they are their own antiparticles. They interact strongly and therefore can be produced significantly at the LHC. They can only decay to a quark and a squark and thus a typical gluino signal is
Because gluinos are Majorana, gluinos can decay to either a quark+anti-squark or an anti-quark+squark with equal probability. Therefore, pairs of gluinos can decay to
This is a distinctive signature because it has same-sign di-leptons and has very little background in the Standard Model.
Sleptons.
Sleptons are the scalar partners of the leptons of the Standard Model. They are not strongly interacting and therefore are not produced very often at hadron colliders unless they are very light.
Because of the high mass of the tau lepton there will be left-right mixing of the stau similar to that of stop and sbottom (see above).
Sfermions will typically be found in decays of a charginos and neutralinos if they are light enough to be a decay product
MSSM fields.
Fermions have bosonic superpartners (called sfermions), and bosons have fermionic superpartners (called bosinos). For most of the Standard Model particles, doubling is very straightforward. However, for the Higgs boson, it is more complicated.
A single Higgsino (the fermionic superpartner of the Higgs boson) would lead to a gauge anomaly and would cause the theory to be inconsistent. However, if two Higgsinos are added, there is no gauge anomaly. The simplest theory is one with two Higgsinos and therefore two scalar Higgs doublets.
Another reason for having two scalar Higgs doublets rather than one is in order to have Yukawa couplings between the Higgs and both down-type quarks and up-type quarks; these are the terms responsible for the quarks' masses. In the Standard Model the down-type quarks couple to the Higgs field (which has Y=-1/2) and the up-type quarks to its complex conjugate (which has Y=+1/2). However, in a supersymmetric theory this is not allowed, so two types of Higgs fields are needed.
MSSM superfields.
In supersymmetric theories, every field and its superpartner can be written together as a superfield. The superfield formulation of supersymmetry is very convenient to write down manifestly supersymmetric theories (i.e. one does not have to tediously check that the theory is supersymmetric term by term in the Lagrangian). The MSSM contains vector superfields associated with the Standard Model gauge groups which contain the vector bosons and associated gauginos. It also contains chiral superfields for the Standard Model fermions and Higgs bosons (and their respective superpartners).
MSSM Higgs Mass.
The MSSM Higgs Mass is a prediction of the Minimal Supersymmetric Standard Model. The mass of the lightest Higgs boson is set by the Higgs "quartic coupling". Quartic couplings are not soft supersymmetry-breaking parameters since they lead to a quadratic divergence of the Higgs mass. Furthermore, there are no supersymmetric parameters to make the Higgs mass a free parameter in the MSSM (though not in non-minimal extensions). This means that Higgs mass is a prediction of the MSSM. The LEP II and the IV experiments placed a lower limit on the Higgs mass of 114.4 GeV. This lower limit is significantly above where the MSSM would typically predict it to be, and while it does not rule out the MSSM, the discovery of the Higgs with a mass of 125 GeV makes proponents of the MSSM nervous.
Formulas.
The only susy-preserving operator that creates a quartic coupling for the Higgs in the MSSM arise for the D-terms of
the SU(2) and U(1) gauge sector and the magnitude of the quartic coupling is set by the size of the gauge couplings.
This leads to the prediction that the Standard Model-like Higgs mass (the scalar that couples approximately to the vev) is limited to be less than the Z mass
formula_22 .
Since supersymmetry is broken, there are radiative corrections to the quartic coupling that can increase the Higgs mass. These dominantly arise from the 'top sector'
formula_23
where formula_24 is the top mass and formula_25 is the mass of the top squark. This result can be interpreted as the RG running of the Higgs quartic coupling from the scale of supersymmetry to the top mass—however since the top squark mass should be relatively close to the top mass, this is usually a fairly modest contribution and increases the Higgs mass to roughly the LEP II bound of 114 GeV before the top squark becomes too heavy.
Finally there is a contribution from the top squark A-terms
formula_26
where formula_27 is a dimensionless number. This contributes an additional term to the Higgs mass at loop level, but is not logarithmically enhanced
formula_28
by pushing formula_29 (known as 'maximal mixing') it is possible to push the Higgs mass to 125 GeV without decoupling the top squark or adding new dynamics to the MSSM.
As the Higgs was found at around 125 GeV (along with no other superparticles) at the LHC, this strongly hints at new dynamics beyond the MSSM, such as the 'Next to Minimal Supersymmetric Standard Model' (NMSSM); and suggests some correlation to the little hierarchy problem.
The MSSM Lagrangian.
The Lagrangian for the MSSM contains several pieces.
formula_30
The constant term is unphysical in global supersymmetry (as opposed to supergravity).
Soft Susy breaking.
The last piece of the MSSM Lagrangian is the soft supersymmetry breaking Lagrangian. The vast majority of the parameters of the MSSM are in the susy breaking Lagrangian. The soft susy breaking are divided into roughly three pieces.
formula_31
Where formula_32 are the gauginos and formula_33 is different for the wino, bino and gluino.
formula_34
where formula_13 are any of the scalars in the MSSM and formula_36 are formula_37 hermitean matrices for the squarks and sleptons of a given set of gauge quantum numbers. The eigenvalues of these matrices are actually the masses squared, rather than the masses.
formula_40
The formula_38 terms are formula_37 complex matrices much as the scalar masses are.
formula_43
The reason these soft terms are not often mentioned are that they arise through local supersymmetry and not global supersymmetry, although they are required otherwise if the Goldstino were massless it would contradict observation. The Goldstino mode is eaten by the Gravitino to become massive, through a gauge shift, which also absorbs the would-be "mass" term of the Goldstino.
Problems with the MSSM.
There are several problems with the MSSM — most of them falling into understanding the parameters.
Theories of supersymmetry breaking.
A large amount of theoretical effort has been spent trying to understand the mechanism for soft supersymmetry breaking that produces the desired properties in the superpartner masses and interactions. The three most extensively studied mechanisms are:
Gravity-mediated supersymmetry breaking.
Gravity-mediated supersymmetry breaking is a method of communicating supersymmetry breaking to the supersymmetric Standard Model through gravitational interactions. It was the first method proposed to communicate supersymmetry breaking. In gravity-mediated supersymmetry-breaking models, there is a part of the theory that only interacts with the MSSM through gravitational interaction. This hidden sector of the theory breaks supersymmetry. Through the supersymmetric version of the Higgs mechanism, the gravitino, the supersymmetric version of the graviton, acquires a mass. After the gravitino has a mass, gravitational radiative corrections to soft masses are incompletely cancelled beneath the gravitino's mass.
It is currently believed that it is not generic to have a sector completely decoupled from the MSSM and there should be higher dimension operators that couple different sectors together with the higher dimension operators suppressed by the Planck scale. These operators give as large of a contribution to the soft supersymmetry breaking masses as the gravitational loops; therefore, today people usually consider gravity mediation to be gravitational sized direct interactions between the hidden sector and the MSSM.
mSUGRA stands for minimal supergravity. The construction of a realistic model of interactions within "N" = 1 supergravity framework where supersymmetry breaking is communicated through the supergravity interactions was carried out by Ali Chamseddine, Richard Arnowitt, and Pran Nath in 1982. mSUGRA is one of the most widely investigated models of particle physics due to its predictive power requiring only 4 input parameters and a sign, to determine the low energy phenomenology from the scale of Grand Unification. The most widely used set of parameters is:
Gravity-Mediated Supersymmetry Breaking was assumed to be flavor universal because of the universality of gravity; however, in 1986 Hall, Kostelecky, and Raby showed that Planck-scale physics that are necessary to generate the Standard-Model Yukawa couplings spoil the universality of the supersymmetry breaking.
Gauge-mediated supersymmetry breaking (GMSB).
Gauge-mediated supersymmetry breaking is method of communicating supersymmetry breaking to the supersymmetric Standard Model through the Standard Model's gauge interactions. Typically a hidden sector breaks supersymmetry and communicates it to massive messenger fields that are charged under the Standard Model. These messenger fields induce a gaugino mass at one loop and then this is transmitted on to the scalar superpartners at two loops. Requiring stop squarks below 2 TeV, the maximum Higgs boson mass predicted is just 121.5GeV. With the Higgs being discovered at 125GeV - this model requires stops above 2 TeV.
Anomaly-mediated supersymmetry breaking (AMSB).
Anomaly-mediated supersymmetry breaking is a special type of gravity mediated supersymmetry breaking that results in supersymmetry breaking being communicated to the supersymmetric Standard Model through the conformal anomaly. Requiring stop squarks below 2 TeV, the maximum Higgs boson mass predicted is just 121.0GeV. With the Higgs being discovered at 125GeV - this scenario requires stops heavier than 2 TeV.
Phenomenological MSSM (pMSSM).
The unconstrained MSSM has more than 100 parameters in addition to the Standard Model parameters.
This makes any phenomenological analysis (e.g. finding regions in parameter space consistent
with observed data) impractical. Under the following three assumptions:
one can reduce the number of additional parameters to the following 19 quantities of the phenomenological MSSM (pMSSM):
The large parameter space of pMSSM makes searches in pMSSM extremely challenging and makes pMSSM difficult to exclude.

</doc>
<doc id="230488" url="https://en.wikipedia.org/wiki?curid=230488" title="Minkowski space">
Minkowski space

In mathematical physics, Minkowski space or Minkowski spacetime is a combination of Euclidean space and time into a four-dimensional manifold where the spacetime interval between any two events is independent of the inertial frame of reference in which they are recorded. Although initially developed by mathematician Hermann Minkowski for Maxwell's equations of electromagnetism, the mathematical structure of Minkowski spacetime was shown to be an immediate consequence of the postulates of special relativity.
Minkowski space is closely associated with Einstein's theory of special relativity, and is the most common mathematical structure on which special relativity is formulated. While the individual components in Euclidean space and time will often differ due to length contraction and time dilation, in Minkowski spacetime, all frames of reference will agree on the total distance in spacetime between events. Because it treats time differently than the three spatial dimensions, Minkowski space differs from four-dimensional Euclidean space.
In Euclidean space, the isometry group (the maps preserving the regular inner product) is the Euclidean group. The analogous isometry group for Minkowski space, preserving intervals of spacetime equipped with the associated non-positive definite bilinear form (here called the Minkowski inner product,) is the Poincaré group. The Minkowski inner product is defined as to yield the spacetime interval between two events when given their coordinate difference vector as argument.
History.
Four-dimensional Euclidean spacetime.
In 1905, and later published in 1906, Henri Poincaré showed that by taking time to be an imaginary fourth spacetime coordinate (√ "c t"), a Lorentz transformation can be regarded as a rotation of coordinates in a four-dimensional Euclidean space with three real coordinates representing space, and one imaginary coordinate, representing time, as the fourth dimension. Since the space is then a pseudo-Euclidean space, the rotation is a representation of a hyperbolic rotation, although Poincaré did not give this interpretation, his purpose being only to explain the Lorentz transformation in terms of the familiar Euclidean rotation.
This idea was elaborated by Hermann Minkowski, who used it to restate the Maxwell equations in four dimensions, showing directly their invariance under the Lorentz transformation. He further reformulated in four dimensions the then-recent theory of special relativity of Einstein. From this he concluded that time and space should be treated equally, and so arose his concept of events taking place in a unified four-dimensional spacetime continuum.
Minkowski space.
In a further development, he gave an alternative formulation of this idea that used a real time coordinate instead of an imaginary one, representing the four variables of space and time in coordinate form in a four dimensional affine space. Points in this space correspond to events in spacetime. In this space, there is a defined light-cone associated with each point (see diagram above), and events not on the light-cone are classified by their relation to the apex as "spacelike" or "timelike". It is principally this view of spacetime that is current nowadays, although the older view involving imaginary time has also influenced special relativity. Minkowski, aware of the fundamental restatement of the theory which he had made, said
For further historical information see references , and .
Mathematical structure.
For an overview, Minkowski space is a -dimensional real vector space equipped with a nondegenerate, symmetric bilinear form on the tangent space at each point in spacetime, here simply called the Minkowski inner product, with signature either or . In practice, one need not be concerned with the tangent spaces. The vector space nature of Minkowski space allows for the canonical identification of vectors in tangent spaces at points (events) with vectors (points, events) in Minkowski space itself. For some purposes it is desirable to identify tangent vectors at a point with "displacement vectors" at , which is, of course, admissible by essentially the same canonical identification.
The signature refers to which sign the Minkowski inner product yields when given space and time basis vectors as arguments. In general, mathematicians and general relativists prefer the former while particle physicists tend to use the latter. Arguments for the former (pure space vectors yield positive "norm-squared") include "continuity" from the Euclidean case corresponding to the non-relativistic limit . Arguments for the latter (pure space vectors yield negative "norm-squared") include that otherwise ubiquitous minus signs in particle physics go away.
Mathematically associated to this bilinear form is a tensor of type at each point in spacetime, called the Minkowski metric. The Minkowski metric, the bilinear form, and the Minkowski inner product are actually all the very same object. In coordinates, this is the matrix representing the bilinear form. Keeping this in mind may facilitate reading what follows.
For comparison, in general relativity, a Lorentzian manifold is likewise equipped with a metric tensor , which is a nondegenerate symmetric bilinear form on the tangent space at each point of . In coordinates, it may be represented by a matrix "depending on spacetime position". Minkowski space is thus a comparatively simple special case of a Lorentzian manifold. Its metric tensor, called the Minkowski metric, is in coordinates the same symmetric matrix at every point of , and its arguments can, per above, be taken as vectors in spacetime itself.
Introducing more terminology (but not more structure), Minkowski space is thus a pseudo-Euclidean space with total dimension and signature or . Elements of Minkowski space are called events. Minkowski space is often denoted or to emphasize the chosen signature, or just . It is perhaps the simplest example of a pseudo-Riemannian manifold.
Pseudo-Euclidean metric generalities.
The Minkowski metric is the metric tensor of Minkowski space. It is a Pseudo-Euclidean metric. As such it is a nondegenerate symmetric bilinear form, a type tensor. It accepts two arguments , vectors in , the tangent space at in . Due to the above-mentioned canonical identification of with itself, it accepts arguments with both and in .
As a notational convention, vectors in , called 4-vectors, are denoted in sans-serif italics, and not, as is common in the Eucliedean setting, with boldface . The latter is generally reserved for the -vector part (to be introduced below) of a -vector.
The definition
yields an inner product-like structure on , previously and also henceforth, called the Minkowski inner product, similar to the Euclidean inner product, but it describes a different geometry. It has the following properties.
The first two conditions imply bilinearity. The defining "difference" between a pseudo-inner product and an inner product proper is that the former is "not" required to be positive definite, that is, is allowed.
Two vectors and are said to be orthogonal if .
A vector is called a unit vector if . A basis for consisting of mutually orthogonal unit vectors is called an orthonormal basis.
For a given inertial frame, an orthonormal basis in space, combined by the unit time vector, forms an orthonormal basis in Minkowski space. The number of positive and negative unit vectors in any such basis is a fixed pair of numbers, equal to the signature of the bilinear form associated with the inner product. This is Sylvester's law of inertia.
More terminology (but not more structure): The Minkowski metric is a pseudo-Riemannian metric, more specifically, a Lorentzian metric, even more specifically, "the" Lorentz metric, reserved for -dimensional flat spacetime with the remaining ambiguity only being the signature convention.
Minkowski metric.
From the two postulates of special relativity follows that the spacetime interval between two events ,
is independent of the inertial frame chosen. The factor simply means that the choice of signature is left open. The numerical values of , viewed as a matrix representing the Minkowski inner product, follow from the theory of bilinear forms.
Just as the signature of the metric is differently defined in the literature, this quantity is not consistently named. The interval (as defined here) is sometimes referred to as the interval squared. Even the square root of the present interval occurs. When signature and interval are fixed, ambiguity still remains as which coordinate is the time coordinate. It may be the fourth, or it may be the zeroth. This is not an exhaustive list of notational inconsistencies. It is a fact of life that one has to check out the definitions first thing when one consults the relativity literature.
The invariance of the interval under coordinate transformations between inertial frames follows from the invariance of
(with either sign preserved), provided the transformations are linear. This quadratic form can be used to define a bilinear form
via the polarization identity. This bilinear form can in turn be written as
where is a matrix associated with . Possibly confusingly, denote with just as is common practice. The matrix is read off from the explicit bilinear form as
and the bilinear form
with which this section started by assuming its existence, is now identified.
For definiteness and shorter presentation, the signature is adopted below. The choice has no (known) physical implications. The symmetry group preserving the bilinear form with one choice of signature is isomorphic (under the map given here) with the symmetry group preserving the other choice of signature. This means that both choices are in accord with the two postulates of relativity.
Standard basis.
A standard basis for Minkowski space is a set of four mutually orthogonal vectors such that
These conditions can be written compactly in the form
Relative to a standard basis, the components of a vector are written where the Einstein notation is used to write . The component is called the timelike component of while the other three components are called the spatial components. The spatial components of a -vector may be identified with a -vector .
In terms of components, the Minkowski inner product between two vectors and is given by
and
Here lowering of an index with the metric was used. Technically, a non-degenerate bilinear form provides a map between a vector space and its dual, in this context, the map is between the tangent spaces of and the cotangent spaces of . At a point in , the tangent and cotangent spaces are dual. Just as an authentic inner product on a vector space with one argument fixed, by Riesz representation theorem, may be expressed as the action of a linear functional on the vector space, the same holds for the Minkowski inner product of Minkowski space.
Thus if are the components of a vector in a tangent space, then are the components of a vector in the cotangent space (a linear functional). Due to the identification of vectors in tangent spaces with vectors in itself, this is mostly ignored, and vectors with lower indices are referred to as covariant vectors. In this latter interpretation, the covariant vectors are (almost always implicitly) identified with vectors (linear functionals) in the dual of Minkowski space. The ones with upper indices are contravariant vectors. In the same fashion, the inverse of the map from tangent to cotangent spaces, explicitly given by the inverse of in matrix representation, can be used to define raising of an index. The components of this inverse are denoted . It happens that . These maps between a vector space and its dual can be denoted (eta-flat) and (eta-sharp) by the musical analogy.
The time-proven robustness of the formalism itself, sometimes referred to as index gymnastics, ensures that moving vectors around and changing from contravariant to covariant vectors and vice versa is mathematically sound. Incorrect expressions tend to reveal themselves quickly.
Lorentz transformations and symmetry.
The Poincaré group is the group of all transformations preserving the interval. The interval is quite easily seen to be preserved by the translation group in dimensions. The other transformations are those that preserve the interval and leave the origin fixed. Given the bilinear form associated with the Minkowski metric, the appropriate group follows directly from the theory (in particular the definition) of classical groups. In the linked article, one should identify (in its a matrix representation) with the matrix .
The appropriate group is , in this context called the Lorentz group. Its elements are called (homogeneous) Lorentz transformations. For other methods of derivation, with a more physical twist, see derivations of the Lorentz transformations.
Among the simplest Lorentz transformations is a Lorentz boost. For reference, a boost in the -direction is given by
where
is the Lorentz factor, and
Other Lorentz transformations are pure rotations, and hence elements of the SO(3) subgroup of . A general homogeneous Lorentz transformation is a product of a pure boost and a pure rotation. An "inhomogeneous" Lorentz transformation is a homogeneous transformation followed by a translation in space and time. Special transformations are those that invert the space coordinates () and time coordinate () respectively, or both .
All four-vectors in Minkowski space transform, by definition, according to the same formula under Lorentz transformations. Minkowski diagrams illustrate Lorentz transformations.
Causal structure.
Vectors are classified according to the sign of . A vector is timelike if , spacelike if , and null or lightlike if . This can be expressed in terms of the sign of as well, but depends on the signature. The classification of any vector will be the same in all frames of reference, because of the invariance of the interval.
The set of all null vectors at an event of Minkowski space constitutes the light cone of that event. Given a timelike vector , there is a worldline of constant velocity associated with it, represented by a straight line in a Minkowski diagram.
Once a direction of time is chosen, timelike and null vectors can be further decomposed into various classes. For timelike vectors one has
Null vectors fall into three classes:
Spacelike vectors are in elsewhere. The terminology stems from the fact that spacelike separated events are connected by vectors requiring faster-than-light travel, and so cannot possibly influence each other. Together with spacelike and lightlike vectors there are 7 classes in all.
An orthonormal basis for Minkowski space necessarily consists of one timelike and three spacelike unit vectors. If one wishes to work with non-orthonormal bases it is possible to have other combinations of vectors. For example, one can easily construct a (non-orthonormal) basis consisting entirely of null vectors, called a null basis. Over the reals, if two null vectors are orthogonal (zero Minkowski tensor value), then they must be proportional. However, allowing complex numbers, one can obtain a null tetrad, which is a basis consisting of null vectors, some of which are orthogonal to each other.
Vector fields are called timelike, spacelike or null if the associated vectors are timelike, spacelike or null at each point where the field is defined.
Chronological and causality relations.
Let "x", "y" ∈ "M". We say that
Reversed triangle inequality.
If "v" and "w" are both future-directed timelike four-vectors, then in the (+ - - -) sign convention for norm, 
Relationships to other formulations.
Different number of dimensions.
Strictly speaking, Minkowski space refers to a mathematical formulation in four dimensions. However, the mathematics can easily be extended or simplified to create an analogous "Minkowski space" in any number of dimensions. If , -dimensional Minkowski space is a vector space of real dimension on which there is a constant Lorentz metric of signature or . These generalizations are used in theories where spacetime is assumed to have more or less than dimensions. String theory and M-theory are two examples where . In string theory, there appears conformal field theories with spacetime dimensions.
Flat versus curved space.
As a "flat spacetime", the three spatial components of Minkowski spacetime always obey the Pythagorean Theorem. Minkowski space is a suitable basis for special relativity, a good description of physical systems over finite distances in systems without significant gravitation. However, in order to take gravity into account, physicists use the theory of general relativity, which is formulated in the mathematics of a non-Euclidean geometry. When this geometry is used as a model of physical space, it is known as curved space.
Even in curved space, Minkowski space is still a good description in an infinitesimal region surrounding any point (barring gravitational singularities). More abstractly, we say that in the presence of gravity spacetime is described by a curved 4-dimensional manifold for which the tangent space to any point is a 4-dimensional Minkowski space. Thus, the structure of Minkowski space is still essential in the description of general relativity.

</doc>
<doc id="9734557" url="https://en.wikipedia.org/wiki?curid=9734557" title="MKS system of units">
MKS system of units

The MKS system of units is a physical system of units that expresses any given measurement using fundamental units of the metre, kilogramme, and/or second (MKS). In 1901, Giovanni Giorgi proposed to the (AEI) that this system, extended with a fourth unit to be taken from the units of electromagnetism, be used as an international system. 
Historically the MKS system of units succeeded the centimetre–gram–second system of units (CGS) and laid the blueprint for the International System of Units, which now serves as the international standard. Therefore, the exact composition of the MKS system is a historical issue. As a matter of historical record the MKS system incorporated fundamental units other than the metre, kilogram, and second in addition to derived units. An incomplete list of the fundamental and derived units appears below. Since the MKS system of units never had a governing body to rule on a standard definition, the list of units depended on different conventions at different times. 

</doc>
<doc id="1432127" url="https://en.wikipedia.org/wiki?curid=1432127" title="Mode (statistics)">
Mode (statistics)

The mode is the value that appears most often in a set of data. The mode of a discrete probability distribution is the value "x" at which its probability mass function takes its maximum value. In other words, it is the value that is most likely to be sampled. The mode of a continuous probability distribution is the value "x" at which its probability density function has its maximum value, so the mode is at the peak.
Like the statistical mean and median, the mode is a way of expressing, in a single number, important information about a random variable or a population. The numerical value of the mode is the same as that of the mean and median in a normal distribution, and it may be very different in highly skewed distributions.
The mode is not necessarily unique, since the probability mass function or probability density function may take the same maximum value at several points "x"1, "x"2, etc. The most extreme case occurs in uniform distributions, where all values occur equally frequently. When a probability density function has multiple local maxima it is common to refer to all of the local maxima as modes of the distribution. Such a continuous distribution is called multimodal (as opposed to unimodal).
In symmetric unimodal distributions, such as the normal distribution, the mean (if defined), median and mode all coincide. For samples, if it is known that they are drawn from a symmetric distribution, the sample mean can be used as an estimate of the population mode.
Mode of a sample.
The mode of a sample is the element that occurs most often in the collection. For example, the mode of the sample [1, 3, 6, 6, 6, 6, 7, 7, 12, 12, 17] is 6. Given the list of data [1, 1, 2, 4, 4] the mode is not unique - the dataset may be said to be bimodal, while a set with more than two modes may be described as multimodal.
For a sample from a continuous distribution, such as [0.935..., 1.211..., 2.430..., 3.668..., 3.874...], the concept is unusable in its raw form, since no two values will be exactly the same, so each value will occur precisely once. In order to estimate the mode, the usual practice is to discretize the data by assigning frequency values to intervals of equal distance, as for making a histogram, effectively replacing the values by the midpoints of the
intervals they are assigned to. The mode is then the value where the histogram reaches its peak. For small or middle-sized samples the outcome of this procedure is sensitive to the choice of interval width if chosen too narrow or too wide; typically one should have a sizable fraction of the data concentrated in a relatively small number of intervals (5 to 10), while the fraction of the data falling outside these intervals is also sizable. An alternate approach is kernel density estimation, which essentially blurs point samples to produce a continuous estimate of the probability density function which can provide an estimate of the mode.
The following MATLAB (or Octave) code example computes the mode of a sample:
The algorithm requires as a first step to sort the sample in ascending order. It then computes the discrete derivative of the sorted list, and finds the indices where this derivative is positive. Next it computes the discrete derivative of this set of indices, locating the maximum of this derivative of indices, and finally evaluates the sorted sample at the point where that maximum occurs, which corresponds to the last member of the stretch of repeated values.
Comparison of mean, median and mode.
Use.
Unlike mean and median, the concept of mode also makes sense for "nominal data" (i.e., not consisting of numerical values in the case of mean, or even of ordered values in the case of median). For example, taking a sample of Korean family names, one might find that "Kim" occurs more often than any other name. Then "Kim" would be the mode of the sample. In any voting system where a plurality determines victory, a single modal value determines the victor, while a multi-modal outcome would require some tie-breaking procedure to take place.
Unlike median, the concept of mode makes sense for any random variable assuming values from a vector space, including the real numbers (a one-dimensional vector space) and the integers (which can be considered embedded in the reals). For example, a distribution of points in the plane will typically have a mean and a mode, but the concept of median does not apply. The median makes sense when there is a linear order on the possible values. Generalizations of the concept of median to higher-dimensional spaces are the geometric median and the centerpoint.
Uniqueness and definedness.
"For the remainder, the assumption is that we have (a sample of) a real-valued random variable."
For some probability distributions, the expected value may be infinite or undefined, but if defined, it is unique. The mean of a (finite) sample is always defined. The median is the value such that the fractions not exceeding it and not falling below it are each at least 1/2. It is not necessarily unique, but never infinite or totally undefined. For a data sample it is the "halfway" value when the list of values is ordered in increasing value, where usually for a list of even length the numerical average is taken of the two values closest to "halfway". Finally, as said before, the mode is not necessarily unique. Certain pathological distributions (for example, the Cantor distribution) have no defined mode at all. For a finite data sample, the mode is one (or more) of the values in the sample.
Properties.
Assuming definedness, and for simplicity uniqueness, the following are some of the most interesting properties.
Example for a skewed distribution.
An example of a skewed distribution is personal wealth: Few people are very rich, but among those some are extremely rich. However, many are rather poor.
A well-known class of distributions that can be arbitrarily skewed is given by the log-normal distribution. It is obtained by transforming a random variable "X" having a normal distribution into random variable "Y" = "e""X". Then the logarithm of random variable "Y" is normally distributed, hence the name.
Taking the mean μ of "X" to be 0, the median of "Y" will be 1, independent of the standard deviation σ of "X". This is so because "X" has a symmetric distribution, so its median is also 0. The transformation from "X" to "Y" is monotonic, and so we find the median "e"0 = 1 for "Y".
When "X" has standard deviation σ = 0.25, the distribution of "Y" is weakly skewed. Using formulas for the log-normal distribution, we find:
Indeed, the median is about one third on the way from mean to mode.
When "X" has a larger standard deviation, σ = 1, the distribution of "Y" is strongly skewed. Now
Here, Pearson's rule of thumb fails.
Van Zwet condition.
Van Zwet derived an inequality which provides sufficient conditions for this inequality to hold. The inequality
holds if
for all "x" where F() is the cumulative distribution function of the distribution.
Unimodal distributions.
It can be shown for a unimodal distribution that the median formula_4 and the mean formula_5 lie within (3/5)1/2 ≈ 0.7746 standard deviations of each other. In symbols,
where |.| is the absolute value.
A similar relation holds between the median and the mode: they lie within 31/2 ≈ 1.732 standard deviations of each other:
Confidence interval for the mode with a single data point.
It is a common but false belief that from a single observation "x" we can not gain information about the variability in the population and that consequently that ﬁnite length conﬁdence intervals for mean and/or variance are impossible even in principle.
It is possible for an unknown unimodal distribution to estimate a confidence interval for the mode with a sample size of 1. This was first shown by Abbot and Rosenblatt and extended by Blachman and Machol. This confidence interval can be sharpened if the distribution can be assumed to be symmetrical. It is further possible to sharpen this interval if the distribution is normally distributed.
Let the confidence interval be 1 - "α". Then the confidence intervals for the general, symmetric and normally distributed variates respectively are
where "X" is the variate, "θ" is the mode and || is the absolute value.
These estimates are conservative. The confidence intervals for the mode at the 90% level given by these estimators are "X" ± 19 | "X" - "θ" |, "X" ± 9 | "X" - "θ" | and "X" ± 5.84 | "X" - "θ" | for the general, symmetric and normally distributed variates respectively. The 95% confidence interval for a normally distributed variate is given by "X" ± 10.7 | "X" - "θ" |. It may be worth noting that the mean and the mode coincide if the variates are normally distributed.
The 95% bound for a normally distributed variate has been improved and is now known to be "X" ± 9.68 | "X" - "θ" | The bound for a 99% confidence interval is "X" ± 48.39 | "X" - "θ'|
Machol has shown that given a known density symmetrical about 0 that given a single sample value ("x") that the 90% confidence intervals of population mean are
where "ν" is the population median.
If the precise form of the distribution is not known but it is known to be symmetrical about zero then we have
where "X" is the variate, "μ" is the population mean and "a" and "k" are arbitrary real numbers.
It is also possible to estimate a confidence interval for the standard deviation from a single observation if the distribution is symmetrical about 0. For a normal distribution the with an unknown variance and a single data point ("X") the 90%, 95% and 99% confidence intervals for the standard deviation are [ 0, 8|"X"| ], [ 0, 17|"X"| ] and [ 0, 70|"X"| ]. These intervals may be shorted if the mean is known to be bounded by a multiple of the standard deviation.
If the distribution is known to be normal then it is possible to estimate a confidence interval for the mean and variance from a simple value. The 90% confidence intervals are
The confidence intervals can be estimated for any chosen range.
This method is not limited to the normal distribution but can be used with any known distribution.
Statistical tests.
These estimators have been used to create hypothesis tests for simple samples from normal or symmetrical unimodal distributions. Let the distribution have an assumed mean ( "μ"0 ). The null hypothesis is that the assumed mean of the distribution lies within the confidence interval of the sample mean ( "m" ). The null hypothesis is accepted if
where "x" is the value of the sample and "k" is a constant. The null hypothesis is rejected if
The value of "k" depends on the choice of confidence interval and the nature of the assumed distribution.
If the distribution is assumed or is known to be normal then the values of "k" for the 50%, 66.6%, 75%, 80%, 90%, 95% and 99% confidence intervals are 0.50, 1.26, 1.80, 2.31, 4.79, 9.66 and 48.39 respectively.
If the distribution is assumed or known to be unimodal and symmetrical but not normal then the values of "k" for the 50%, 66.6%, 75%, 80%, 90%, 95% and 99% confidence intervals are 0.50, 1.87, 2.91, 3.94, 8.97, 18.99, 99.00 respectively.
To see how this test works we assume or know "a priori" that the population from which the sample is drawn has a mean of "μ"0 and that the population has a symmetrical unimodal distribution - a class that includes the normal distribution. We wish to know if the mean estimated from the sample is representative of the population at a pre chosen level of confidence.
Assume that the distribution is normal and let the confidence interval be 95%. Then "k" = 9.66.
Assuming that the sample is representative of the population, the sample mean ( "m" ) will then lie within the range determined from the formula:
If subsequent sampling shows that the sample mean lies outside these parameters the sample mean is to be considered to differ significantly from the population mean.
History.
The term mode originates with Karl Pearson in 1895.

</doc>
<doc id="19555" url="https://en.wikipedia.org/wiki?curid=19555" title="Molecule">
Molecule

A molecule ( from Latin moles "mass") is an electrically neutral group of two or more atoms held together by chemical bonds. Molecules are distinguished from ions by their lack of electrical charge. However, in quantum physics, organic chemistry, and biochemistry, the term "molecule" is often used less strictly, also being applied to polyatomic ions.
In the kinetic theory of gases, the term "molecule" is often used for any gaseous particle regardless of its composition. According to this definition, noble gas atoms are considered molecules as they are in fact monoatomic molecules.
A molecule may be homonuclear, that is, it consists of atoms of a single chemical element, as with oxygen (O2); or it may be heteronuclear, a chemical compound composed of more than one element, as with water (H2O). Atoms and complexes connected by non-covalent bonds such as hydrogen bonds or ionic bonds are generally not considered single molecules.
Molecules as components of matter are common in organic substances (and therefore biochemistry). They also make up most of the oceans and atmosphere. However, the majority of familiar solid substances on Earth, including most of the minerals that make up the crust, mantle, and core of the Earth, contain many chemical bonds, but are "not" made of identifiable molecules. Also, no typical molecule can be defined for ionic crystals (salts) and covalent crystals (network solids), although these are often composed of repeating unit cells that extend either in a plane (such as in graphene) or three-dimensionally (such as in diamond, quartz, or sodium chloride). The theme of repeated unit-cellular-structure also holds for most condensed phases with metallic bonding, which means that solid metals are also not made of molecules. In glasses (solids that exist in a vitreous disordered state), atoms may also be held together by chemical bonds without presence of any definable molecule, but also without any of the regularity of repeating units that characterizes crystals.
Molecular science.
The science of molecules is called "molecular chemistry" or "molecular physics", depending on whether the focus is on chemistry or physics. Molecular chemistry deals with the laws governing the interaction between molecules that results in the formation and breakage of chemical bonds, while molecular physics deals with the laws governing their structure and properties. In practice, however, this distinction is vague. In molecular sciences, a molecule consists of a stable system (bound state) composed of two or more atoms. Polyatomic ions may sometimes be usefully thought of as electrically charged molecules. The term "unstable molecule" is used for very reactive species, i.e., short-lived assemblies (resonances) of electrons and nuclei, such as radicals, molecular ions, Rydberg molecules, transition states, van der Waals complexes, or systems of colliding atoms as in Bose–Einstein condensate.
History and etymology.
According to Merriam-Webster and the Online Etymology Dictionary, the word "molecule" derives from the Latin "moles" or small unit of mass.
The definition of the molecule has evolved as knowledge of the structure of molecules has increased. Earlier definitions were less precise, defining molecules as the smallest particles of pure chemical substances that still retain their composition and chemical properties. This definition often breaks down since many substances in ordinary experience, such as rocks, salts, and metals, are composed of large crystalline networks of chemically bonded atoms or ions, but are not made of discrete molecules.
Molecular size.
Most molecules are far too small to be seen with the naked eye, but there are exceptions. DNA, a macromolecule, can reach macroscopic sizes, as can molecules of many polymers. Molecules commonly used as building blocks for organic synthesis have a dimension of a few angstroms (Å) to several dozen Å. Single molecules cannot usually be observed by light (as noted above), but small molecules and even the outlines of individual atoms may be traced in some circumstances by use of an atomic force microscope. Some of the largest molecules are macromolecules or supermolecules.
The smallest molecule is the diatomic hydrogen (H2), with a bond length of 0.74 Å.
Effective molecular radius is the size a molecule displays in solution.
The table of permselectivity for different substances contains examples.
Molecular formulas.
Chemical formula types.
The chemical formula for a molecule uses a single line of chemical element symbols, numbers, and sometimes also other symbols, such as parentheses, dashes, brackets, and "plus" (+) and "minus" (−) signs. These are limited to a single typographic line of symbols, which may include subscripts and superscripts.
A compound's empirical formula is a very simple type of chemical formula. It is the simplest integer ratio of the chemical elements that constitute it. For example, water is always composed of a 2:1 ratio of hydrogen to oxygen atoms, and ethyl alcohol or ethanol is always composed of carbon, hydrogen, and oxygen in a 2:6:1 ratio. However, this does not determine the kind of molecule uniquely – dimethyl ether has the same ratios as ethanol, for instance. Molecules with the same atoms in different arrangements are called isomers. Also carbohydrates, for example, have the same ratio (carbon:hydrogen:oxygen = 1:2:1) (and thus the same empirical formula) but different total numbers of atoms in the molecule.
The molecular formula reflects the exact number of atoms that compose the molecule and so characterizes different molecules. However different isomers can have the same atomic composition while being different molecules.
The empirical formula is often the same as the molecular formula but not always. For example, the molecule acetylene has molecular formula C2H2, but the simplest integer ratio of elements is CH.
The molecular mass can be calculated from the chemical formula and is expressed in conventional atomic mass units equal to 1/12 of the mass of a neutral carbon-12 (12C isotope) atom. For network solids, the term formula unit is used in stoichiometric calculations.
Structural formula.
For molecules with a complicated 3-dimensional structure, especially involving atoms bonded to four different substituents, a simple molecular formula or even semi-structural chemical formula may not be enough to completely specify the molecule. In this case, a graphical type of formula called a structural formula may be needed. Structural formulas may in turn be represented with a one-dimensional chemical name, but such chemical nomenclature requires many words and terms which are not part of chemical formulas.
Molecular geometry.
Molecules have fixed equilibrium geometries—bond lengths and angles— about which they continuously oscillate through vibrational and rotational motions. A pure substance is composed of molecules with the same average geometrical structure. The chemical formula and the structure of a molecule are the two important factors that determine its properties, particularly its reactivity. Isomers share a chemical formula but normally have very different properties because of their different structures. Stereoisomers, a particular type of isomers, may have very similar physico-chemical properties and at the same time different biochemical activities.
Molecular spectroscopy.
Molecular spectroscopy deals with the response (spectrum) of molecules interacting with probing signals of known energy (or frequency, according to Planck's formula). Molecules have quantized energy levels that can be analyzed by detecting the molecule's energy exchange through absorbance or emission.
Spectroscopy does not generally refer to diffraction studies where particles such as neutrons, electrons, or high energy X-rays interact with a regular arrangement of molecules (as in a crystal).
Theoretical aspects.
The study of molecules by molecular physics and theoretical chemistry is largely based on quantum mechanics and is essential for the understanding of the chemical bond. The simplest of molecules is the hydrogen molecule-ion, H2+, and the simplest of all the chemical bonds is the one-electron bond. H2+ is composed of two positively charged protons and one negatively charged electron, which means that the Schrödinger equation for the system can be solved more easily due to the lack of electron–electron repulsion. With the development of fast digital computers, approximate solutions for more complicated molecules became possible and are one of the main aspects of computational chemistry.
When trying to define rigorously whether an arrangement of atoms is "sufficiently stable" to be considered a molecule, IUPAC suggests that it "must correspond to a depression on the potential energy surface that is deep enough to confine at least one vibrational state". This definition does not depend on the nature of the interaction between the atoms, but only on the strength of the interaction. In fact, it includes weakly bound species that would not traditionally be considered molecules, such as the helium dimer, He2, which has one vibrational bound state and is so loosely bound that it is only likely to be observed at very low temperatures.
Whether or not an arrangement of atoms is "sufficiently stable" to be considered a molecule is inherently an operational definition. Philosophically, therefore, a molecule is not a fundamental entity (in contrast, for instance, to an elementary particle); rather, the concept of a molecule is the chemist's way of making a useful statement about the strengths of atomic-scale interactions in the world that we observe.

</doc>
<doc id="278366" url="https://en.wikipedia.org/wiki?curid=278366" title="Moment (physics)">
Moment (physics)

In physics, moment is a combination of a physical quantity and a distance. Moments are usually defined with respect to a fixed reference point or axis; they deal with physical quantities as measured at some distance from that reference point or axis. For example, a moment of force is the product of a force and its distance from an axis, which causes rotation about that axis. In principle, any physical quantity can be combined with a distance to produce a moment; commonly used quantities include forces, masses, and electric charge distributions.
Elaboration.
In its most simple and basic form, a moment is the product of the distance to some point, raised to some power, multiplied by some physical quantity such as the force, charge, etc. at that point:
where formula_2 is the physical quantity such as a force applied at a point, or a point charge, or a point mass, etc. If the quantity is not concentrated solely at a single point, the moment is the integral of that quantity's density over space:
where formula_4 is the distribution of the density of charge, mass, or whatever quantity is being considered.
More complex forms take into account the angular relationships between the distance and the physical quantity, but the above equations capture the essential feature of a moment, namely the existence of an underlying formula_5 or equivalent term. This implies that there are multiple moments (one for each value of "n") and that the moment generally depends on the reference point from which the distance formula_6 is measured, although for certain moments (technically, the lowest non-zero moment) this dependence vanishes and the moment becomes independent of the reference point.
Each value of "n" corresponds to a different moment: the 1st moment corresponds to "n=1"; the 2nd moment to "n=2", etc. The 0th moment ("n=0") is sometimes called the "monopole moment"; the 1st moment ("n=1") is sometimes called the "dipole moment", and the 2nd moment ("n=2") is sometimes called the "quadrupole moment", especially in the context of electric charge distributions.
Multiple moments.
Assuming a density function that is finite and localized to a particular region, outside that region a "1/r" potential may be expressed as a series of spherical harmonics:
The coefficients formula_17 are known as "multipole moments", and take the form:
where formula_19 expressed in spherical coordinates
formula_20 is a variable of integration. A more complete
treatment may be found in pages describing multipole expansion or
spherical multipole moments. (Note: the convention in the above equations 
was taken from Jackson. The conventions used in the
referenced pages may be slightly different.) 
When formula_4 represents an electric charge density, the formula_17 are, in a sense, projections of the moments of electric charge: formula_23 is the monopole moment; the formula_24 are projections of the dipole moment, the formula_25 are projections of the quadrupole moment, etc.
Applications of multipole moments.
The multipole expansion applies to "1/r" scalar potentials, examples of which include the electric potential and the gravitational potential. For these potentials, the expression can be used to approximate the strength of a field produced by a localized distribution of charges (or mass) by calculating the first few moments. For sufficiently large "r", a reasonable approximation can be obtained from just the monopole and dipole moments. Higher fidelity can be achieved by calculating higher order moments. Extensions of the technique can be used to calculate interaction energies and intermolecular forces.
The technique can also be used to determine the properties of an unknown distribution formula_4. Measurements pertaining to multipole moments may be taken and used to infer properties of the underlying distribution. This technique applies to small objects such as molecules,
but has also been applied to the universe itself, being for example the technique employed by the WMAP and Planck experiments to analyze the Cosmic microwave background radiation.
History.
The concept of moment in physics is derived from the mathematical concept of moments. . The principle of moments is derived from Archimedes' discovery of the operating principle of the lever. In the lever one applies a force, in his day most often human muscle, to an arm, a beam of some sort. Archimedes noted that the amount of force applied to the object, the moment of force, is defined as M = rF, where F is the applied force, and r is the distance from the applied force to object. However, historical evolution of the term 'moment' and its use in different branches of science, such as mathematics, physics and engineering, is unclear.
Federico Commandino, in 1565, translated into Latin from Archimedes:
This was apparently the first use of the word "moment" (Latin, "momentorum") in the sense which we now know it: a moment about a center of rotation.

</doc>
<doc id="157700" url="https://en.wikipedia.org/wiki?curid=157700" title="Moment of inertia">
Moment of inertia

The moment of inertia, otherwise known as the angular mass or rotational inertia, of a rigid body determines the torque needed for a desired angular acceleration about a rotational axis. It depends on the body's mass distribution and the axis chosen, with larger moments requiring more torque to change the body's rotation. It is an extensive (additive) property: the moment of inertia of a composite system is the sum of the moments of inertia of its component subsystems (all taken about the same axis). One of its definitions is the second moment of mass with respect to distance from an axis "r", formula_1, integrating over the entire mass.
For bodies constrained to rotate in a plane, it is sufficient to consider their moment of inertia about an axis perpendicular to the plane. For bodies free to rotate in three dimensions, their moments can be described by a symmetric 3 × 3 matrix; each body has a set of mutually perpendicular principal axes for which this matrix is diagonal and torques around the axes act independently of each other.
Introduction.
When a body is rotating, or free to rotate, around an axis, a torque must be applied to change its angular momentum. The amount of torque needed for any given rate of change in angular momentum is proportional to the moment of inertia of the body. Moment of inertia may be expressed in terms of kilogram-square metres (kg·m2) in SI units and pound-square feet (lbm·ft2) in imperial or US units.
Moment of inertia plays the role in rotational kinetics that Mass (inertia) plays in linear kinetics - both characterize the resistance of a body to changes in its motion. The moment of inertia depends on how mass is distributed around an axis of rotation, and will vary depending on the chosen axis. For a point-like mass, the moment of inertia about some axis is given by "d"2"m", where "d" is the distance to the axis, and "m" is the mass. For an extended body, the moment of inertia is just the sum of all the small pieces of mass multiplied by the square of their distances from the axis in question. For an extended body of a regular shape and uniform density, this summation sometimes produces a simple expression that depends on the dimensions, shape and total mass of the object.
In 1673 Christiaan Huygens introduced this parameter in his study of the oscillation of a body hanging from a pivot, known as a compound pendulum. The term "moment of inertia" was introduced by Leonhard Euler in his book "Theoria motus corporum solidorum seu rigidorum" in 1765, and it is incorporated into Euler's second law.
The natural frequency of oscillation of a compound pendulum is obtained from the ratio of the torque imposed by gravity on the mass of the pendulum to the resistance to acceleration defined by the moment of inertia. Comparison of this natural frequency to that of a simple pendulum consisting of a single point of mass provides a mathematical formulation for moment of inertia of an extended body.
Moment of inertia also appears in momentum, kinetic energy, and in Newton's laws of motion for a rigid body as a physical parameter that combines its shape and mass. There is an interesting difference in the way moment of inertia appears in planar and spatial movement. Planar movement has a single scalar that defines the moment of inertia, while for spatial movement the same calculations yield a 3 × 3 matrix of moments of inertia, called the inertia matrix or inertia tensor.
The moment of inertia of a rotating flywheel is used in a machine to resist variations in applied torque to smooth its rotational output. The moment of inertia of an airplane about its longitudinal, horizontal and vertical axes determines how steering forces on the control surfaces of its wings, elevators and tail affect the plane in roll, pitch and yaw.
Definition.
Moment of inertia "I" is defined as the ratio of the angular momentum "L" of a system to its angular velocity ω around a principal axis, that is
If the angular momentum of a system is constant, then as the moment of inertia gets smaller, the angular velocity must increase. This occurs when spinning figure skaters pull in their outstretched arms or divers move from a straight position to a tuck position during a dive.
If the shape of the body does not change, then its moment of inertia appears in Newton's law of motion as the ratio of an applied torque τ on a body to the angular acceleration α around a principal axis, that is
For a simple pendulum, this definition yields a formula for the moment of inertia "I" in terms of the mass "m" of the pendulum and its distance "r" from the pivot point as,
Thus, moment of inertia depends on both the mass "m" of a body and its geometry, or shape, as defined by the distance "r" to the axis of rotation.
This simple formula generalizes to define moment of inertia for an arbitrarily shaped body as the sum of all the elemental point masses "dm" each multiplied by the square of its perpendicular distance "r" to an axis S .
In general, given an object of mass "m", an effective radius "k" can be defined for an axis through its center of mass, with such a value that its moment of inertia is
where "k" is known as the radius of gyration.
Simple pendulum.
Moment of inertia can be measured using a simple pendulum, because it is the resistance to the rotation caused by gravity. Mathematically, the moment of inertia of the pendulum is the ratio of the torque due to gravity about the pivot of a pendulum to its angular acceleration about that pivot point. For a simple pendulum this is found to be the product of the mass of the particle "m" with the square of its distance "r" to the pivot, that is
This can be shown as follows:
The force of gravity on the mass of a simple pendulum generates a torque formula_7 around the axis perpendicular to the plane of the pendulum movement. Here r is the distance vector perpendicular to and from the force to the torque axis. Here F is the tangential component of the net force on the mass. Associated with this torque is an angular acceleration, formula_8, of the string and mass around this axis. Since the mass is constrained to a circle the tangential acceleration of the mass is formula_9. Since formula_10 the torque equation becomes:
where e is a unit vector perpendicular to the plane of the pendulum. (The second to the last step occurs because of the BAC-CAB rule using the fact that formula_8 is always perpendicular to r.) The quantity is the "moment of inertia" of this single mass around the pivot point.
The quantity also appears in the angular momentum of a simple pendulum, which is calculated from the velocity of the pendulum mass around the pivot, where is the angular velocity of the mass about the pivot point. This angular momentum is given by
using math similar to that used to derive the previous equation.
Similarly, the kinetic energy of the pendulum mass is defined by the velocity of the pendulum around the pivot to yield
This shows that the quantity is how mass combines with the shape of a body to define rotational inertia. The moment of inertia of an arbitrarily shaped body is the sum of the values for all of the elements of mass in the body.
Compound pendulum.
A compound pendulum is a body formed from an assembly of particles of continuous shape that rotates rigidly around a pivot. Its moment of inertia is the sum of the moments of inertia of each of the particles that it is composed of.
The natural frequency (formula_15) of a compound pendulum depends on its moment of inertia, formula_16,
where formula_18 is the mass of the object, formula_19 is local acceleration of gravity, and formula_20 is the distance from the pivot point to the centre of mass of the object. Measuring this frequency of oscillation over small angular displacements provides an effective way of measuring moment of inertia of a body.
Thus, to determine the moment of inertia of the body, simply suspend it from a convenient pivot point formula_21 so that it swings freely in a plane perpendicular to the direction of the desired moment of inertia, then measure its natural frequency or period of oscillation (formula_22), to obtain
where formula_22 is the period (duration) of oscillation (usually averaged over multiple periods).
The moment of inertia of the body about its centre of mass, formula_25, is then calculated using the parallel axis theorem to be
where formula_18 is the mass of the body and formula_20 is the distance from the pivot point formula_21 to the centre of mass formula_30.
Moment of inertia of a body is often defined in terms of its "radius of gyration", which is the radius of a ring of equal mass around the centre of mass of a body that has the same moment of inertia. The radius of gyration formula_31 is calculated from the body's moment of inertia formula_25 and mass formula_18 as the length,
Center of oscillation.
A simple pendulum that has the same natural frequency as a compound pendulum defines the length formula_35 from the pivot to a point called the centre of oscillation of the compound pendulum. This point also corresponds to the centre of percussion. The length formula_35 is determined from the formula,
or
The seconds pendulum, which provides the "tick" and "tock" of a grandfather clock, takes one second to swing from side-to-side. This is a period of two seconds, or a natural frequency of π radians/second for the pendulum. In this case, the distance to the center of oscillation, formula_35, can be computed to be
Notice that the distance to the center of oscillation of the seconds pendulum must be adjusted to accommodate different values for the local acceleration of gravity. Kater's pendulum is a compound pendulum that uses this property to measure the local acceleration of gravity, and is called a gravimeter.
Measuring moment of inertia.
The moment of inertia of complex systems such as a vehicle or airplane around its vertical axis can be measured by suspending the system from three points to form a trifilar pendulum. A trifilar pendulum is a platform supported by three wires designed to oscillate in torsion around its vertical centroidal axis. The period of oscillation of the trifilar pendulum yields the moment of inertia of the system.
Calculating moment of inertia about an axis.
The moment of inertia about an axis of a body is calculated by summing mr2 for every particle in the body, where "r" is the perpendicular distance to the specified axis. To see how moment of inertia arises in the study of the movement of an extended body, it is convenient to consider a rigid assembly of point masses. (This equation can be used for axes that are not principal axes provided that it is understood that this does not fully describe the moment of inertia.)
Consider the kinetic energy of an assembly of masses that lie at the distances from the pivot point "P", which is the nearest point on the axis of rotation. It is the sum of the kinetic energy of the individual masses, 
This shows that the moment of inertia of the body is the sum of each of the mr2 terms, that is
Thus, moment of inertia is a physical property that combines the mass and distribution of the particles around the rotation axis. Notice that rotation about different axes of the same body yield different moments of inertia.
The moment of inertia of a continuous body rotating about a specified axis is calculated in the same way, with the summation replaced by the integral,
Again r is the radius vector to a point in the body from the specified axis through the pivot "P", and (r) is the mass density at each point r. The integration is evaluated over the volume  of the body. The moment of inertia of a flat surface is similar with the mass density being replaced by its areal mass density with the integral evaluated over its area.
Note on second moment of area: The moment of inertia of a body moving in a plane and the second moment of area of a beam's cross-section are often confused. The moment of inertia of body with the shape of the cross-section is the second moment of this area about the z-axis perpendicular to the cross-section, weighted by its density. This is also called the "polar moment of the area", and is the sum of the second moments about the x and y axes. The stresses in a beam are calculated using the second moment of the cross-sectional area around either the x-axis or y-axis depending on the load.
Example calculation of moment of inertia.
The moment of inertia of a compound pendulum constructed from a thin disc mounted at the end of a thin rod that oscillates around a pivot at the other end of the rod, begins with the calculation of the moment of inertia of the thin rod and thin disc about their respective centres of mass.
where is the mass of the rod.
where is its mass.
where "L" is the length of the pendulum. Notice that the parallel axis theorem is used to shift the moment of inertia from the centre of mass to the pivot point of the pendulum.
A list of moments of inertia formulas for standard body shapes provides a way to obtain the moment of inertial of a complex body as an assembly of simpler shaped bodies. The parallel axis theorem is used to shift the reference point of the individual bodies to the reference point of the assembly.
As one more example, consider the moment of inertia of a solid sphere of constant density about an axis through its centre of mass. This is determined by summing the moments of inertia of the thin discs that form the sphere. If the surface of the ball is defined by the equation
then the radius "r" of the disc at the cross-section z along the z-axis is
Therefore, the moment of inertia of the ball is the sum of the moments of inertia of the discs along the z-axis,
where is the mass of the ball.
Moment of inertia in planar movement of a rigid body.
If a mechanical system is constrained to move parallel to a fixed plane, then the rotation of a body in the system occurs around an axis perpendicular to this plane. In this case, the moment of inertia of the mass in this system is a scalar known as the "polar moment of inertia". The definition of the polar moment of inertia can be obtained by considering momentum, kinetic energy and Newton's laws for the planar movement of a rigid system of particles.
If a system of particles, , are assembled into a rigid body, then the momentum of the system can be written in terms of positions relative to a reference point R, and absolute velocities 
where ω is the angular velocity of the system and is the velocity of .
For planar movement the angular velocity vector is directed along the unit vector which is perpendicular to the plane of movement. Introduce the unit vectors from the reference point to a point , and the unit vector so
This defines the relative position vector and the velocity vector for the rigid system of the particles moving in a plane.
Note on the cross product: When a body moves parallel to a ground plane, the trajectories of all the points in the body lie in planes parallel to this ground plane. This means that any rotation that the body undergoes must be around an axis perpendicular to this plane. Planar movement is often presented as projected onto this ground plane so that the axis of rotation appears as a point. In this case, the angular velocity and angular acceleration of the body are scalars and the fact that they are vectors along the rotation axis is ignored. This is usually preferred for introductions to the topic. But in the case of moment of inertia, the combination of mass and geometry benefits from the geometric properties of the cross product. For this reason, in this section on planar movement the angular velocity and accelerations of the body are vectors perpendicular to the ground plane, and the cross product operations are the same as used for the study of spatial rigid body movement.
Angular momentum in planar movement.
The angular momentum vector for the planar movement of a rigid system of particles is given by
Use the centre of mass C as the reference point so
and define the moment of inertia relative to the centre of mass I as
then the equation for angular momentum simplifies to
The moment of inertia about an axis perpendicular to the movement of the rigid system and through the centre of mass is known as the "polar moment of inertia".
For a given amount of angular momentum, a decrease in the moment of inertia results in an increase in the angular velocity. Figure skaters can change their moment of inertia by pulling in their arms. Thus, the angular velocity achieved by a skater with outstretched arms results in a greater angular velocity when the arms are pulled in, because of the reduced moment of inertia.
Kinetic energy in planar movement.
The kinetic energy of a rigid system of particles moving in the plane is given by
This equation expands to yield three terms
Let the reference point be the centre of mass C of the system so the second term becomes zero, and introduce the moment of inertia I so the kinetic energy is given by
The moment of inertia I is the "polar moment of inertia" of the body.
Newton's laws for planar movement.
Newton's laws for a rigid system of N particles, , can be written in terms of a resultant force and torque at a reference point , to yield
where denotes the trajectory of each particle.
The kinematics of a rigid body yields the formula for the acceleration of the particle in terms of the position and acceleration of the reference particle as well as the angular velocity vector and angular acceleration vector of the rigid system of particles as,
For systems that are constrained to planar movement, the angular velocity and angular acceleration vectors are directed along perpendicular to the plane of movement, which simplifies this acceleration equation. In this case, the acceleration vectors can be simplified by introducing the unit vectors from the reference point to a point and the unit vectors , so
This yields the resultant torque on the system as
where , and is the unit vector perpendicular to the plane for all of the particles .
Use the centre of mass as the reference point and define the moment of inertia relative to the centre of mass , then the equation for the resultant torque simplifies to
The parameter is the "polar moment of inertia" of the moving body.
The inertia matrix for spatial movement of a rigid body.
The scalar moments of inertia appear as elements in a matrix when a system of particles is assembled into a rigid body that moves in three-dimensional space. This inertia matrix appears in the calculation of the angular momentum, kinetic energy and resultant torque of the rigid system of particles.
An important application of the inertia matrix and Newton's laws of motion is the analysis of a spinning top. This is discussed in the article on gyroscopic precession. A more detailed presentation can be found in the article on Euler's equations of motion.
Let the system of particles be located at the coordinates with velocities relative to a fixed reference frame. For a (possibly moving) reference point , the relative positions are
and the (absolute) velocities are
where is the angular velocity of the system, and "R" is the velocity of .
Angular momentum.
If the reference point "R" in the assembly, or body, is chosen as the centre of mass "C", then its angular momentum takes the form,
where the terms containing "R" sum to zero by definition of the centre of mass.
To define the inertia matrix, let us first note that a skew-symmetric matrix [] could be constructed from a vector that performs the cross product operation, such that
This matrix [] has the components of as its elements, in the form
Now construct the skew-symmetric matrix [Δri]= [] obtained from the relative position vector Δri=, and use this skew-symmetric matrix to define,
where [] defined by
is the symmetric inertia matrix of the rigid system of particles measured relative to the centre of mass "C".
Kinetic energy.
The kinetic energy of a rigid system of particles can be formulated in terms of the centre of mass and a matrix of mass moments of inertia of the system. Let the system of particles be located at the coordinates r with velocities v, then the kinetic energy is
where Δri= ri-C is the position vector of a particle relative to the centre of mass.
This equation expands to yield three terms
The second term in this equation is zero because C is the centre of mass. Introduce the skew-symmetric matrix [Δr] so the kinetic energy becomes
Thus, the kinetic energy of the rigid system of particles is given by
where [I] is the inertia matrix relative to the centre of mass and M is the total mass.
Resultant torque.
The inertia matrix appears in the application of Newton's second law to a rigid assembly of particles. The resultant torque on this system is,
where a is the acceleration of the particle P. The kinematics of a rigid body yields the formula for the acceleration of the particle P in terms of the position R and acceleration A of the reference point, as well as the angular velocity vector ω and angular acceleration vector α of the rigid system as,
Use the centre of mass C as the reference point, and introduce the skew-symmetric matrix [Δri]=[r-C] to represent the cross product (r - C)x, to obtain
The calculation uses the identity
obtained from the Jacobi identity for the triple cross product as shown in the proof below:
Thus, the resultant torque on the rigid system of particles is given by
where [I] is the inertia matrix relative to the centre of mass.
Parallel axis theorem.
The inertia matrix of a body depends on the choice of the reference point. There is a useful relationship between the inertia matrix relative to the centre of mass C and the inertia matrix relative to another point R. This relationship is called the parallel axis theorem.
Consider the inertia matrix [I] obtained for a rigid system of particles measured relative to a reference point R, given by
Let C be the centre of mass of the rigid system, then
where d is the vector from the centre of mass C to the reference point R. Use this equation to compute the inertia matrix,
Expand this equation to obtain
The first term is the inertia matrix [I] relative to the centre of mass. The second and third terms are zero by definition of the centre of mass C. And the last term is the total mass of the system multiplied by the square of the skew-symmetric matrix [d] constructed from d.
The result is the parallel axis theorem,
where d is the vector from the centre of mass C to the reference point R.
Note on the minus sign: By using the skew symmetric matrix of position vectors relative to the reference point, the inertia matrix of each particle has the form −"m"[r]2, which is similar to the "mr"2 that appears in planar movement. However, to make this to work out correctly a minus sign is needed. This minus sign can be absorbed into the term "m"[r]T[r], if desired, by using the skew-symmetry property of [r].
The inertia matrix and the scalar moment of inertia around an arbitrary axis.
The scalar moment of inertia, IL, of a body about a specified axis whose direction is specified by the unit vector S and passes through the body at a point R is as follows:
where [I] is the moment of inertia matrix of the system relative to the reference point R.
This is derived as follows. Let a rigid assembly of particles, , have coordinates r. Choose R as a reference point and compute the moment of inertia around an axis L defined by the unit vector S through the reference point R. The moment of inertia of the system around this line L=R+tS is computed by determining the perpendicular vector from this axis to the particle P given by
where [I] is the identity matrix and [S S] is the outer product matrix formed from the unit vector S along the line L.
To relate this scalar moment of inertia to the inertia matrix of the body, introduce the skew-symmetric matrix [S] such that [S]y=S x y, then we have the identity
which relies on the fact that S is a unit vector.
The magnitude squared of the perpendicular vector is
The simplification of this equation uses the identity
where the dot and the cross products have been interchanged. Expand the cross products to compute
where [Δr] is the skew symmetric matrix obtained from the vector Δr=r-R.
Thus, the moment of inertia around the line L through R in the direction S is obtained from the calculation
or
where [I] is the moment of inertia matrix of the system relative to the reference point R.
This shows that the inertia matrix can be used to calculate the moment of inertia of a body around any specified rotation axis in the body.
The inertia tensor.
The inertia matrix is often described as the inertia tensor, which consists of the same moments of inertia and products of inertia about the three coordinate axes. The inertia tensor is constructed from the nine component tensors, (the symbol formula_95 is the tensor product)
where e, i=1,2,3 are the three orthogonal unit vectors defining the inertial frame in which the body moves. Using this basis the inertia tensor is given by
This tensor is of degree two because the component tensors are each constructed from two basis vectors. In this form the inertia tensor is also called the "inertia binor".
For a rigid system of particles each of mass m with position coordinates r=(x, y, z), the inertia tensor is given by
where E is the identity tensor
The inertia tensor for a continuous body is given by
where r defines the coordinates of a point in the body and ρ(r) is the mass density at that point. The integral is taken over the volume "V" of the body. The inertia tensor is symmetric because I= I.
Alternatively it can also be written in terms of the hat operator as:
The inertia tensor can be used in the same way as the inertia matrix to compute the scalar moment of inertia about an arbitrary axis in the direction n,
where the dot product is taken with the corresponding elements in the component tensors. A product of inertia term such as I is obtained by the computation
and can be interpreted as the moment of inertia around the x-axis when the object rotates around the y-axis.
The components of tensors of degree two can be assembled into a matrix. For the inertia tensor this matrix is given by,
It is common in rigid body mechanics to use notation that explicitly identifies the x, y, and z axes, such as I and I, for the components of the inertia tensor.
Identities for a skew-symmetric matrix.
To compute moment of inertia of a mass around an axis, the perpendicular vector from the mass to the axis is needed. If the axis L is defined by the unit vector S through the reference point R, then the perpendicular vector from the line L to the point r is given by
where [I] is the identity matrix and [S S] is the outer product matrix formed from the unit vector S along the line L. Recall that skew-symmetric matrix [S] is constructed so that [S]y=S x y. The matrix [I-SST] in this equation subtracts the component of Δr=r-R that is parallel to S.
The previous sections show that in computing the moment of inertia matrix this operator yields a similar operator using the components of the vector Δr that is
It is helpful to keep the following identities in mind to compare the equations that define the inertia tensor and the inertia matrix.
Let [R] be the skew symmetric matrix associated with the position vector R=(x, y, z), then the product in the inertia matrix becomes
This can be viewed as another way of computing the perpendicular distance from an axis to a point, because the matrix formed by the outer product [R R] yields the identify
where [I] is the 3x3 identity matrix.
Also notice, that
where "tr" denotes the sum of the diagonal elements of the outer product matrix, known as its trace.
The inertia matrix in different reference frames.
The use of the inertia matrix in Newton's second law assumes its components are computed relative to axes parallel to the inertial frame and not relative to a body-fixed reference frame. This means that as the body moves the components of the inertia matrix change with time. In contrast, the components of the inertia matrix measured in a body-fixed frame are constant.
Body frame inertia matrix.
Let the body frame inertia matrix relative to the centre of mass be denoted [I], and define the orientation of the body frame relative to the inertial frame by the rotation matrix [A], such that,
where vectors y in the body fixed coordinate frame have coordinates x in the inertial frame. Then, the inertia matrix of the body measured in the inertial frame is given by
Notice that [A] changes as the body moves, while [I] remains constant.
Principal axes.
Measured in the body frame the inertia matrix is a constant real symmetric matrix. A real symmetric matrix has the eigendecomposition into the product of a rotation matrix [Q] and a diagonal matrix [Λ], given by
where
The columns of the rotation matrix [Q] define the directions of the principal axes of the body, and the constants I, I and I are called the principal moments of inertia. This result was first shown by J. J. Sylvester (1852), and is a form of Sylvester's law of inertia.
For bodies with constant density an axis of rotational symmetry is a principal axis.
Inertia of an ellipsoid.
The moment of inertia matrix in body-frame coordinates is a quadratic form that defines a surface in the body called Poinsot's ellipsoid. Let [Λ] be the inertia matrix relative to the centre of mass aligned with the principal axes, then the surface
or
defines an ellipsoid in the body frame. Write this equation in the form,
to see that the semi-principal diameters of this ellipsoid are given by
Let a point x on this ellipsoid be defined in terms of its magnitude and direction, x=|x|n, where n is a unit vector. Then the relationship presented above, between the inertia matrix and the scalar moment of inertia I around an axis in the direction n, yields
Thus, the magnitude of a point x in the direction n on the inertia ellipsoid is

</doc>
<doc id="20431" url="https://en.wikipedia.org/wiki?curid=20431" title="Momentum">
Momentum

In classical mechanics, linear momentum or translational momentum (pl. momenta; SI unit kg m/s, or equivalently, N s) is the product of the mass and velocity of an object. For example, a heavy truck moving rapidly has a large momentum—it takes a large or prolonged force to get the truck up to this speed, and it takes a large or prolonged force to bring it to a stop afterwards. If the truck were lighter, or moving more slowly, then it would have less momentum.
Like velocity, linear momentum is a vector quantity, possessing a direction as well as a magnitude:
where is the three-dimensional vector stating the object's momentum in the three directions of three-dimensional space, is the three-dimensional velocity vector giving the object's rate of movement in each direction, and is the object's mass.
Linear momentum is also a "conserved" quantity, meaning that if a closed system is not affected by external forces, its total linear momentum cannot change. 
In classical mechanics, conservation of linear momentum is implied by Newton's laws. It also holds in special relativity (with a modified formula) and, with appropriate definitions, a (generalized) linear momentum conservation law holds in electrodynamics, quantum mechanics, quantum field theory, and general relativity. It is ultimately an expression of one of the fundamental symmetries of space and time, that of translational symmetry.
Newtonian mechanics.
Momentum has a direction as well as magnitude. Quantities that have both a magnitude and a direction are known as vector quantities. Because momentum has a direction, it can be used to predict the resulting direction of objects after they collide, as well as their speeds. Below, the basic properties of momentum are described in one dimension. The vector equations are almost identical to the scalar equations (see multiple dimensions).
Single particle.
The momentum of a particle is traditionally represented by the letter . It is the product of two quantities, the mass (represented by the letter ) and velocity ():
The units of momentum are the product of the units of mass and velocity. In SI units, if the mass is in kilograms and the velocity in meters per second then the momentum is in kilogram meters/second (kg m/s). An equivalent derived unit is the newton second (1 N s = 1 kg m/s). In cgs units, if the mass is in grams and the velocity in centimeters per second then the momentum is in gram centimeters/second (g cm/s) or dyne seconds (1 dyne s = 1 g m/s).
Being a vector, momentum has magnitude and direction. For example, a 1 kg model airplane, traveling due north at 1 m/s in straight and level flight, has a momentum of 1 kg m/s due north measured from the ground.
Many particles.
The momentum of a system of particles is the sum of their momenta. If two particles have masses and , and velocities and , the total momentum is
The momenta of more than two particles can be added in the same way.
A system of particles has a center of mass, a point determined by the weighted sum of their positions:
If all the particles are moving, the center of mass will generally be moving as well (unless the system is in pure rotation around it). If the center of mass is moving at velocity , the momentum is:
This is known as Euler's first law.
Relation to force.
If a force is applied to a particle for a time interval , the momentum of the particle changes by an amount
In differential form, this is Newton's second law; the rate of change of the momentum of a particle is proportional to the force acting on it,
If the force depends on time, the change in momentum (or impulse) between times and is
Under the assumption of constant mass , it is equivalent to write
so the force is equal to mass times acceleration.
"Example": A model airplane of 1 kg accelerates from rest to a velocity of 6 m/s due north in 2 s. The net force required to produce this acceleration is 3 newtons due north. The change in momentum is 6 kg m/s. The rate of change of momentum is 3 (kg m/s)/s = 3 N.
Conservation.
In a closed system (one that does not exchange any matter with its surroundings and is not acted on by external forces) the total momentum is constant. This fact, known as the "law of conservation of momentum", is implied by Newton's laws of motion. Suppose, for example, that two particles interact. Because of the third law, the forces between them are equal and opposite. If the particles are numbered 1 and 2, the second law states that and . Therefore
with the negative sign indicating that the forces oppose. Equivalently,
If the velocities of the particles are and before the interaction, and afterwards they are and , then
This law holds no matter how complicated the force is between particles. Similarly, if there are several particles, the momentum exchanged between each pair of particles adds up to zero, so the total change in momentum is zero. This conservation law applies to all interactions, including collisions and separations caused by explosive forces. It can also be generalized to situations where Newton's laws do not hold, for example in the theory of relativity and in electrodynamics.
Dependence on reference frame.
Momentum is a measurable quantity, and the measurement depends on the motion of the observer. For example: if an apple is sitting in a glass elevator that is descending, an outside observer, looking into the elevator, sees the apple moving, so, to that observer, the apple has a non-zero momentum. To someone inside the elevator, the apple does not move, so, it has zero momentum. The two observers each have a frame of reference, in which, they observe motions, and, if the elevator is descending steadily, they will see behavior that is consistent with those same physical laws.
Suppose a particle has position in a stationary frame of reference. From the point of view of another frame of reference, moving at a uniform speed , the position (represented by a primed coordinate) changes with time as
This is called a Galilean transformation. If the particle is moving at speed in the first frame of reference, in the second, it is moving at speed
Since does not change, the accelerations are the same:
Thus, momentum is conserved in both reference frames. Moreover, as long as the force has the same form, in both frames, Newton's second law is unchanged. Forces such as Newtonian gravity, which depend only on the scalar distance between objects, satisfy this criterion. This independence of reference frame is called Newtonian relativity or Galilean invariance.
A change of reference frame, can, often, simplify calculations of motion. For example, in a collision of two particles, a reference frame can be chosen, where, one particle begins at rest. Another, commonly used reference frame, is the center of mass frame - one that is moving with the center of mass. In this frame,
the total momentum is zero.
Application to collisions.
By itself, the law of conservation of momentum is not enough to determine the motion of particles after a collision. Another property of the motion, kinetic energy, must be known. This is not necessarily conserved. If it is conserved, the collision is called an "elastic collision"; if not, it is an "inelastic collision".
Elastic collisions.
An elastic collision is one in which no kinetic energy is lost. Perfectly elastic "collisions" can occur when the objects do not touch each other, as for example in atomic or nuclear scattering where electric repulsion keeps them apart. A slingshot maneuver of a satellite around a planet can also be viewed as a perfectly elastic collision from a distance. A collision between two pool balls is a good example of an "almost" totally elastic collision, due to their high rigidity; but when bodies come in contact there is always some dissipation.
A head-on elastic collision between two bodies can be represented by velocities in one dimension, along a line passing through the bodies. If the velocities are and before the collision and and after, the equations expressing conservation of momentum and kinetic energy are:
A change of reference frame can often simplify the analysis of a collision. For example, suppose there are two bodies of equal mass , one stationary and one approaching the other at a speed (as in the figure). The center of mass is moving at speed and both bodies are moving towards it at speed . Because of the symmetry, after the collision both must be moving away from the center of mass at the same speed. Adding the speed of the center of mass to both, we find that the body that was moving is now stopped and the other is moving away at speed . The bodies have exchanged their velocities. Regardless of the velocities of the bodies, a switch to the center of mass frame leads us to the same conclusion. Therefore, the final velocities are given by
In general, when the initial velocities are known, the final velocities are given by
If one body has much greater mass than the other, its velocity will be little affected by a collision while the other body will experience a large change.
Inelastic collisions.
In an inelastic collision, some of the kinetic energy of the colliding bodies is converted into other forms of energy such as heat or sound. Examples include traffic collisions, in which the effect of lost kinetic energy can be seen in the damage to the vehicles; electrons losing some of their energy to atoms (as in the Franck–Hertz experiment); and particle accelerators in which the kinetic energy is converted into mass in the form of new particles.
In a perfectly inelastic collision (such as a bug hitting a windshield), both bodies have the same motion afterwards. If one body is motionless to begin with, the equation for conservation of momentum is
so
In a frame of reference moving at the speed , the objects are brought to rest by the collision and 100% of the kinetic energy is converted.
One measure of the inelasticity of the collision is the coefficient of restitution , defined as the ratio of relative velocity of separation to relative velocity of approach. In applying this measure to ball sports, this can be easily measured using the following formula:
The momentum and energy equations also apply to the motions of objects that begin together and then move apart. For example, an explosion is the result of a chain reaction that transforms potential energy stored in chemical, mechanical, or nuclear form into kinetic energy, acoustic energy, and electromagnetic radiation. Rockets also make use of conservation of momentum: propellant is thrust outward, gaining momentum, and an equal and opposite momentum is imparted to the rocket.
Multiple dimensions.
Real motion has both direction and velocity and must be represented by a vector. In a coordinate system with axes, velocity has components in the direction, in the direction, in the direction. The vector is represented by a boldface symbol:
Similarly, the momentum is a vector quantity and is represented by a boldface symbol:
The equations in the previous sections, work in vector form if the scalars and are replaced by vectors and . Each vector equation represents three scalar equations. For example,
represents three equations:
The kinetic energy equations are exceptions to the above replacement rule. The equations are still one-dimensional, but each scalar represents the magnitude of the vector, for example,
Each vector equation represents three scalar equations. Often coordinates can be chosen so that only two components are needed, as in the figure. Each component can be obtained separately and the results combined to produce a vector result.
A simple construction involving the center of mass frame can be used to show that if a stationary elastic sphere is struck by a moving sphere, the two will head off at right angles after the collision (as in the figure).
Objects of variable mass.
The concept of momentum plays a fundamental role in explaining the behavior of variable-mass objects such as a rocket ejecting fuel or a star accreting gas. In analyzing such an object, one treats the object's mass as a function that varies with time: . The momentum of the object at time is therefore . One might then try to invoke Newton's second law of motion by saying that the external force on the object is related to its momentum by , but this is incorrect, as is the related expression found by applying the product rule to :
This equation does not correctly describe the motion of variable-mass objects. The correct equation is
where is the velocity of the ejected/accreted mass "as seen in the object's rest frame". This is distinct from , which is the velocity of the object itself as seen in an inertial frame.
This equation is derived by keeping track of both the momentum of the object as well as the momentum of the ejected/accreted mass. When considered together, the object and the mass constitute a closed system in which total momentum is conserved.
Relativistic mechanics.
Lorentz invariance.
Newtonian physics assumes that absolute time and space exist outside of any observer; this gives rise to the Galilean invariance described earlier. It also results in a prediction that the speed of light can vary from one reference frame to another. This is contrary to observation. In the special theory of relativity, Einstein keeps the postulate that the equations of motion do not depend on the reference frame, but assumes that the speed of light is invariant. As a result, position and time in two reference frames are related by the Lorentz transformation instead of the Galilean transformation.
Consider, for example, a reference frame moving relative to another at velocity in the direction. The Galilean transformation gives the coordinates of the moving frame as
while the Lorentz transformation gives
where is the Lorentz factor:
Newton's second law, with mass fixed, is not invariant under a Lorentz transformation. However, it can be made invariant by making the "inertial mass" of an object a function of velocity:
 is the object's invariant mass.
The modified momentum,
obeys Newton's second law:
Within the domain of classical mechanics, relativistic momentum closely approximates Newtonian momentum: at low velocity, is approximately equal to , the Newtonian expression for momentum.
Four-vector formulation.
In the theory of special relativity, physical quantities are expressed in terms of four-vectors that include time as a fourth coordinate along with the three space coordinates. These vectors are generally represented by capital letters, for example for position. The expression for the "four-momentum" depends on how the coordinates are expressed. Time may be given in its normal units or multiplied by the speed of light so that all the components of the four-vector have dimensions of length. If the latter scaling is used, an interval of proper time, , defined by
is invariant under Lorentz transformations (in this expression and in what follows the metric signature has been used, different authors use different conventions). Mathematically this invariance can be ensured in one of two ways: by treating the four-vectors as Euclidean vectors and multiplying time by ; or by keeping time a real quantity and embedding the vectors in a Minkowski space. In a Minkowski space, the scalar product of two four-vectors and is defined as
In all the coordinate systems, the (contravariant) relativistic four-velocity is defined by
and the (contravariant) four-momentum is
where is the invariant mass. If (in Minkowski space), then
Using Einstein's mass-energy equivalence, , this can be rewritten as
Thus, conservation of four-momentum is Lorentz-invariant and implies conservation of both mass and energy.
The magnitude of the momentum four-vector is equal to :
and is invariant across all reference frames.
The relativistic energy–momentum relationship holds even for massless particles such as photons; by setting it follows that
In a game of relativistic "billiards", if a stationary particle is hit by a moving particle in an elastic collision, the paths formed by the two afterwards will form an acute angle. This is unlike the non-relativistic case where they travel at right angles.
Generalized coordinates.
Newton's laws can be difficult to apply to many kinds of motion because the motion is limited by "constraints". For example, a bead on an abacus is constrained to move along its wire and a pendulum bob is constrained to swing at a fixed distance from the pivot. Many such constraints can be incorporated by changing the normal Cartesian coordinates to a set of "generalized coordinates" that may be fewer in number. Refined mathematical methods have been developed for solving mechanics problems in generalized coordinates. They introduce a "generalized momentum", also known as the "canonical" or "conjugate momentum", that extends the concepts of both linear momentum and angular momentum. To distinguish it from generalized momentum, the product of mass and velocity is also referred to as "mechanical", "kinetic" or "kinematic momentum". The two main methods are described below.
Lagrangian mechanics.
In Lagrangian mechanics, a Lagrangian is defined as the difference between the kinetic energy and the potential energy :
If the generalized coordinates are represented as a vector and time differentiation is represented by a dot over the variable, then the equations of motion (known as the Lagrange or Euler–Lagrange equations) are a set of equations:
If a coordinate is not a Cartesian coordinate, the associated generalized momentum component does not necessarily have the dimensions of linear momentum. Even if is a Cartesian coordinate, will not be the same as the mechanical momentum if the potential depends on velocity. Some sources represent the kinematic momentum by the symbol .
In this mathematical framework, a generalized momentum is associated with the generalized coordinates. Its components are defined as
Each component is said to be the "conjugate momentum" for the coordinate .
Now if a given coordinate does not appear in the Lagrangian (although its time derivative might appear), then
This is the generalization of the conservation of momentum.
Even if the generalized coordinates are just the ordinary spatial coordinates, the conjugate momenta are not necessarily the ordinary momentum coordinates. An example is found in the section on electromagnetism.
Hamiltonian mechanics.
In Hamiltonian mechanics, the Lagrangian (a function of generalized coordinates and their derivatives) is replaced by a Hamiltonian that is a function of generalized coordinates and momentum. The Hamiltonian is defined as
where the momentum is obtained by differentiating the Lagrangian as above. The Hamiltonian equations of motion are
As in Lagrangian mechanics, if a generalized coordinate does not appear in the Hamiltonian, its conjugate momentum component is conserved.
Symmetry and conservation.
Conservation of momentum is a mathematical consequence of the homogeneity (shift symmetry) of space (position in space is the canonical conjugate quantity to momentum). That is, conservation of momentum is a consequence of the fact that the laws of physics do not depend on position; this is a special case of Noether's theorem.
Electromagnetism.
In Newtonian mechanics, the law of conservation of momentum can be derived from the law of action and reaction, which states that every force has a reciprocating equal and opposite force. Under some circumstances, moving charged particles can exert forces on each other in non-opposite directions. Moreover, Maxwell's equations, the foundation of classical electrodynamics, are Lorentz-invariant. Nevertheless, the combined momentum of the particles and the electromagnetic field is conserved.
Vacuum.
In Maxwell's equations, the forces between particles are mediated by electric and magnetic fields. The electromagnetic force ("Lorentz force") on a particle with charge due to a combination of electric field and magnetic field (as given by the "B-field" ) is
This force imparts a momentum to the particle, so by Newton's second law the particle must impart a momentum to the electromagnetic fields.
In a vacuum, the momentum per unit volume is
where is the vacuum permeability and is the speed of light. The momentum density is proportional to the Poynting vector which gives the directional rate of energy transfer per unit area:
If momentum is to be conserved in a volume , changes in the momentum of matter through the Lorentz force must be balanced by changes in the momentum of the electromagnetic field and outflow of momentum. If is the momentum of all the particles in a volume , and the particles are treated as a continuum, then Newton's second law gives
The electromagnetic momentum is
and the equation for conservation of each component of the momentum is
The term on the right is an integral over the surface representing momentum flow into and out of the volume, and is a component of the surface normal of . The quantity is called the Maxwell stress tensor, defined as
Media.
The above results are for the "microscopic" Maxwell equations, applicable to electromagnetic forces in a vacuum (or on a very small scale in media). It is more difficult to define momentum density in media because the division into electromagnetic and mechanical is arbitrary. The definition of electromagnetic momentum density is modified to
where the H-field is related to the B-field and the magnetization by
The electromagnetic stress tensor depends on the properties of the media.
Particle in field.
If a charged particle moves in an electromagnetic field, neither its kinetic momentum nor its canonical momentum is conserved.
Lagrangian and Hamiltonian formulation.
The "kinetic momentum" is different from the "canonical momentum" (synonymous with the generalized momentum) conjugate to the ordinary position coordinates , because includes a contribution from the electric potential and vector potential :
where is the velocity (see time derivative), is the electric charge of the particle and is the Lorentz factor. See also Electromagnetism (momentum). If neither nor depends on position, is conserved.
The classical Hamiltonian for a particle in any field equals the total energy of the system – the kinetic energy (where , see dot product) plus the potential energy . For a particle in an electromagnetic field, the potential energy is , and since the kinetic energy always corresponds to the kinetic momentum , replacing the kinetic momentum by the above equation () leads to the Hamiltonian in the table.
These Lagrangian and Hamiltonian expressions can derive the Lorentz force.
Canonical commutation relations.
The kinetic momentum ( above) satisfies the commutation relation:
where: , , are indices labelling vector components, is a component of the magnetic field, and is the Levi-Civita symbol, here in 3 dimensions.
Quantum mechanics.
In quantum mechanics, momentum is defined as an operator on the wave function. The Heisenberg uncertainty principle defines limits on how accurately the momentum and position of a single observable system can be known at once. In quantum mechanics, position and momentum are conjugate variables.
For a single particle described in the position basis the momentum operator can be written as
where is the gradient operator, is the reduced Planck constant, and is the imaginary unit. This is a commonly encountered form of the momentum operator, though the momentum operator in other bases can take other forms. For example, in momentum space the momentum operator is represented as
where the operator acting on a wave function yields that wave function multiplied by the value , in an analogous fashion to the way that the position operator acting on a wave function yields that wave function multiplied by the value "x".
For both massive and massless objects, relativistic momentum is related to the de Broglie wavelength by
Electromagnetic radiation (including visible light, ultraviolet light, and radio waves) is carried by photons. Even though photons (the particle aspect of light) have no mass, they still carry momentum. This leads to applications such as the solar sail. The calculation of the momentum of light within dielectric media is somewhat controversial (see Abraham–Minkowski controversy).
Deformable bodies and fluids.
Conservation in a continuum.
In fields such as fluid dynamics and solid mechanics, it is not feasible to follow the motion of individual atoms or molecules. Instead, the materials must be approximated by a continuum in which there is a particle or fluid parcel at each point that is assigned the average of the properties of atoms in a small region nearby. In particular, it has a density and velocity that depend on time and position . The momentum per unit volume is .
Consider a column of water in hydrostatic equilibrium. All the forces on the water are in balance and the water is motionless. On any given drop of water, two forces are balanced. The first is gravity, which acts directly on each atom and molecule inside. The gravitational force per unit volume is , where is the gravitational acceleration. The second force is the sum of all the forces exerted on its surface by the surrounding water. The force from below is greater than the force from above by just the amount needed to balance gravity. The normal force per unit area is the pressure . The average force per unit volume inside the droplet is the gradient of the pressure, so the force balance equation is
If the forces are not balanced, the droplet accelerates. This acceleration is not simply the partial derivative because the fluid in a given volume changes with time. Instead, the material derivative is needed:
Applied to any physical quantity, the material derivative includes the rate of change at a point and the changes due to advection as fluid is carried past the point. Per unit volume, the rate of change in momentum is equal to . This is equal to the net force on the droplet.
Forces that can change the momentum of a droplet include the gradient of the pressure and gravity, as above. In addition, surface forces can deform the droplet. In the simplest case, a shear stress , exerted by a force parallel to the surface of the droplet, is proportional to the rate of deformation or strain rate. Such a shear stress occurs if the fluid has a velocity gradient because the fluid is moving faster on one side than another. If the speed in the direction varies with , the tangential force in direction per unit area normal to the direction is
where is the viscosity. This is also a flux, or flow per unit area, of x-momentum through the surface.
Including the effect of viscosity, the momentum balance equations for the incompressible flow of a Newtonian fluid are
These are known as the Navier–Stokes equations.
The momentum balance equations can be extended to more general materials, including solids. For each surface with normal in direction and force in direction , there is a stress component . The nine components make up the Cauchy stress tensor , which includes both pressure and shear. The local conservation of momentum is expressed by the Cauchy momentum equation:
where is the body force.
The Cauchy momentum equation is broadly applicable to deformations of solids and liquids. The relationship between the stresses and the strain rate depends on the properties of the material (see Types of viscosity).
Acoustic waves.
A disturbance in a medium gives rise to oscillations, or waves, that propagate away from their source. In a fluid, small changes in pressure can often be described by the acoustic wave equation:
where is the speed of sound. In a solid, similar equations can be obtained for propagation of pressure (P-waves) and shear (S-waves).
The flux, or transport per unit area, of a momentum component by a velocity is equal to . In the linear approximation that leads to the above acoustic equation, the time average of this flux is zero. However, nonlinear effects can give rise to a nonzero average. It is possible for momentum flux to occur even though the wave itself does not have a mean momentum.
History of the concept.
In about 530 A.D., working in Alexandria, Byzantine philosopher John Philoponus developed a concept of momentum in his commentary to Aristotle's "Physics".
Aristotle claimed that everything that is moving must be kept moving by something. For example, a thrown ball must be kept moving by motions of the air. Most writers continued to accept Aristotle's theory until the time of Galileo, but a few were skeptical. Philoponus pointed out the absurdity in Aristotle's claim that motion of an object is promoted by the same air that is resisting its passage. He proposed instead that an impetus was imparted to the object in the act of throwing it. Ibn Sīnā (also known by his Latinized name Avicenna) read Philoponus and published his own theory of motion in "The Book of Healing" in 1020. He agreed that an impetus is imparted to a projectile by the thrower; but unlike Philoponus, who believed that it was a temporary virtue that would decline even in a vacuum, he viewed it as a persistent, requiring external forces such as air resistance to dissipate it. 
The work of Philoponus, and possibly that of Ibn Sīnā, was read and refined by the European philosophers Peter Olivi and Jean Buridan. Buridan, who in about 1350 was made rector of the University of Paris, referred to impetus being proportional to the weight times the speed. Moreover, Buridan's theory was different from his predecessor's in that he did not consider impetus to be self-dissipating, asserting that a body would be arrested by the forces of air resistance and gravity which might be opposing its impetus.
René Descartes believed that the total "quantity of motion" in the universe is conserved, where the quantity of motion is understood as the product of size and speed. This should not be read as a statement of the modern law of momentum, since he had no concept of mass as distinct from weight and size, and more importantly he believed that it is speed rather than velocity that is conserved. So for Descartes if a moving object were to bounce off a surface, changing its direction but not its speed, there would be no change in its quantity of motion. Galileo, later, in his "Two New Sciences", used the Italian word "impeto".
Leibniz, in his "Discourse on Metaphysics", gave an argument against Descartes' construction of the conservation of the "quantity of motion" using an example of dropping blocks of different sizes different distances. He points out that force is conserved but quantity of motion, construed as the product of size and speed of an object, is not conserved.
The first correct statement of the law of conservation of momentum was by English mathematician John Wallis in his 1670 work, "Mechanica sive De Motu, Tractatus Geometricus": "the initial state of the body, either of rest or of motion, will persist" and "If the force is greater than the resistance, motion will result". Wallis uses "momentum" and "vis" for force. Newton's "Philosophiæ Naturalis Principia Mathematica", when it was first published in 1687, showed a similar casting around for words to use for the mathematical momentum. His Definition II defines "quantitas motus", "quantity of motion", as "arising from the velocity and quantity of matter conjointly", which identifies it as momentum. Thus when in Law II he refers to "mutatio motus", "change of motion", being proportional to the force impressed, he is generally taken to mean momentum and not motion. It remained only to assign a standard term to the quantity of motion. The first use of "momentum" in its proper mathematical sense is not clear but by the time of Jenning's "Miscellanea" in 1721, four years before the final edition of Newton's "Principia Mathematica", momentum or "quantity of motion" was being defined for students as "a rectangle", the product of and , where is "quantity of material" and is "velocity", .

</doc>
<doc id="2288549" url="https://en.wikipedia.org/wiki?curid=2288549" title="Momentum operator">
Momentum operator

In quantum mechanics, momentum (like all other physical variables) is defined as an operator, which "acts on" or pre-multiplies the wave function to extract the momentum eigenvalue from the wave function: the momentum vector a particle would have when measured in an experiment. The momentum operator is an example of a differential operator.
At the time quantum mechanics was developed in the 1920s, the momentum operator was found by many theoretical physicists, including Niels Bohr, Arnold Sommerfeld, Erwin Schrödinger, and Eugene Wigner.
Origin from De Broglie plane waves.
The momentum and energy operators can be constructed in the following way.
One dimension.
Starting in one dimension, using the plane wave solution to Schrödinger's equation:
The first order partial derivative with respect to space is
By expressing from the De Broglie relation:
the formula for the derivative of becomes:
This suggests the operator equivalence:
so the momentum value is a scalar factor, the momentum of the particle and the value that is measured, is the eigenvalue of the operator.
Since the partial derivative is a linear operator, the momentum operator is also linear, and because any wavefunction can be expressed as a superposition of other states, when this momentum operator acts on the entire superimposed wave, it yields the momentum eigenvalues for each plane wave component, the momenta add to the total momentum of the superimposed wave.
Three dimensions.
The derivation in three dimensions is the same, except the gradient operator del is used instead of one partial derivative. In three dimensions, the plane wave solution to Schrödinger's equation is:
and the gradient is
where and are the unit vectors for the three spatial dimensions, hence
This momentum operator is in position space because the partial derivatives were taken with respect to the spatial variables.
Definition (position space).
For a single particle with no electric charge and no spin, the momentum operator can be written in the position basis as:
where is the gradient operator, is the reduced Planck constant, and is the imaginary unit.
In one spatial dimension this becomes:
This is a commonly encountered form of the momentum operator, though not the most general one. For a charged particle in an electromagnetic field, described by the scalar potential  and vector potential , the momentum operator must be replaced by:
where the canonical momentum operator is the above momentum operator:
This is of course true for electrically neutral particles also, since the second term vanishes if and the original operator appears.
Properties.
Hermiticity.
The momentum operator is always a Hermitian operator when it acts on physical (in particular, normalizable) quantum states.
Canonical commutation relation.
One can easily show that by appropriately using the momentum basis and the position basis:
The Heisenberg uncertainty principle defines limits on how accurately the momentum and position of a single observable system can be known at once. In quantum mechanics, position and momentum are conjugate variables.
Fourier transform.
One can show that the Fourier transform of the momentum in quantum mechanics is the position operator. The Fourier transform turns the momentum-basis into the position-basis. The following discussion uses the bra–ket notation:
The same applies for the position operator in the momentum basis:
and other useful relations:
where stands for Dirac's delta function.
Derivation from infinitesimal translations.
The translation operator is denoted , where represents the length of the translation. It satisfies the following identity:
that becomes
Assuming the function to be analytic (i.e. differentiable in some domain of the complex plane), one may expand in a Taylor series about :
so for infinitesimal values of :
As it is known from classical mechanics, the momentum is the generator of translation, so the relation between translation and momentum operators is:
thus
4-momentum operator.
Inserting the 3d momentum operator above and the energy operator into the 4-momentum (as a 1-form with metric signature):
obtains the 4-momentum operator;
where is the 4-gradient, and the becomes preceding the 3-momentum operator. This operator occurs in relativistic quantum field theory, such as the Dirac equation and other relativistic wave equations, since energy and momentum combine into the 4-momentum vector above, momentum and energy operators correspond to space and time derivatives, and they need to be first order partial derivatives for Lorentz covariance.
The Dirac operator and Dirac slash of the 4-momentum is given by contracting with the gamma matrices:
If the signature was , the operator would be
instead.

</doc>
<doc id="402481" url="https://en.wikipedia.org/wiki?curid=402481" title="Monopole">
Monopole

Monopole may refer to:

</doc>
<doc id="56098" url="https://en.wikipedia.org/wiki?curid=56098" title="Monte Carlo method">
Monte Carlo method

Monte Carlo methods (or Monte Carlo experiments) are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. They are often used in physical and mathematical problems and are most useful when it is difficult or impossible to use other mathematical methods. Monte Carlo methods are mainly used in three distinct problem classes: optimization, numerical integration, and generating draws from a probability distribution.
In physics-related problems, Monte Carlo methods are quite useful for simulating systems with many coupled degrees of freedom, such as fluids, disordered materials, strongly coupled solids, and cellular structures (see cellular Potts model, interacting particle systems, McKean-Vlasov processes, kinetic models of gases). Other examples include modeling phenomena with significant uncertainty in inputs such as the calculation of risk in business and, in math, evaluation of multidimensional definite integrals with complicated boundary conditions. In application to space and oil exploration problems, Monte Carlo–based predictions of failure, cost overruns and schedule overruns are routinely better than human intuition or alternative "soft" methods.
In principle, Monte Carlo methods can be used to solve any problem having a probabilistic interpretation. By the law of large numbers, integrals described by the expected value of some random variable can be approximated by taking the empirical mean (a.k.a. the sample mean) of independent samples of the variable. When the probability distribution of the variable is too complex, mathematicians often use a Markov Chain Monte Carlo (MCMC) sampler. The central idea is to design a judicious Markov chain model with a prescribed stationary probability distribution. By the ergodic theorem, the stationary probability distribution is approximated by the empirical measures of the random states of the MCMC sampler.
In other important problems we are interested in generating draws from a sequence of probability distributions satisfying a nonlinear evolution equation. These flows of probability distributions can always be interpreted as the distributions of the random states of a Markov process whose transition probabilities depends on the distributions of the current random states (see McKean-Vlasov processes, nonlinear filtering equation). In other instances we are given a flow of probability distributions with an increasing level of sampling complexity (path spaces models with an increasing time horizon, Boltzmann-Gibbs measures associated with decreasing temperature parameters, and many others). These models can also be seen as the evolution of the law of the random states of a nonlinear Markov chain. A natural way to simulate these sophisticated nonlinear Markov processes is to sample a large number of copies of the process, replacing in the evolution equation the unknown distributions of the random states by the sampled empirical measures. In contrast with traditional Monte Carlo and Markov chain Monte Carlo methodologies these mean field particle techniques rely on sequential interacting samples. The terminology mean field reflects the fact that each of the "samples (a.k.a. particles, individuals, walkers, agents, creatures, or phenotypes)" interacts with the empirical measures of the process. When the size of the system tends to infinity, these random empirical measures converge to the deterministic distribution of the random states of the nonlinear Markov chain, so that the statistical interaction between particles vanishes.
Introduction.
Monte Carlo methods vary, but tend to follow a particular pattern:
For example, consider a circle inscribed in a unit square. Given that the circle and the square have a ratio of areas that is /4, the value of can be approximated using a Monte Carlo method:
In this procedure the domain of inputs is the square that circumscribes our circle. We generate random inputs by scattering grains over the square then perform a computation on each input (test whether it falls within the circle). Finally, we aggregate the results to obtain our final result, the approximation of .
There are two important points to consider here: Firstly, if the grains are not uniformly distributed, then our approximation will be poor. Secondly, there should be a large number of inputs. The approximation is generally poor if only a few grains are randomly dropped into the whole square. On average, the approximation improves as more grains are dropped.
Uses of Monte Carlo methods require large amounts of random numbers, and it was their use that spurred the development of pseudorandom number generators, which were far quicker to use than the tables of random numbers that had been previously used for statistical sampling.
History.
Before the Monte Carlo method was developed, simulations tested a previously understood deterministic problem and statistical sampling was used to estimate uncertainties in the simulations. Monte Carlo simulations invert this approach, solving deterministic problems using a probabilistic analog (see Simulated annealing).
An early variant of the Monte Carlo method can be seen in the Buffon's needle experiment, in which can be estimated by dropping needles on a floor made of parallel and equidistant strips. In the 1930s, Enrico Fermi first experimented with the Monte Carlo method while studying neutron diffusion, but did not publish anything on it.
The modern version of the Markov Chain Monte Carlo method was invented in the late 1940s by Stanislaw Ulam, while he was working on nuclear weapons projects at the Los Alamos National Laboratory. Immediately after Ulam's breakthrough, John von Neumann understood its importance and programmed the ENIAC computer to carry out Monte Carlo calculations. In 1946, physicists at Los Alamos Scientific Laboratory were investigating radiation shielding and the distance that neutrons would likely travel through various materials. Despite having most of the necessary data, such as the average distance a neutron would travel in a substance before it collided with an atomic nucleus, and how much energy the neutron was likely to give off following a collision, the Los Alamos physicists were unable to solve the problem using conventional, deterministic mathematical methods. Stanislaw Ulam had the idea of using random experiments. He recounts his inspiration as follows:
Being secret, the work of von Neumann and Ulam required a code name. A colleague of von Neumann and Ulam, Nicholas Metropolis, suggested using the name "Monte Carlo", which refers to the Monte Carlo Casino in Monaco where Ulam's uncle would borrow money from relatives to gamble. Using lists of "truly random" random numbers was extremely slow, but von Neumann developed a way to calculate pseudorandom numbers, using the middle-square method. Though this method has been criticized as crude, von Neumann was aware of this: he justified it as being faster than any other method at his disposal, and also noted that when it went awry it did so obviously, unlike methods that could be subtly incorrect.
Monte Carlo methods were central to the simulations required for the Manhattan Project, though severely limited by the computational tools at the time. In the 1950s they were used at Los Alamos for early work relating to the development of the hydrogen bomb, and became popularized in the fields of physics, physical chemistry, and operations research. The Rand Corporation and the U.S. Air Force were two of the major organizations responsible for funding and disseminating information on Monte Carlo methods during this time, and they began to find a wide application in many different fields.
The theory of more sophisticated mean field type particle Monte Carlo methods had certainly started by the mid-1960s, with the work of Henry P. McKean Jr. on Markov interpretations of a class of nonlinear parabolic partial differential equations arising in fluid mechanics. We also quote an earlier pioneering article by Theodore E. Harris and Herman Kahn, published in 1951, using mean field genetic-type Monte Carlo methods for estimating particle transmission energies. Mean field genetic type Monte Carlo methodologies are also used as heuristic natural search algorithms (a.k.a. Metaheuristic) in evolutionary computing. The origins of these mean field computational techniques can be traced to 1950 and 1954 with the work of Alan Turing on genetic type mutation-selection learning machines and the articles by Nils Aall Barricelli at the Institute for Advanced Study in Princeton, New Jersey.
Quantum Monte Carlo, and more specifically Diffusion Monte Carlo methods can also be interpreted as a mean field particle Monte Carlo approximation of Feynman-Kac path integrals. The origins of Quantum Monte Carlo methods are often attributed to Enrico Fermi and Robert Richtmyer who developed in 1948 a mean field particle interpretation of neutron-chain reactions, but the first heuristic-like and genetic type particle algorithm (a.k.a. Resampled or Reconfiguration Monte Carlo methods) for estimating ground state energies of quantum systems (in reduced matrix models) is due to Jack H. Hetherington in 1984 In molecular chemistry, the use of genetic heuristic-like particle methodologies (a.k.a. pruning and enrichment strategies) can be traced back to 1955 with the seminal work of Marshall. N. Rosenbluth and Arianna. W. Rosenbluth.
The use of Sequential Monte Carlo in advanced Signal processing and Bayesian inference is more recent. It was in 1993, that Gordon et al., published in their seminal work the first application of a Monte Carlo resampling algorithm in Bayesian statistical inference. The authors named their algorithm 'the bootstrap filter', and demonstrated that compared to other filtering methods, their bootstrap algorithm does not require any assumption about that state-space or the noise of the system. We also quote another pioneering article in this field of Genshiro Kitagawa on a related "Monte Carlo filter", and the ones by Pierre Del Moral and Himilcon Carvalho, Pierre Del Moral, André Monin and Gérard Salut on particle filters published in the mid-1990s. Particle filters were also developed in signal processing in the early 1989-1992 by P. Del Moral, J.C. Noyer, G. Rigal, and G. Salut in the LAAS-CNRS in a series of restricted and classified research reports with STCAN (Service Technique des Constructions et Armes Navales), the IT company DIGILOG, and the LAAS-CNRS (the Laboratory for Analysis and Architecture of Systems) on RADAR/SONAR and GPS signal processing problems. These Sequential Monte Carlo methodologies can be interpreted as an acceptance-rejection sampler equipped with an interacting recycling mechanism.
From 1950 to 1996, all the publications on Sequential Monte Carlo methodologies including the pruning and resample Monte Carlo methods introduced in computational physics and molecular chemistry, present natural and heuristic-like algorithms applied to different situations without a single proof of their consistency, nor a discussion on the bias of the estimates and on genealogical and ancestral tree based algorithms. The mathematical foundations and the first rigorous analysis of these particle algorithms are due to Pierre Del Moral in 1996. Branching type particle methodologies with varying population sizes were also developed in the end of the 1990s by Dan Crisan, Jessica Gaines and Terry Lyons, and by Dan Crisan, Pierre Del Moral and Terry Lyons. Further developments in this field were developed in 2000 by P. Del Moral, A. Guionnet and L. Miclo.
Definitions.
There is no consensus on how "Monte Carlo" should be defined. For example, Ripley defines most probabilistic modeling as "stochastic simulation", with "Monte Carlo" being reserved for Monte Carlo integration and Monte Carlo statistical tests. Sawilowsky distinguishes between a simulation, a Monte Carlo method, and a Monte Carlo simulation: a simulation is a fictitious representation of reality, a Monte Carlo method is a technique that can be used to solve a mathematical or statistical problem, and a Monte Carlo simulation uses repeated sampling to determine the properties of some phenomenon (or behavior). Examples:
Kalos and Whitlock point out that such distinctions are not always easy to maintain. For example, the emission of radiation from atoms is a natural stochastic process. It can be simulated directly, or its average behavior can be described by stochastic equations that can themselves be solved using Monte Carlo methods. "Indeed, the same computer code can be viewed simultaneously as a 'natural simulation' or as a solution of the equations by natural sampling."
Monte Carlo and random numbers.
Monte Carlo simulation methods do not always require truly random numbers to be useful — while for some applications, such as primality testing, unpredictability is vital. Many of the most useful techniques use deterministic, pseudorandom sequences, making it easy to test and re-run simulations. The only quality usually necessary to make good simulations is for the pseudo-random sequence to appear "random enough" in a certain sense.
What this means depends on the application, but typically they should pass a series of statistical tests. Testing that the numbers are uniformly distributed or follow another desired distribution when a large enough number of elements of the sequence are considered is one of the simplest, and most common ones. Weak correlations between successive samples is also often desirable/necessary.
Sawilowsky lists the characteristics of a high quality Monte Carlo simulation:
Pseudo-random number sampling algorithms are used to transform uniformly distributed pseudo-random numbers into numbers that are distributed according to a given probability distribution.
Low-discrepancy sequences are often used instead of random sampling from a space as they ensure even coverage and normally have a faster order of convergence than Monte Carlo simulations using random or pseudorandom sequences. Methods based on their use are called quasi-Monte Carlo methods.
Monte Carlo simulation versus "what if" scenarios.
There are ways of using probabilities that are definitely not Monte Carlo simulations — for example, deterministic modeling using single-point estimates. Each uncertain variable within a model is assigned a “best guess” estimate. Scenarios (such as best, worst, or most likely case) for each input variable are chosen and the results recorded.
By contrast, Monte Carlo simulations sample from a probability distribution for each variable to produce hundreds or thousands of possible outcomes. The results are analyzed to get probabilities of different outcomes occurring. For example, a comparison of a spreadsheet cost construction model run using traditional “what if” scenarios, and then running the comparison again with Monte Carlo simulation and triangular probability distributions shows that the Monte Carlo analysis has a narrower range than the “what if” analysis. This is because the “what if” analysis gives equal weight to all scenarios (see quantifying uncertainty in corporate finance), while the Monte Carlo method hardly samples in the very low probability regions. The samples in such regions are called "rare events".
Applications.
Monte Carlo methods are especially useful for simulating phenomena with significant uncertainty in inputs and systems with a large number of coupled degrees of freedom. Areas of application include:
Physical sciences.
Monte Carlo methods are very important in computational physics, physical chemistry, and related applied fields, and have diverse applications from complicated quantum chromodynamics calculations to designing heat shields and aerodynamic forms as well as in modeling radiation transport for radiation dosimetry calculations. In statistical physics Monte Carlo molecular modeling is an alternative to computational molecular dynamics, and Monte Carlo methods are used to compute statistical field theories of simple particle and polymer systems. Quantum Monte Carlo methods solve the many-body problem for quantum systems. In experimental particle physics, Monte Carlo methods are used for designing detectors, understanding their behavior and comparing experimental data to theory. In astrophysics, they are used in such diverse manners as to model both galaxy evolution and microwave radiation transmission through a rough planetary surface. Monte Carlo methods are also used in the ensemble models that form the basis of modern weather forecasting.
Engineering.
Monte Carlo methods are widely used in engineering for sensitivity analysis and quantitative probabilistic analysis in process design. The need arises from the interactive, co-linear and non-linear behavior of typical process simulations. For example,
Computational biology.
Monte Carlo methods are used in various fields of computational biology, for example for Bayesian inference in phylogeny, or for studying biological systems such as genomes, proteins, or membranes.
The systems can be studied in the coarse-grained or "ab initio" frameworks depending on the desired accuracy. 
Computer simulations allow us to monitor the local environment of a particular molecule to see if some chemical
reaction is happening for instance. In cases where it is not feasible to conduct a physical experiment, thought experiments can be conducted (for instance: breaking bonds, introducing impurities at specific sites, changing the local/global structure, or introducing external fields).
Computer graphics.
Path Tracing, occasionally referred to as Monte Carlo Ray Tracing, renders a 3D scene by randomly tracing samples of possible light paths. Repeated sampling of any given pixel will eventually cause the average of the samples to converge on the correct solution of the rendering equation, making it one of the most physically accurate 3D graphics rendering methods in existence.
Applied statistics.
In applied statistics, Monte Carlo methods are generally used for two purposes:
Monte Carlo methods are also a compromise between approximate randomization and permutation tests. An approximate randomization test is based on a specified subset of all permutations (which entails potentially enormous housekeeping of which permutations have been considered). The Monte Carlo approach is based on a specified number of randomly drawn permutations (exchanging a minor loss in precision if a permutation is drawn twice – or more frequently—for the efficiency of not having to track which permutations have already been selected).
Artificial intelligence for games.
Monte Carlo methods have been developed into a technique called Monte-Carlo tree search that is useful for searching for the best move in a game. Possible moves are organized in a search tree and a large number of random simulations are used to estimate the long-term potential of each move. A black box simulator represents the opponent's moves.
The Monte Carlo Tree Search (MCTS) method has four steps:
The net effect, over the course of many simulated games, is that the value of a node representing a move will go up or down, hopefully corresponding to whether or not that node represents a good move.
Monte Carlo Tree Search has been used successfully to play games such as Go, Tantrix, Battleship, Havannah, and Arimaa.
Design and visuals.
Monte Carlo methods are also efficient in solving coupled integral differential equations of radiation fields and energy transport, and thus these methods have been used in global illumination computations that produce photo-realistic images of virtual 3D models, with applications in video games, architecture, design, computer generated films, and cinematic special effects.
Finance and business.
Monte Carlo methods in finance are often used to evaluate investments in projects at a business unit or corporate level, or to evaluate financial derivatives. They can be used to model project schedules, where simulations aggregate estimates for worst-case, best-case, and most likely durations for each task to determine outcomes for the overall project. Monte Carlo methods are also used in option pricing, default risk analysis.
Use in mathematics.
In general, Monte Carlo methods are used in mathematics to solve various problems by generating suitable random numbers (see also Random number generation) and observing that fraction of the numbers that obeys some property or properties. The method is useful for obtaining numerical solutions to problems too complicated to solve analytically. The most common application of the Monte Carlo method is Monte Carlo integration.
Integration.
Deterministic numerical integration algorithms work well in a small number of dimensions, but encounter two problems when the functions have many variables. First, the number of function evaluations needed increases rapidly with the number of dimensions. For example, if 10 evaluations provide adequate accuracy in one dimension, then 10100 points are needed for 100 dimensions—far too many to be computed. This is called the curse of dimensionality. Second, the boundary of a multidimensional region may be very complicated, so it may not be feasible to reduce the problem to an iterated integral. 100 dimensions is by no means unusual, since in many physical problems, a "dimension" is equivalent to a degree of freedom.
Monte Carlo methods provide a way out of this exponential increase in computation time. As long as the function in question is reasonably well-behaved, it can be estimated by randomly selecting points in 100-dimensional space, and taking some kind of average of the function values at these points. By the central limit theorem, this method displays formula_1 convergence—i.e., quadrupling the number of sampled points halves the error, regardless of the number of dimensions.
A refinement of this method, known as importance sampling in statistics, involves sampling the points randomly, but more frequently where the integrand is large. To do this precisely one would have to already know the integral, but one can approximate the integral by an integral of a similar function or use adaptive routines such as stratified sampling, recursive stratified sampling, adaptive umbrella sampling or the VEGAS algorithm.
A similar approach, the quasi-Monte Carlo method, uses low-discrepancy sequences. These sequences "fill" the area better and sample the most important points more frequently, so quasi-Monte Carlo methods can often converge on the integral more quickly.
Another class of methods for sampling points in a volume is to simulate random walks over it (Markov chain Monte Carlo). Such methods include the Metropolis-Hastings algorithm, Gibbs sampling, Wang and Landau algorithm, and interacting type MCMC methodologies such as the Sequential Monte Carlo samplers.
Simulation and optimization.
Another powerful and very popular application for random numbers in numerical simulation is in numerical optimization. The problem is to minimize (or maximize) functions of some vector that often has a large number of dimensions. Many problems can be phrased in this way: for example, a computer chess program could be seen as trying to find the set of, say, 10 moves that produces the best evaluation function at the end. In the traveling salesman problem the goal is to minimize distance traveled. There are also applications to engineering design, such as multidisciplinary design optimization. It has been applied with quasi-one-dimensional models to solve particle dynamics problems by efficiently exploring large configuration space.
The traveling salesman problem is what is called a conventional optimization problem. That is, all the facts (distances between each destination point) needed to determine the optimal path to follow are known with certainty and the goal is to run through the possible travel choices to come up with the one with the lowest total distance. However, let's assume that instead of wanting to minimize the total distance traveled to visit each desired destination, we wanted to minimize the total time needed to reach each destination. This goes beyond conventional optimization since travel time is inherently uncertain (traffic jams, time of day, etc.). As a result, to determine our optimal path we would want to use simulation - optimization to first understand the range of potential times it could take to go from one point to another (represented by a probability distribution in this case rather than a specific distance) and then optimize our travel decisions to identify the best path to follow taking that uncertainty into account.
Inverse problems.
Probabilistic formulation of inverse problems leads to the definition of a probability distribution in the model space. This probability distribution combines prior information with new information obtained by measuring some observable parameters (data). As, in the general case, the theory linking data with model parameters is nonlinear, the posterior probability in the model space may not be easy to describe (it may be multimodal, some moments may not be defined, etc.).
When analyzing an inverse problem, obtaining a maximum likelihood model is usually not sufficient, as we normally also wish to have information on the resolution power of the data. In the general case we may have a large number of model parameters, and an inspection of the marginal probability densities of interest may be impractical, or even useless. But it is possible to pseudorandomly generate a large collection of models according to the posterior probability distribution and to analyze and display the models in such a way that information on the relative likelihoods of model properties is conveyed to the spectator. This can be accomplished by means of an efficient Monte Carlo method, even in cases where no explicit formula for the "a priori" distribution is available.
The best-known importance sampling method, the Metropolis algorithm, can be generalized, and this gives a method that allows analysis of (possibly highly nonlinear) inverse problems with complex "a priori" information and data with an arbitrary noise distribution.
Petroleum reservoir management.
Monte Carlo methods are very popular in hydrocarbon reservoir management in the context of nonlinear inverse problems. This includes generating computational models of oil and gas reservoirs for consistency with observed production data. For the goal of decision making and uncertainty assessment, Monte Carlo methods are used for generating multiple geological realizations.

</doc>
<doc id="995746" url="https://en.wikipedia.org/wiki?curid=995746" title="Multiplier (Fourier analysis)">
Multiplier (Fourier analysis)

In Fourier analysis, a multiplier operator is a type of linear operator, or transformation of functions. These operators act on a function by altering its Fourier transform. Specifically they multiply the Fourier transform of a function by a specified function known as the multiplier or symbol. Occasionally, the term "multiplier operator" itself is shortened simply to "multiplier". In simple terms, the multiplier reshapes the frequencies involved in any function. This class of operators turns out to be broad: general theory shows that a translation-invariant operator on a group which obeys some (very mild) regularity conditions can be expressed as a multiplier operator, and conversely. Many familiar operators, such as translations and differentiation, are multiplier operators, although there are many more complicated examples such as the Hilbert transform.
In signal processing, a multiplier operator is called a "filter", and the multiplier is the filter's frequency response (or transfer function).
In the wider context, multiplier operators are special cases of spectral multiplier operators, which arise from the functional calculus of an operator (or family of commuting operators). They are also special cases of pseudo-differential operators, and more generally Fourier integral operators. There are natural questions in this field that are still open, such as characterizing the "Lp" bounded multiplier operators (see below). In this context, multipliers are unrelated to Lagrange multipliers, except for the fact that they both involve the multiplication operation.
"For the necessary background on the Fourier transform, see that page. Additional important background may be found on the pages operator norm and "Lp" space."
Examples.
In the setting of periodic functions defined on the unit circle, the Fourier transform of a function is simply the sequence of its Fourier coefficients. To see that differentiation can be realized as multiplier, consider the Fourier series for the derivative of a periodic function "ƒ"("t"). After using integration by parts in the definition of the Fourier coefficient we have that
So, formally, it follows that the Fourier series for the derivative is simply "in" multiplied by the Fourier series for "ƒ". This is the same as saying that differentiation is a multiplier operator with multiplier "in".
An example of a multiplier operator acting on functions on the real line is the Hilbert transform. It can be shown that the Hilbert transform is a multiplier operator whose multiplier is given by the m("ξ") = −"i" sgn("ξ"), where sgn is the signum function.
Finally another important example of a multiplier is the characteristic function of the unit ball in R"n" which arises in the study of "partial sums" for the Fourier transform (see Convergence of Fourier series).
Definition.
Multiplier operators can be defined on any group "G" for which the Fourier transform is also defined (in particular, on any locally compact abelian group). The general definition is as follows. If "f" : "G" → C is a sufficiently regular function, let formula_2 denote its Fourier transform (where formula_3 is the Pontryagin dual of "G"). Let formula_4 denote another function, which we shall call the "multiplier". Then the multiplier operator formula_5 associated to this symbol "m" is defined via the formula
In other words, the Fourier transform of "Tf" at a frequency ξ is given by the Fourier transform of "f" at that frequency, multiplied by the value of the multiplier at that frequency. This explains the terminology "multiplier".
Note that the above definition only defines Tf implicitly; in order to recover "Tf" explicitly one needs to invert the Fourier transform. This can be easily done if both "f" and "m" are sufficiently smooth and integrable. One of the major problems in the subject is to determine, for any specified multiplier "m", whether the corresponding Fourier multiplier operator continues to be well-defined when "f" has very low regularity, for instance if it is only assumed to lie in an "Lp" space. See the discussion on the "boundedness problem" below. As a bare minimum, one usually requires the multiplier "m" to be bounded and measurable; this is sufficient to establish boundedness on formula_7 but is in general not strong enough to give boundedness on other spaces.
One can view the multiplier operator "T" as the composition of three operators, namely the Fourier transform, the operation of pointwise multiplication by "m", and then the inverse Fourier transform. Equivalently, "T" is the conjugation of the pointwise multiplication operator by the Fourier transform. Thus one can think of multiplier operators as operators which are diagonalized by the Fourier transform.
Multiplier operators on common groups.
We now specialize the above general definition to specific groups "G". First consider the unit circle
formula_8; functions on "G" can thus be thought of as 2π-periodic functions on the real line. In this group, the Pontryagin dual is the group of integers, formula_9. The Fourier transform (for sufficiently regular functions "f") is given by
and the inverse Fourier transform is given by
A multiplier in this setting is simply a sequence formula_12 of numbers, and the operator
formula_5 associated to this multiplier is then given by the formula
at least for sufficiently well-behaved choices of the multiplier formula_12 and the function "f".
Now let "G" be a Euclidean space formula_16. Here the dual group is also Euclidean, formula_17, and the Fourier and inverse Fourier transforms are given by the formulae
A multiplier in this setting is a function formula_20, and the associated multiplier operator formula_5 is defined by
again assuming sufficiently strong regularity and boundedness assumptions on the multiplier and function.
In the sense of distributions, there is no difference between multiplier operators and convolution operators; every multiplier "T" can also be expressed in the form "Tf" = "f*K" for some distribution "K", known as the "convolution kernel" of "T". In this view, translation by an amount "x"0 is convolution with a Dirac delta function δ(· − "x"0), differentiation is convolution with δ'. Further examples are given in the table below.
Further examples.
The following table shows some common examples of multiplier operators on the unit circle formula_23.
The following table shows some common examples of multiplier operators on Euclidean space formula_16.
General considerations.
The map formula_25 is a homomorphism of C*-algebras. This follows because the sum of two multiplier operators formula_26 and formula_27 is a multiplier operators with multiplier formula_28, the composition of these two multiplier operators is a multiplier operator with multiplier formula_29, and the adjoint of a multiplier operator formula_26 is another multiplier operator with multiplier formula_31.
In particular, we see that any two multiplier operators commute with each other. It is known that multiplier operators are translation-invariant. Conversely, one can show that any translation-invariant linear operator which is bounded on "L"2("G") is a multiplier operator.
The "Lp" boundedness problem.
The "Lp" boundedness problem (for any particular "p") for a given group "G" is, stated simply, to identify the multipliers "m" such that the corresponding multiplier operator is bounded from "Lp"("G") to "Lp"("G"). Such multipliers are usually simply referred to as ""Lp" multipliers". Note that as multiplier operators are always linear, such operators are bounded if and only if they are continuous. This problem is considered to be extremely difficult in general, but many special cases can be treated. The problem depends greatly on "p", although there is a duality relationship: if formula_32 and 1 ≤ "p", "q" ≤ ∞, then a multiplier operator is bounded on "Lp" if and only if it is bounded on "Lq".
The Riesz-Thorin theorem shows that if a multiplier operator is bounded on two different "Lp" spaces, then it is also bounded on all intermediate spaces. Hence we get that the space of multipliers is smallest for "L"1 and "L"∞ and grows as one approaches "L"2, which has the largest multiplier space.
Boundedness on "L"2.
This is the easiest case. Parseval's theorem allows to solve this problem completely and obtain that a function "m" is an "L"2("G") multiplier if and only if it is bounded and measurable.
Boundedness on "L"1 or "L"∞.
This case is more complicated than the Hilbertian ("L"2) case, but is fully resolved. The following is true:
Theorem: "In the euclidean space ℝ"n", a function formula_33 is an" "L"1 "multiplier (equivalently an "L"∞ multiplier) if and only if there exists a finite Borel measure μ such that" "m" "is the Fourier transform of μ."
Boundedness on "L""p" for 1 < "p" < ∞.
In this general case, necessary and sufficient conditions for boundedness have not been established, even for Euclidean space or the unit circle. However, several necessary conditions and several sufficient conditions are known. For instance it is known that in order for a multiplier operator to be bounded on even a single "Lp" space, the multiplier must be bounded and measurable (this follows from the characterisation of "L"2 multipliers above and the inclusion property). However, this is not sufficient except when "p" = 2.
Results that give sufficient conditions for boundedness are known as multiplier theorems. Two such results are given below.
Marcinkiewicz multiplier theorem.
Let formula_34 be a bounded function that is continuously differentiable in every set of the form formula_35 for formula_36 and has derivative such that
Then "m" is an "Lp" multiplier for all 1 < "p" < ∞.
Mikhlin multiplier theorem.
Let "m" be a bounded function on R"n" which is smooth except possibly at the origin, and such that the function formula_38 is bounded for all integers formula_39: then "m" is an "Lp" multiplier for all 1 < "p" < ∞.
This is a special case of the Hörmander-Mikhlin multiplier theorem.
The proofs of these two theorems are fairly tricky, involving techniques from Calderón–Zygmund theory
and the Marcinkiewicz interpolation theorem: for the original proof, see or .
Examples.
Translations are bounded operators on any "Lp". Differentiation is not bounded on any "Lp". The Hilbert transform is bounded only for "p" strictly between 1 and ∞. The fact that it is unbounded on "L"∞ is easy, since it is well known that the Hilbert transform of a step function is unbounded. Duality gives the same for "p" = 1. However, both the Marcinkiewicz and Mikhlin multiplier theorems show that the Hilbert transform is bounded in "Lp" for all 1 < "p" < ∞.
Another interesting case on the unit circle is when the sequence formula_40 is constant on the intervals formula_41 and formula_42. From the Marcinkiewicz multiplier theorem (adapted to the context of the unit circle) we see that any such sequence (bounded, of course) is a multiplier for every 1 < "p" < ∞.
In one dimension, the disk multiplier operator formula_43 is bounded on "Lp" for every 1 < "p" < ∞. However, in 1972, Charles Fefferman showed the surprising result that in two and higher dimensions the disk multiplier operator formula_43 is unbounded on "Lp" for every "p" ≠ 2. The corresponding problem for Bochner–Riesz multipliers is only partially solved; see also Bochner–Riesz operator and Bochner–Riesz conjecture.

</doc>
<doc id="20146" url="https://en.wikipedia.org/wiki?curid=20146" title="Muon">
Muon

The muon (; from the Greek letter mu (μ) used to represent it) is an elementary particle similar to the electron, with electric charge of −1 "e" and a spin of, but with a much greater mass (). It is classified as a lepton, together with the electron (mass ), the tau (mass ), and the three neutrinos (electron neutrino , muon neutrino and tau neutrino ). As is the case with other leptons, the muon is not believed to have any sub-structure—that is, it is not thought to be composed of any simpler particles.
The muon is an unstable subatomic particle with a mean lifetime of . Among all known unstable subatomic particles, only the neutron (lasting around 15 minutes) and some atomic nuclei have a longer decay lifetime; others decay significantly faster. The decay of the muon (as well as of the neutron, the longest-lived unstable baryon), is mediated by the weak interaction exclusively. Muon decay always produces at least three particles, which must include an electron of the same charge as the muon and two neutrinos of different types.
Like all elementary particles, the muon has a corresponding antiparticle of opposite charge (+1 "e") but equal mass and spin: the antimuon (also called a "positive muon"). Muons are denoted by and antimuons by . Muons were previously called mu mesons, but are not classified as mesons by modern particle physicists (see ), and that name is no longer used by the physics community.
Muons have a mass of , which is about 207 times that of the electron. Due to their greater mass, muons are not as sharply accelerated when they encounter electromagnetic fields, and do not emit as much bremsstrahlung (deceleration radiation). This allows muons of a given energy to penetrate far more deeply into matter than electrons, since the deceleration of electrons and muons is primarily due to energy loss by the bremsstrahlung mechanism. As an example, so-called "secondary muons", generated by cosmic rays hitting the atmosphere, can penetrate to the Earth's surface, and even into deep mines.
Because muons have a very large mass and energy compared with the decay energy of radioactivity, they are never produced by radioactive decay. They are, however, produced in copious amounts in high-energy interactions in normal matter, in certain particle accelerator experiments with hadrons, or naturally in cosmic ray interactions with matter. These interactions usually produce pi mesons initially, which most often decay to muons.
As with the case of the other charged leptons, the muon has an associated muon neutrino, denoted by , which is not the same particle as the electron neutrino, and does not participate in the same nuclear reactions.
History.
Muons were discovered by Carl D. Anderson and Seth Neddermeyer at Caltech in 1936, while studying cosmic radiation. Anderson had noticed particles that curved differently from electrons and other known particles when passed through a magnetic field. They were negatively charged but curved less sharply than electrons, but more sharply than protons, for particles of the same velocity. It was assumed that the magnitude of their negative electric charge was equal to that of the electron, and so to account for the difference in curvature, it was supposed that their mass was greater than an electron but smaller than a proton. Thus Anderson initially called the new particle a "mesotron", adopting the prefix "meso-" from the Greek word for "mid-". The existence of the muon was confirmed in 1937 by J. C. Street and E. C. Stevenson's cloud chamber experiment.
A particle with a mass in the meson range had been predicted before the discovery of any mesons, by theorist Hideki Yukawa:
"It seems natural to modify the theory of Heisenberg and Fermi in the following way. The transition of a heavy particle from neutron state to proton state is not always accompanied by the emission of light particles. The transition is sometimes taken up by another heavy particle."
Because of its mass, the mu meson was initially thought to be Yukawa's particle, but it later proved to have the wrong properties. Yukawa's predicted particle, the pi meson, was finally identified in 1947 (again from cosmic ray interactions), and shown to differ from the earlier-discovered mu meson by having the correct properties to be a particle which mediated the nuclear force.
With two particles now known with the intermediate mass, the more general term "meson" was adopted to refer to any such particle within the correct mass range between electrons and nucleons. Further, in order to differentiate between the two different types of mesons after the second meson was discovered, the initial mesotron particle was renamed the "mu meson" (the Greek letter "μ" ("mu") corresponds to "m"), and the new 1947 meson (Yukawa's particle) was named the pi meson.
As more types of mesons were discovered in accelerator experiments later, it was eventually found that the mu meson significantly differed not only from the pi meson (of about the same mass), but also from all other types of mesons. The difference, in part, was that mu mesons did not interact with the nuclear force, as pi mesons did (and were required to do, in Yukawa's theory). Newer mesons also showed evidence of behaving like the pi meson in nuclear interactions, but not like the mu meson. Also, the mu meson's decay products included both a neutrino and an antineutrino, rather than just one or the other, as was observed in the decay of other charged mesons.
In the eventual Standard Model of particle physics codified in the 1970s, all mesons other than the mu meson were understood to be hadrons—that is, particles made of quarks—and thus subject to the nuclear force. In the quark model, a "meson" was no longer defined by mass (for some had been discovered that were very massive—more than nucleons), but instead were particles composed of exactly two quarks (a quark and antiquark), unlike the baryons, which are defined as particles composed of three quarks (protons and neutrons were the lightest baryons). Mu mesons, however, had shown themselves to be fundamental particles (leptons) like electrons, with no quark structure. Thus, mu mesons were not mesons at all, in the new sense and use of the term "meson" used with the quark model of particle structure.
With this change in definition, the term "mu meson" was abandoned, and replaced whenever possible with the modern term "muon", making the term mu meson only historical. In the new quark model, other types of mesons sometimes continued to be referred to in shorter terminology (e.g., "pion" for pi meson), but in the case of the muon, it retained the shorter name and was never again properly referred to by older "mu meson" terminology.
The eventual recognition of the "mu meson" muon as a simple "heavy electron" with no role at all in the nuclear interaction, seemed so incongruous and surprising at the time, that Nobel laureate I. I. Rabi famously quipped, "Who ordered that?"
In the Rossi–Hall experiment (1941), muons were used to observe the time dilation (or alternately, length contraction) predicted by special relativity, for the first time.
Muon sources.
On Earth, most naturally occurring muons are created by quasars and supernovas, which consist mostly of protons, many arriving from deep space at very high energy
When a cosmic ray proton impacts atomic nuclei in the upper atmosphere, pions are created. These decay within a relatively short distance (meters) into muons (their preferred decay product), and muon neutrinos. The muons from these high energy cosmic rays generally continue in about the same direction as the original proton, at a velocity near the speed of light. Although their lifetime "without" relativistic effects would allow a half-survival distance of only about 456 m (2,197 µs×ln(2) × 0,9997×c) at most (as seen from Earth) the time dilation effect of special relativity (from the viewpoint of the Earth) allows cosmic ray secondary muons to survive the flight to the Earth's surface, since in the Earth frame, the muons have a longer half life due to their velocity. From the viewpoint (inertial frame) of the muon, on the other hand, it is the length contraction effect of special relativity which allows this penetration, since in the muon frame, its lifetime is unaffected, but the length contraction causes distances through the atmosphere and Earth to be far shorter than these distances in the Earth rest-frame. Both effects are equally valid ways of explaining the fast muon's unusual survival over distances.
Since muons are unusually penetrative of ordinary matter, like neutrinos, they are also detectable deep underground (700 meters at the Soudan 2 detector) and underwater, where they form a major part of the natural background ionizing radiation. Like cosmic rays, as noted, this secondary muon radiation is also directional.
The same nuclear reaction described above (i.e. hadron-hadron impacts to produce pion beams, which then quickly decay to muon beams over short distances) is used by particle physicists to produce muon beams, such as the beam used for the muon "g" − 2 experiment.
Muon decay.
Muons are unstable elementary particles and are heavier than electrons and neutrinos but lighter than all other matter particles. They decay via the weak interaction. Because lepton numbers must be conserved, one of the product neutrinos of muon decay must be a muon-type neutrino and the other an electron-type antineutrino (antimuon decay produces the corresponding antiparticles, as detailed below). Because charge must be conserved, one of the products of muon decay is always an electron of the same charge as the muon (a positron if it is a positive muon). Thus all muons decay to at least an electron, and two neutrinos. Sometimes, besides these necessary products, additional other particles that have no net charge and spin of zero (e.g., a pair of photons, or an electron-positron pair), are produced.
The dominant muon decay mode (sometimes called the Michel decay after Louis Michel) is the simplest possible: the muon decays to an electron, an electron antineutrino, and a muon neutrino. Antimuons, in mirror fashion, most often decay to the corresponding antiparticles: a positron, an electron neutrino, and a muon antineutrino. In formulaic terms, these two decays are:
The mean lifetime, = 1/, of the (positive) muon is (. The equality of the muon and antimuon lifetimes has been established to better than one part in 104.
The muon decay width which follows from Fermi's golden rule follows Sargent's law of fifth-power dependence on ,
where formula_2, formula_3 is the Fermi coupling constant and formula_4 is the fraction of the maximum energy transmitted to the electron.
The decay distributions of the electron in muon decays have been parameterised using the so-called Michel parameters. The values of these four parameters are predicted unambiguously in the Standard Model of particle physics, thus muon decays represent a good test of the space-time structure of the weak interaction. No deviation from the Standard Model predictions has yet been found.
For the decay of the muon, the expected decay distribution for the Standard Model values of Michel parameters is
where formula_6 is the angle between the muon's polarization vector formula_7 and the decay-electron momentum vector, and formula_8 is the fraction of muons that are forward-polarized. Integrating this expression over electron energy gives the angular distribution of the daughter electrons:
The electron energy distribution integrated over the polar angle (valid for formula_10) is
Due to the muons decaying by the weak interaction, parity conservation is violated. Replacing the formula_12 term in the expected decay values of the Michel Parameters with a formula_13 term, where is the Larmor frequency from Larmor precession of the muon in a uniform magnetic field, given by:
formula_14
where is mass of the muon, is charge, is the muon g-factor and is applied field.
A change in the electron distribution computed using the standard, unprecessional, Michel Parameters can be seen displaying a periodicity of π radians. This can be shown to physically correspond to a phase change of π, introduced in the electron distribution as the angular momentum is changed by the action of the charge conjugation operator, which is conserved by the weak interaction.
The observation of Parity violation in muon decay can be compared to the concept of violation of parity in weak interactions in general as an extension of The Wu Experiment, as well as the change of angular momentum introduced by a phase change of π corresponding to the charge-parity operator being invariant in this interaction. This fact is true for all lepton interactions in The Standard Model.
Certain neutrino-less decay modes are kinematically allowed but forbidden in the Standard Model. Examples forbidden by lepton flavour conservation are:
Observation of such decay modes would constitute clear evidence for theories beyond the Standard Model. Upper limits for the branching fractions of such decay modes were measured in many experiments starting more than years ago. The current upper limit for the → + branching fraction was measured 2013 in the MEG experiment and is 5.7 × 10−13.
Muonic atoms.
The muon was the first elementary particle discovered that does not appear in ordinary atoms. Negative muons can, however, form muonic atoms (also called mu-mesic atoms), by replacing an electron in ordinary atoms. Muonic hydrogen atoms are much smaller than typical hydrogen atoms because the much larger mass of the muon gives it a much more localized ground-state wavefunction than is observed for the electron. In multi-electron atoms, when only one of the electrons is replaced by a muon, the size of the atom continues to be determined by the other electrons, and the atomic size is nearly unchanged. However, in such cases the orbital of the muon continues to be smaller and far closer to the nucleus than the atomic orbitals of the electrons.
Muonic helium is created by substituting a muon for one of the electrons in helium-4. The muon orbits much closer to the nucleus, so muonic helium can therefore be regarded like an isotope of helium whose nucleus consists of two neutrons, two protons and a muon, with a single electron outside. Colloquially, it could be called "helium 4.1", since the mass of the muon is roughly 0.1 amu. Chemically, muonic helium, possessing an unpaired valence electron, can bond with other atoms, and behaves more like a hydrogen atom than an inert helium atom.
A positive muon, when stopped in ordinary matter, can also bind an electron and form an exotic atom known as muonium (Mu) atom, in which the muon acts as the nucleus. The positive muon, in this context, can be considered a pseudo-isotope of hydrogen with one ninth of the mass of the proton. Because the reduced mass of muonium, and hence its Bohr radius, is very close to that of hydrogen, this short-lived "atom" behaves chemically — to a first approximation — like hydrogen, deuterium and tritium.
Use in measurement of the proton charge radius.
The experimental technique that is expected to provide the most precise determination of the root-mean-square charge radius of the proton is the measurement of the frequency of photons (precise "color" of light ) emitted or absorbed by atomic transitions in muonic hydrogen. This form of hydrogen atom is composed of a negatively charged muon bound to a proton. The muon is particularly well suited for this purpose because its much larger mass results in a much more compact bound state and hence a larger probability for it to be found inside the proton in muonic hydrogen compared to the electron in atomic hydrogen. The Lamb shift in muonic hydrogen was measured by driving the muon from a 2s state up to an excited 2p state using a laser. The frequency of the photons required to induce two such (slightly different) transitions were reported in 2014 to be 50 and 55 THz which, according to present theories of quantum electrodynamics, yield an appropriately averaged value of for the charge radius of the proton.
The internationally accepted value of the proton's charge radius is based on a suitable average of results from older measurements of effects caused by the nonzero size of the proton on scattering of electrons by nuclei and the light spectrum (photon energies) from excited atomic hydrogen. The official value updated in 2014 is (see orders of magnitude for comparison to other sizes). 
The expected precision of this result is inferior to that from muonic hydrogen by about a factor of fifteen, yet they disagree by about 5.6 times the nominal uncertainty in the difference (a discrepancy called 5.6σ in scientific notation). A conference of the world experts on this topic led to the decision to exclude the muon result from influencing the official 2014 value, in order to avoid hiding the mysterious discrepancy.
This "proton radius puzzle" remained unresolved as of late 2015, and has attracted much attention, in part because of the possibility that both measurements are valid, which would imply the influence of some "new physics".
Anomalous magnetic dipole moment.
The anomalous magnetic dipole moment is the difference between the experimentally observed value of the magnetic dipole moment and the theoretical value predicted by the Dirac equation. The measurement and prediction of this value is very important in the precision tests of QED (quantum electrodynamics). The E821 experiment at Brookhaven National Laboratory (BNL) studied the precession of muon and anti-muon in a constant external magnetic field as they circulated in a confining storage ring. E821 reported the following average value in 2006:
where the first errors are statistical and the second systematic.
The prediction for the value of the muon anomalous magnetic moment includes three parts:
The difference between the g-factors of the muon and the electron is due to their difference in mass. Because of the muon's larger mass, contributions to the theoretical calculation of its anomalous magnetic dipole moment from Standard Model weak interactions and from contributions involving hadrons are important at the current level of precision, whereas these effects are not important for the electron. The muon's anomalous magnetic dipole moment is also sensitive to contributions from new physics beyond the Standard Model, such as supersymmetry. For this reason, the muon's anomalous magnetic moment is normally used as a probe for new physics beyond the Standard Model rather than as a test of QED. A new experiment at Fermilab using the E821 magnet will improve the precision of this measurement.
Muon radiography and tomography.
Since muons are much more deeply penetrating than X-rays or gamma rays, muon imaging can be used with much thicker material or, with cosmic ray sources, larger objects. One example is commercial muon tomography used to image entire cargo containers to detect shielded nuclear material, as well as explosives or other contraband.
The technique of muon transmission radiography based on cosmic ray sources was first used in the 1950s to measure the depth of the overburden of a tunnel in Australia
and in the 1960s to search for possible hidden chambers in the Pyramid of Chephren in Giza.
In 2003, the scientists at Los Alamos National Laboratory developed a new imaging technique: muon scattering tomography. With muon scattering tomography, both incoming and outgoing trajectories for each particle are reconstructed, such as with sealed aluminum drift tubes. Since the development of this technique, several companies have started to use it.
In August 2014, Decision Sciences International Corporation announced it had been awarded a contract by Toshiba for use of its muon tracking detectors in reclaiming the Fukushima nuclear complex. The Fukushima Daiichi Tracker (FDT) was proposed to make a few months of muon measurements to show the distribution of the reactor cores.
In December 2014, Tepco reported that they would be using two different muon imaging techniques at Fukushima, "Muon Scanning Method" on Unit 1 (the most badly damaged, where the fuel may have left the reactor vessel) and "Muon Scattering Method" on Unit 2.
The International Research Institute for Nuclear Decommissioning IRID in Japan and the High Energy Accelerator Research Organization KEK call the method they developed for Unit 1 the muon permeation method; 1,200 optical fibers for wavelength conversion light up when muons come into contact with them. After a month of data collection, it is hoped to reveal the location and amount of fuel debris still inside the reactor. The measurements began in February 2015.

</doc>
<doc id="241033" url="https://en.wikipedia.org/wiki?curid=241033" title="Muon neutrino">
Muon neutrino

The muon neutrino is a subatomic lepton elementary particle which has the symbol and no net electric charge. Together with the muon it forms the second generation of leptons, hence its name "muon neutrino". It was first hypothesized in the early 1940s by several people, and was discovered in 1962 by Leon Lederman, Melvin Schwartz and Jack Steinberger. The discovery was rewarded with the 1988 Nobel Prize in Physics.
Discovery.
In 1962 Leon M. Lederman, Melvin Schwartz and Jack Steinberger established by performing an experiment at the Brookhaven National Laboratory that more than one type of neutrino exists by first detecting interactions of the muon neutrino (already hypothesised with the name "neutretto"), which earned them the 1988 Nobel Prize.
Speed.
In September 2011, OPERA researchers reported that muon neutrinos were apparently traveling at faster than light speed. This result was confirmed again in a second experiment in November 2011. These results have been viewed skeptically by the scientific community at large, and more experiments have/are investigating the phenomenon. In March 2012, the ICARUS team published results directly contradicting the results of OPERA.
Later in July 2012 the apparent anomalous super-luminous propagation of neutrinos was traced to a faulty element of the fibre optic timing system in Gran-Sasso. After it was corrected the neutrinos appeared to travel with the speed of light within the errors of the experiment.

</doc>
<doc id="285048" url="https://en.wikipedia.org/wiki?curid=285048" title="Muon-catalyzed fusion">
Muon-catalyzed fusion

Muon-catalyzed fusion (μCF) is a process allowing nuclear fusion to take place at temperatures significantly lower than the temperatures required for thermonuclear fusion, even at room temperature or lower. It is one of the few known ways of catalyzing nuclear fusion reactions.
Muons are unstable subatomic particles. They are similar to electrons, but are about 207 times more massive. If a muon replaces one of the electrons in a hydrogen molecule, the nuclei are consequently drawn 207 times closer together than in a normal molecule. When the nuclei are this close together, the probability of nuclear fusion is greatly increased, to the point where a significant number of fusion events can happen at room temperature.
Current techniques for creating large numbers of muons require large amounts of energy, larger than the amounts produced by the catalyzed nuclear fusion reactions. This prevents it from becoming a practical power source. Moreover, each muon has about a 1% chance of "sticking" to the alpha particle produced by the nuclear fusion of a deuterium with a tritium, removing the "stuck" muon from the catalytic cycle, meaning that each muon can only catalyze at most a few hundred deuterium tritium nuclear fusion reactions. So, these two factors, of muons being too expensive to make and then sticking too easily to alpha particles, limit muon-catalyzed fusion to a laboratory curiosity. To create useful room-temperature muon-catalyzed fusion, reactors would need a cheaper, more efficient muon source and/or a way for each individual muon to catalyze many more fusion reactions.
History.
Andrei Sakharov and F.C. Frank predicted the phenomenon of muon-catalyzed fusion on theoretical grounds before 1950. Yakov Borisovich Zel'dovich also wrote about the phenomenon of muon-catalyzed fusion in 1954. Luis W. Alvarez "et al.", when analyzing the outcome of some experiments with muons incident on a hydrogen bubble chamber at Berkeley in 1956, observed muon-catalysis of exothermic p-d, proton and deuteron, nuclear fusion, which results in a helion, a gamma ray, and a release of about 5.5 MeV of energy. The Alvarez experimental results, in particular, spurred John David Jackson to publish one of the first comprehensive theoretical studies of muon-catalyzed fusion in his ground-breaking 1957 paper. This paper contained the first serious speculations on useful energy release from muon-catalyzed fusion. Jackson concluded that it would be impractical as an energy source, unless the "alpha-sticking problem" (see below) could be solved, leading potentially to an energetically cheaper and more efficient way of utilizing the catalyzing muons. This assessment has, so far, stood the test of time
Viability as a power source.
Potential benefits.
If muon-catalyzed d-t nuclear fusion were able to be realized practically, it would be a much more attractive way of generating power than conventional nuclear fission reactors because muon-catalyzed d-t nuclear fusion (like most other types of nuclear fusion), produces far fewer harmful (and far less long-lived) radioactive wastes, and very little greenhouse gas. Practical and economically sensible muon-catalyzed d-t nuclear fusion would go a long way toward reducing the production of greenhouse gases, such as carbon dioxide (CO2), by reducing or even eliminating the need to burn fossil fuels and biomass that contain carbon, for example.
Some people have proposed hybrid fusion/fission schemes to use the large amount of neutrons produced in muon-catalyzed d-t nuclear fusions to breed fissile fuels, from fertile material - for example, thorium-232 could breed uranium-233 in this way. The fissile fuels that have been bred can then be "burned," either in a conventional supercritical nuclear fission reactor or in an unconventional subcritical fission reactor, for example, a reactor using nuclear transmutation to process nuclear waste, or a reactor using the energy amplifier concept devised by Carlo Rubbia and others.
Problems facing practical exploitation.
Except for some refinements, little has changed since Jackson's assessment of the feasibility of muon-catalyzed fusion, other than Vesman's prediction of the hyperfine resonant formation of the muonic (d-μ-t)+ molecular ion, which was subsequently experimentally observed. This helped spark renewed interest in the whole field of muon-catalyzed fusion, which remains an active area of research worldwide among those who continue to be fascinated and intrigued (and frustrated) by this tantalizing approach to controllable nuclear fusion that "almost" works. Clearly, as Jackson observed in his 1957 paper, muon-catalyzed fusion is "unlikely" to provide "useful power production... unless an energetically cheaper way of producing μ−-mesons can be found."
One practical problem with the muon-catalyzed fusion process is that muons are unstable, decaying in about (in their rest frame). Hence, there needs to be some cheap means of producing muons, and the muons must be arranged to catalyze as many nuclear fusion reactions as possible before decaying.
Another, and in many ways more serious, problem is the "alpha-sticking" problem, which was recognized by Jackson in his 1957 paper. The α-sticking problem is the approximately 1% probability of the muon "sticking" to the alpha particle that results from deuteron-triton nuclear fusion, thereby effectively removing the muon from the muon-catalysis process altogether. Even if muons were absolutely stable, each muon could catalyze, on average, only about 100 d-t fusions before sticking to an alpha particle, which is only about one-fifth the number of muon catalyzed d-t fusions needed for break-even, where as much thermal energy is generated as electrical energy is consumed to produce the muons in the first place, according to Jackson's rough 1957 estimate.
More recent measurements seem to point to more encouraging values for the α-sticking probability, finding the α-sticking probability to be about 0.5% (or perhaps even about 0.4% or 0.3%), which could mean as many as about 200 (or perhaps even about 250 or about 333) muon-catalyzed d-t fusions per muon. Indeed, the team led by Steven E. Jones achieved 150 d-t fusions per muon (average) at the Los Alamos Meson Physics Facility. Unfortunately, 200 (or 250 or even 333) muon-catalyzed d-t fusions per muon is still not enough to reach break-even. Even with break-even, the conversion efficiency from "thermal" energy to "electrical" energy is only about 40% or so, further limiting viability. The best recent estimates of the "electrical" "energy cost" per muon is about with accelerators that are (coincidentally) about 40% efficient at transforming "electrical" energy from the power grid into acceleration of the deuterons.
As of 2012, no practical method of producing energy through this means has been published, although some discoveries using the Hall effect show promise.
Alternative estimation of breakeven.
According to Gordon Pusch, a physicist at Argonne National Laboratory, various breakeven calculations on muon-catalyzed fusion omit the heat energy the muon beam itself deposits in the target. By taking this factor into account, muon-catalyzed fusion can already exceed breakeven; however, the recirculated power is usually large compared to power out to the electrical grid (about 3-5 times as large, according to estimates). Despite this rather high recirculated power, the overall cycle efficiency is not too bad (roughly comparable to conventional fission reactors); however needing 4-6 MW electrical generating capacity for each megawatt out to the grid probably represents an unacceptably large capital investment. Pusch suggested using Bogdan Maglich's "migma" self-colliding beam concept to significantly increase the muon production efficiency, by eliminating target losses, and using tritium nuclei as the driver beam, to optimize the number of negative muons.
Process.
To create this effect, a stream of negative muons, most often created by decaying pions, is sent to a block that may be made up of all three hydrogen isotopes (protium, deuterium, and/or tritium), where the block is usually frozen, and the block may be at temperatures of about 3 kelvin (−270 degrees Celsius) or so. The muon may bump the electron from one of the hydrogen isotopes. The muon, 207 times more massive than the electron, effectively shields and reduces the electromagnetic repulsion between two nuclei and draws them much closer into a covalent bond than an electron can. Because the nuclei are so close, the strong nuclear force is able to kick in and bind both nuclei together. They fuse, release the catalytic muon (most of the time), and part of the original mass of both nuclei is released as energetic particles, as with any other type of nuclear fusion. The release of the catalytic muon is critical to continue the reactions. The majority of the muons continue to bond with other hydrogen isotopes and continue fusing nuclei together. However, not all of the muons are recycled: some bond with other debris emitted following the fusion of the nuclei (such as alpha particles and helions), removing the muons from the catalytic process. This gradually chokes off the reactions, as there are fewer and fewer muons with which the nuclei may bond. The number of reactions achieved in the lab can be as high as 150 d-t fusions per muon (average).
Deuterium-tritium (d-t or dt).
In the muon-catalyzed fusion of most interest, a positively charged deuteron (d), a positively charged triton (t), and a muon essentially form a positively charged muonic molecular heavy hydrogen ion (d-μ-t)+. The muon, with a rest mass about 207 times greater than the rest mass of an electron, is able to drag the more massive triton and deuteron about 207 times closer together to each other
 in the "muonic" (d-μ-t)+ molecular ion than can an electron in the corresponding "electronic" (d-e-t)+ molecular ion. The average separation between the triton and the deuteron in the electronic molecular ion is about one angstrom (100 pm), so the average separation between the triton and the deuteron in the muonic molecular ion is about 207 times smaller than that. Due to the strong nuclear force, whenever the triton and the deuteron in the muonic molecular ion happen to get even closer to each other during their periodic vibrational motions, the probability is very greatly enhanced that the positively charged triton and the positively charged deuteron would undergo quantum tunnelling through the repulsive Coulomb barrier that acts to keep them apart. Indeed, the quantum mechanical tunnelling probability depends roughly exponentially on the average separation between the triton and the deuteron, allowing a single muon to catalyze the d-t nuclear fusion in less than about half a picosecond, once the muonic molecular ion is formed.
The formation time of the muonic molecular ion is one of the "rate-limiting steps" in muon-catalyzed fusion that can easily take up to ten thousand or more picoseconds in a liquid molecular deuterium and tritium mixture (D2, DT, T2), for example. Each catalyzing muon thus spends most of its ephemeral existence of about 2.2 microseconds, as measured in its rest frame wandering around looking for suitable deuterons and tritons with which to bind.
Another way of looking at muon-catalyzed fusion is to try to visualize the ground state orbit of a muon around either a deuteron or a triton. Suppose the muon happens to have fallen into an orbit around a deuteron initially, which it has about a 50% chance of doing if there are approximately equal numbers of deuterons and tritons present, forming an electrically neutral "muonic" deuterium atom (d-μ)0 that acts somewhat like a "fat, heavy neutron" due both to its relatively small size (again, about 207 times smaller than an electrically neutral "electronic" deuterium atom (d-e)0) and to the very effective "shielding" by the muon of the positive charge of the proton in the deuteron. Even so, the muon still has a much greater chance of being "transferred" to any triton that comes near enough to the muonic deuterium than it does of forming a muonic molecular ion. The electrically neutral muonic tritium atom (t-μ)0 thus formed will act somewhat like an even "fatter, heavier neutron," but it will most likely hang on to its muon, eventually forming a muonic molecular ion, most likely due to the resonant formation of a hyperfine molecular state within an entire deuterium molecule D2 (d=e2=d), with the muonic molecular ion acting as a "fatter, heavier nucleus" of the "fatter, heavier" neutral "muonic/electronic" deuterium molecule ([d-μ-t]=e2=d), as predicted by Vesman, an Estonian graduate student, in 1967.
Once the muonic molecular ion state is formed, the shielding by the muon of the positive charges of the proton of the triton and the proton of the deuteron from each other allows the triton and the deuteron to tunnel through the coulomb barrier in time span of order of a nanosecond The muon survives the d-t muon-catalyzed nuclear fusion reaction and remains available (usually) to catalyze further d-t muon-catalyzed nuclear fusions. Each exothermic d-t nuclear fusion releases about 17.6 MeV of energy in the form of a "very fast" neutron having a kinetic energy of about 14.1 MeV and an alpha particle α (a helium-4 nucleus) with a kinetic energy of about 3.5 MeV. An additional 4.8 MeV can be gleaned by having the fast neutrons "moderated" in a suitable "blanket" surrounding the reaction chamber, with the blanket containing lithium-6, whose nuclei, known by some as "lithions," readily and exothermically absorb thermal neutrons, the lithium-6 being transmuted thereby into an alpha particle and a triton.
Deuterium-deuterium (d-d or dd) and other types.
The first kind of muon-catalyzed fusion to be observed experimentally, by L.W. Alvarez "et al.", was actually protium (H or 1H1) and deuterium (D or 1H2) muon-catalyzed fusion. The fusion rate for p-d (or pd) muon-catalyzed fusion has been estimated to be about a million times slower than the fusion rate for d-t muon-catalyzed fusion.
Of more practical interest, deuterium-deuterium muon-catalyzed fusion has been frequently observed and extensively studied experimentally, in large part because deuterium already exists in relative abundance and, like hydrogen, deuterium is not at all radioactive (Tritium rarely occurs naturally, and is radioactive with a half-life of about 12.5 years.)
The fusion rate for d-d muon-catalyzed fusion has been estimated to be only about 1% of the fusion rate for d-t muon-catalyzed fusion, but this still gives about one d-d nuclear fusion every 10 to 100 picoseconds or so. However, the energy released with every d-d muon-catalyzed fusion reaction is only about 20% or so of the energy released with every d-t muon-catalyzed fusion reaction. Moreover, the catalyzing muon has a probability of sticking to at least one of the d-d muon-catalyzed fusion reaction products that Jackson in this 1957 paper estimated to be at least 10 times greater than the corresponding probability of the catalyzing muon sticking to at least one of the d-t muon-catalyzed fusion reaction products, thereby preventing the muon from catalyzing any more nuclear fusions. Effectively, this means that each muon catalyzing d-d muon-catalyzed fusion reactions in pure deuterium is only able to catalyze about one-tenth of the number of d-t muon-catalyzed fusion reactions that each muon is able to catalyze in a mixture of equal amounts of deuterium and tritium, and each d-d fusion only yields about one-fifth of the yield of each d-t fusion, thereby making the prospects for useful energy release from d-d muon-catalyzed fusion at least 50 times worse than the already dim prospects for useful energy release from d-t muon-catalyzed fusion.
Potential "aneutronic" (or substantially aneutronic) nuclear fusion possibilities, which result in essentially no neutrons among the nuclear fusion products, are almost certainly not very amenable to muon-catalyzed fusion. This is somewhat disappointing because aneutronic nuclear fusion reactions typically produce substantially only energetic charged particles whose energy could potentially be converted to more useful "electrical" energy with a much higher efficiency than is the case with the conversion of "thermal" energy. One such essentially aneutronic nuclear fusion reaction involves a deuteron from deuterium fusing with a helion (h+2) from helium-3, which yields an energetic alpha particle and a much more energetic proton, both positively charged (with a few neutrons coming from inevitable d-d nuclear fusion side reactions). However, one muon with only one negative electric charge is incapable of shielding both positive charges of a helion from the one positive charge of a deuteron. The chances of the requisite "two" muons being present simultaneously are exceptionally remote.
In culture.
The term "cold fusion" was coined to refer to muon-catalyzed fusion in a 1956 New York Times article about Luis W. Alvarez's paper.
In 1957 Theodore Sturgeon wrote a short story "The Pod in The Barrier" where humanity has ubiquitous cold fusion reactors that work with muons. The reaction is "When Hydrogen One and Hydrogen Two are in the presence of Mu Mesons, they fuse into Helium Three, with an energy yield in electron volts of 5.4 times ten to the fifth power". Unlike the thermonuclear bomb contained in the Pod (which is used to destroy the Barrier) they can become temporarily disabled by "concentrated disbelief" that muon fusion works.

</doc>
<doc id="20398" url="https://en.wikipedia.org/wiki?curid=20398" title="Muonium">
Muonium

Muonium is an exotic atom made up of an antimuon and an electron, which was discovered in 1960 and is given the chemical symbol . During the muon's lifetime, muonium can enter into compounds such as muonium chloride () or sodium muonide (). Due to the mass difference between the antimuon and the electron, muonium () is more similar to atomic hydrogen () than positronium (). Its Bohr radius and ionization energy are within 0.5% of hydrogen, deuterium, and tritium.
Although muonium is short-lived, physical chemists study it using muon spin spectroscopy (μSR), a magnetic resonance technique analogous to nuclear magnetic resonance (NMR) or electron spin resonance (ESR) spectroscopy. Like ESR, μSR is useful for the analysis of chemical transformations and the structure of compounds with novel or potentially valuable electronic properties. Muonium is usually studied by muon spin rotation, in which the Mu atom's spin precesses in a magnetic field applied transverse to the muon spin direction (since muons are typically produced in a spin-polarized state from the decay of pions), and by avoided level crossing (ALC), which is also called level crossing resonance (LCR). The latter employs a magnetic field applied longitudinally to the polarization direction, and monitors the relaxation of muon spins caused by "flip/flop" transitions with other magnetic nuclei.
Because the muon is a lepton, the atomic energy levels of muonium can be calculated with great precision from quantum electrodynamics (QED), unlike in the case of hydrogen, where the precision is limited by uncertainties related to the internal structure of the proton. For this reason, muonium is an ideal system for studying bound-state QED and also for searching for physics beyond the standard model.
Nomenclature.
Normally in the nomenclature of particle physics, an atom composed of a positively charged particle bound to an electron is named after the positive particle with "-ium" appended, in this case "muium". The suffix "-onium" is mostly used for bound states of a particle with its own antiparticle. The exotic atom consisting of a muon and an antimuon is known as "true muonium". It is yet to be observed, but it may have been generated in the collision of electron and positron beams.

</doc>
<doc id="39782" url="https://en.wikipedia.org/wiki?curid=39782" title="N-sphere">
N-sphere

In mathematics, the "n"-sphere is the generalization of the ordinary sphere to spaces of arbitrary dimension. For any natural number "n", an "n"-sphere of radius "r" is defined as the set of points in ("n" + 1)-dimensional Euclidean space which are at distance "r" from a central point, where the radius "r" may be any positive real number. Thus, the "n"-sphere centred at the origin is defined by:
It is an "n"-dimensional manifold in Euclidean ("n" + 1)-space.
In particular:
Spheres of dimension are sometimes called hyperspheres, with a 3-sphere sometimes known as a glome. The "n"-sphere of unit radius centered at the origin is called the unit "n"-sphere, denoted "S""n". The unit "n"-sphere is often referred to as "the" "n"-sphere.
An "n"-sphere is the surface or boundary of an -dimensional ball, and is an "n"-dimensional manifold. For , the "n"-spheres are the simply connected "n"-dimensional manifolds of constant, positive curvature. The "n"-spheres admit several other topological descriptions: for example, they can be constructed by gluing two "n"-dimensional Euclidean spaces together, by identifying the boundary of an "n"-cube with a point, or (inductively) by forming the suspension of an -sphere.
Description.
For any natural number "n", an "n"-sphere of radius "r" is defined as the set of points in ("n" + 1)-dimensional Euclidean space that are at distance "r" from some fixed point c, where "r" may be any positive real number and where c may be any point in ("n" + 1)-dimensional space. In particular:
Euclidean coordinates in ("n" + 1)-space.
The set of points in ("n" + 1)-space: ("x"1,"x"2,…,"x""n"+1) that define an "n"-sphere, ("S""n") is represented by the equation:
where "c" is a center point, and "r" is the radius.
The above "n"-sphere exists in ("n" + 1)-dimensional Euclidean space and is an example of an "n"-manifold. The volume form ω of an "n"-sphere of radius "r" is given by
where * is the Hodge star operator; see for a discussion and proof of this formula in the case "r" = 1. As a result, formula_4
"n"-ball.
The space enclosed by an "n"-sphere is called an ("n" + 1)-ball. An ("n" + 1)-ball is closed if it includes the "n"-sphere, and it is open if it does not include the "n"-sphere.
Specifically:
Topological description.
Topologically, an "n"-sphere can be constructed as a one-point compactification of "n"-dimensional Euclidean space. Briefly, the "n"-sphere can be described as formula_5, which is "n"-dimensional Euclidean space plus a single point representing infinity in all directions.
In particular, if a single point is removed from an "n"-sphere, it becomes homeomorphic to formula_6. This forms the basis for stereographic projection.
Volume and surface area.
formula_7 and formula_8 are the "n"-dimensional volume and surface area of the "n"-ball and "n"-sphere of radius formula_9, respectively.
The constants formula_10 and formula_11 (for the unit ball and sphere) are related by the recurrences:
The surfaces and volumes can also be given in closed form:
where formula_15 is the gamma function. Derivations of these equations are given in this section. 
In general, the volumes of the "n"-ball in "n"-dimensional Euclidean space, and the "n"-sphere in ("n" + 1)-dimensional Euclidean, of radius "R", are proportional to the "n"th power of the radius, "R". We write formula_16 for the volume of the "n"-ball and formula_17 for the surface of the "n"-sphere, both of radius formula_9.
Interestingly, given the radius "R", the volume and the surface area of the "n"-sphere reaches a maximum and then decrease towards zero as the dimension "n" increases. In particular, the volume formula_19 of the "n"-sphere of constant radius "R" in "n"-dimensions
reaches a maximum for dimension formula_20 if formula_21 <"R"< formula_10 and formula_23 if formula_24 where formula_25 for formula_26. Similarly, defining the sequence formula_27, the surface area formula_28 of the "n"-sphere of constant radius "R" in "n" dimensions reaches a maximum for dimension formula_20 if formula_30 and formula_31 if formula_32.
Examples.
The 0-ball consists of a single point. The 0-dimensional Hausdorff measure is the number of points in a set, so 
The unit 1-ball is the interval formula_34 of length 2. So,
The 0-sphere consists of its two end-points, formula_36. So 
The unit 1-sphere is the unit circle in the Euclidean plane, and this has circumference (1-dimensional measure)
The region enclosed by the unit 1-sphere is the 2-ball, or unit disc, and this has area (2-dimensional measure)
Analogously, in 3-dimensional Euclidean space, the surface area (2-dimensional measure) of the unit 2-sphere is given by
and the volume enclosed is the volume (3-dimensional measure) of the unit 3-ball, given by
Recurrences.
The "surface area", or properly the "n"-dimensional volume, of the "n"-sphere at the boundary of the ("n" + 1)-ball of radius formula_9 is related to the volume of the ball by the differential equation
or, equivalently, representing the unit "n"-ball as a union of concentric ("n" − 1)-sphere "shells",
So, 
We can also represent the unit ("n" + 2)-sphere as a union of tori, each the product of a circle (1-sphere) with an "n"-sphere. Let formula_46 and formula_47, so that formula_48 and formula_49. Then,
Since formula_51, the equation
formula_52
holds for all "n".
This completes our derivation of the recurrences:
Closed forms.
Combining the recurrences, we see that formula_55. So it is simple to show by induction on "k" that,
where formula_58 denotes the double factorial, defined for odd integers by .
In general, the volume, in "n"-dimensional Euclidean space, of the unit "n"-ball, is given by
where formula_15 is the gamma function, which satisfies formula_61.
By multiplying formula_10 by formula_63, differentiating with respect to formula_9, and then setting formula_65, we get the closed form
Other relations.
The recurrences can be combined to give a "reverse-direction" recurrence relation for surface area, as depicted in the diagram:
Index-shifting "n" to then yields the recurrence relations:
where "S"0 = 2, "V"1 = 2, "S"1 = 2 and "V"2 = .
The recurrence relation for formula_10 can also be proved via integration with 2-dimensional polar coordinates:
Spherical coordinates.
We may define a coordinate system in an "n"-dimensional Euclidean space which is analogous
to the spherical coordinate system defined for 3-dimensional Euclidean space, in which the coordinates consist of a radial coordinate, formula_72 and "n" − 1 angular coordinates formula_73 where formula_74 ranges over formula_75 radians (or over [0, 360) degrees) and the other angles range over formula_76 radians (or over [0, 180] degrees). If formula_77 are the Cartesian coordinates, then we may compute formula_78 from formula_79 with:
Except in the special cases described below, the inverse transformation is unique:
where if formula_82 for some formula_83 but all of formula_84 are zero then formula_85 when formula_86, and formula_87 radians (180 degrees) when formula_88.
There are some special cases where the inverse transform is not unique; formula_89 for any formula_83 will be ambiguous whenever all of formula_91 are zero; in this case formula_89 may be chosen to be zero.
Spherical volume element.
Expressing the angular measures in radians, the volume element in "n"-dimensional Euclidean space will be found from the Jacobian of the transformation:
and the above equation for the volume of the "n"-ball can be recovered by integrating:
The volume element of the ("n"-1)–sphere, which generalizes the area element of the 2-sphere, is given by
The natural choice of an orthogonal basis over the angular coordinates is a product of ultraspherical polynomials,
for "j" = 1, 2, ..., "n" − 2, and the "e" "isφ""j"
for the angle "j" = "n" − 1 in concordance with the spherical harmonics.
Stereographic projection.
Just as a two-dimensional sphere embedded in three dimensions can be mapped onto a two-dimensional plane by a stereographic projection, an "n"-sphere can be mapped onto an "n"-dimensional hyperplane by the "n"-dimensional version of the stereographic projection. For example, the point formula_98 on a two-dimensional sphere of radius 1 maps to the point formula_99 on the formula_100 plane. In other words,
Likewise, the stereographic projection of an "n"-sphere formula_102 of radius 1 will map to the formula_103 dimensional hyperplane formula_104 perpendicular to the formula_105 axis as
Generating random points.
Uniformly at random from the ("n" − 1)-sphere.
To generate uniformly distributed random points on the ("n" − 1)-sphere ("i.e.", the surface of the "n"-ball), gives the following algorithm.
Generate an "n"-dimensional vector of normal deviates (it suffices to use N(0, 1), although in fact the choice of the variance is arbitrary), formula_107.
Now calculate the "radius" of this point, formula_108
The vector formula_109 is uniformly distributed over the surface of the unit "n"-ball.
Examples.
For example, when "n" = 2 the normal distribution exp(−"x"12) when expanded over another axis exp(−"x"22) after multiplication takes the form exp(−"x"12−"x"22) or exp(−"r"2) and so is only 
dependent on distance from the origin.
Alternatives.
Another way to generate a random distribution on a hypersphere is to make a uniform distribution
over a hypercube that includes the unit hyperball, exclude those points that are outside the hyperball, then project the remaining interior points outward from the origin onto the surface. This will give a uniform distribution, but it is necessary to remove the exterior points. As the relative volume of the hyperball to the hypercube decreases very rapidly with dimension, this procedure will succeed with high probability only for fairly small numbers of dimensions.
Wendel's theorem gives the probability that all of the points generated will lie in the same half of the hypersphere.
Uniformly at random from the "n"-ball.
With a point selected from the surface of the "n"-ball uniformly at random, one needs only a radius to obtain a point uniformly at random within the "n"-ball. If "u" is a number generated uniformly at random from the interval [0, 1] and x is a point selected uniformly at random from the surface of the "n"-ball then u1/nx is uniformly distributed over the entire unit "n"-ball.

</doc>
<doc id="33710742" url="https://en.wikipedia.org/wiki?curid=33710742" title="Natural units">
Natural units

In physics, natural units are physical units of measurement based only on universal physical constants. For example, the elementary charge "e" is a natural unit of electric charge, and the speed of light "c" is a natural unit of speed. A purely natural system of units has all of its units defined in this way, and usually such that the numerical values of the selected physical constants in terms of these units are exactly 1. These constants are then typically omitted from mathematical expressions of physical laws, and while this has the apparent advantage of simplicity, it may entail a loss of clarity due to the loss of information for dimensional analysis.
Introduction.
Natural units are intended to elegantly simplify particular algebraic expressions appearing in the laws of physics or to normalize some chosen physical quantities that are properties of universal elementary particles and are reasonably believed to be constant. However, there is a choice of which quantities to set to unity in a natural system of units, and quantities which are set to unity in one system may take a different value or even be assumed to vary in another natural unit system.
Natural units are "natural" because the origin of their definition comes only from properties of nature and not from any human construct. Planck units are often, without qualification, called "natural units", although they constitute only one of several systems of natural units, albeit the best known such system. Planck units (up to a simple multiplier for each unit) might be considered one of the most "natural" systems in that the set of units is not based on properties of any prototype, object, or particle but are solely derived from the properties of free space.
As with other systems of units, the base units of a set of natural units will include definitions and values for length, mass, time, temperature, and electric charge (in lieu of electric current). Some physicists do not recognize temperature as a fundamental physical quantity, since it expresses the energy per degree of freedom of a particle, which can be expressed in terms of energy (or mass, length, and time). Virtually every system of natural units normalizes Boltzmann's constant to , which can be thought of as simply a way of defining the unit temperature.
In the SI unit system, electric charge is a separate fundamental dimension of physical quantity, but in natural unit systems charge is expressed in terms of the mechanical units of mass, length, and time, similarly to cgs. There are two common ways to relate charge to mass, length, and time: In Lorentz–Heaviside units (also called "rationalized"), Coulomb's law is , and in Gaussian units (also called "non-rationalized"), Coulomb's law is . Both possibilities are incorporated into different natural unit systems.
Notation and use.
Natural units are most commonly used by "setting the units to one". For example, many natural unit systems include the equation in the unit-system definition, where is the speed of light. If a velocity is half the speed of light, then as and , hence . The equation means "the velocity has the value one-half when measured in Planck units", or "the velocity is one-half the Planck unit of velocity".
The equation can be plugged in anywhere else. For example, Einstein's equation can be rewritten in Planck units as . This equation means "The energy of a particle, measured in Planck units of energy, equals the mass of the particle, measured in Planck units of mass."
Advantages and disadvantages.
Compared to SI or other unit systems, natural units have both advantages and disadvantages:
Choosing constants to normalize.
Out of the many physical constants, the designer of a system of natural unit systems must choose a few of these constants to normalize (set equal to ). It is not possible to normalize just "any" set of constants. For example, the mass of a proton and the mass of an electron cannot both be normalized: if the mass of an electron is defined to be , then the mass of a proton has to be approximately 1836. In a less trivial example, the fine-structure constant, , cannot be set to , at least not independently, because it is a dimensionless number defined in terms of other quantities, some of which one may want to set to unity as well. The fine-structure constant is related to other fundamental constants through
where is the Coulomb constant, is the elementary charge, is the reduced Planck constant, and is the speed of light.
Electromagnetism units.
In SI units, electric charge is expressed in coulombs, a separate unit which is additional to the "mechanical" units (mass, length, time), even though the traditional definition of the ampere refers to some of these other units. In natural unit systems, however, electric charge has units of .
There are two main natural unit systems for electromagnetism:
Of these, Lorentz–Heaviside is somewhat more common, mainly because Maxwell's equations are simpler in Lorentz-Heaviside units than they are in Gaussian units.
In the two unit systems, the elementary charge "e" satisfies:
where is the reduced Planck constant, is the speed of light, and is the fine-structure constant.
In a natural unit system where , Lorentz–Heaviside units can be derived from SI units by setting . Gaussian units can be derived from SI units by a more complicated set of transformations, such as multiplying all electric fields by (4πε0)−1/2, multiplying all magnetic susceptibilities by , and so on.
Systems of natural units.
Planck units.
Planck units are defined by
where is the speed of light, is the reduced Planck constant, is the gravitational constant, is the Coulomb constant, and is the Boltzmann constant.
Planck units are a system of natural units that is not defined in terms of properties of any prototype, physical object, or even elementary particle. They only refer to the basic structure of the laws of physics: and are part of the structure of spacetime in general relativity, and captures the relationship between energy and frequency which is at the foundation of quantum mechanics. This makes Planck units particularly useful and common in theories of quantum gravity, including string theory.
Planck units may be considered "more natural" even than other natural unit systems discussed below, as Planck units are not based on any arbitrarily chosen prototype object or particle. For example, some other systems use the mass of an electron as a parameter to be normalized. But the electron is just one of 16 known massive elementary particles, all with different masses, and there is no compelling reason, within fundamental physics, to emphasize the electron mass over some other elementary particle's mass.
Stoney units.
Stoney units are defined by:
where is the speed of light, is the gravitational constant, is the Coulomb constant, is the elementary charge, and is the Boltzmann constant.
George Johnstone Stoney was the first physicist to introduce the concept of natural units. He presented the idea in a lecture entitled "On the Physical Units of Nature" delivered to the British Association in 1874. Stoney units differ from Planck units by fixing the elementary charge at 1, instead of Planck's constant (only discovered after Stoney's proposal).
Stoney units are rarely used in modern physics for calculations, but they are of historical interest.
Atomic units.
There are two types of atomic units, closely related.
Hartree atomic units:
Rydberg atomic units:
Coulomb's constant is generally expressed as 
These units are designed to simplify atomic and molecular physics and chemistry, especially the hydrogen atom, and are widely used in these fields. The Hartree units were first proposed by Douglas Hartree, and are more common than the Rydberg units.
The units are designed especially to characterize the behavior of an electron in the ground state of a hydrogen atom. For example, using the Hartree convention, in the Bohr model of the hydrogen atom, an electron in the ground state has , , , ionization energy = , etc.
The unit of energy is called the Hartree energy in the Hartree system and the Rydberg energy in the Rydberg system. They differ by a factor of . The speed of light is relatively large in atomic units ( in Hartree or in Rydberg), which comes from the fact that an electron in hydrogen tends to move much slower than the speed of light. The gravitational constant is extremely small in atomic units (around ), which comes from the fact that the gravitational force between two electrons is far weaker than the Coulomb force. The unit length, , is the Bohr radius, .
The values of and shown above imply that , as in Gaussian units, "not" Lorentz–Heaviside units. However, hybrids of the Gaussian and Lorentz–Heaviside units are sometimes used, leading to inconsistent conventions for magnetism-related units.
Quantum chromodynamics (QCD) units.
The electron mass is replaced with that of the proton. "Strong units" are "convenient for work in QCD and nuclear physics, where quantum mechanics and relativity are omnipresent and the proton is an object of central interest".
"Natural units" (particle physics and cosmology).
In particle physics and cosmology, the phrase "natural units" generally means:
where is the reduced Planck constant, is the speed of light, and is the Boltzmann constant.
Both Planck units and QCD units are this type of "Natural units". Like the other systems, the electromagnetism units can be based on either Lorentz–Heaviside units or Gaussian units. The unit of charge is different in each.
Finally, one more unit is needed to construct a usable system of units that includes energy and mass. Most commonly, electron-volt (eV) is used, despite the fact that this is not a "natural" unit in the sense discussed above – it is defined by a natural property, the elementary charge, and the anthropogenic unit of electric potential, the volt. (The SI prefixed multiples of eV are used as well: keV, MeV, GeV, etc.)
With the addition of eV (or any other auxiliary unit with the proper dimension), any quantity can be expressed. For example, a distance of 1.0 cm can be expressed in terms of eV, in natural units, as:
Geometrized units.
The geometrized unit system, used in general relativity, is not a completely defined system. In this system, the base physical units are chosen so that the speed of light and the gravitational constant are set equal to unity. Other units may be treated however desired. Planck units and Stoney units are examples of geometrized unit systems.
Summary table.
where:

</doc>
<doc id="48395" url="https://en.wikipedia.org/wiki?curid=48395" title="Navier–Stokes equations">
Navier–Stokes equations

In physics, the Navier–Stokes equations , named after Claude-Louis Navier and George Gabriel Stokes, describe the motion of viscous fluid substances. These balance equations arise from applying Newton's second law to fluid motion, together with the assumption that the stress in the fluid is the sum of a diffusing viscous term (proportional to the gradient of velocity) and a pressure term—hence describing "viscous flow". The main difference between them and the simpler Euler equations for inviscid flow is that Navier–Stokes equations also in the Froude limit (no external field) are not conservation equations, but rather a dissipative system, in the sense that they cannot be put into the quasilinear homogeneous form:
Navier–Stokes equations are useful because they describe the physics of many things of scientific and engineering interest. They may be used to model the weather, ocean currents, water flow in a pipe and air flow around a wing. The Navier–Stokes equations in their full and simplified forms help with the design of aircraft and cars, the study of blood flow, the design of power stations, the analysis of pollution, and many other things. Coupled with Maxwell's equations they can be used to model and study magnetohydrodynamics.
The Navier–Stokes equations are also of great interest in a purely mathematical sense. Somewhat surprisingly, given their wide range of practical uses, it has not yet been proven that in three dimensions solutions always exist (existence), or that if they do exist, then they do not contain any singularity (they are smooth). These are called the Navier–Stokes existence and smoothness problems. The Clay Mathematics Institute has called this one of the seven most important open problems in mathematics and has offered a US$1,000,000 prize for a solution or a counter-example.
Flow velocity.
The solution of the Navier–Stokes equations is a flow velocity. It is a field, since it is defined at every point in a region of space and an interval of time. Once the velocity field is calculated, other quantities of interest, such as pressure or temperature, may be found. This is different from what one normally sees in classical mechanics, where solutions are typically trajectories of position of a particle or deflection of a continuum. Studying velocity instead of position makes more sense for a fluid; however for visualization purposes one can compute various trajectories.
General continuum equations.
The Navier–Stokes momentum equation can be derived as a particular form of the Cauchy momentum equation.
In an inertial frame of reference, the conservation form of the equations of continuum motion is:
where
The left side of the equation describes acceleration, and may be composed of time-dependent, convective, and hydrostatic effects (also the effects of non-inertial coordinates if present). The right side of the equation is in effect a summation of body forces (such as gravity) and divergence of deviatoric stress.
In the Eulerian forms it is apparent that the assumption of no deviatoric stress brings Cauchy equations to the Euler equations.
All non-relativistic balance equations, such as the Navier–Stokes equations, can be derived by beginning with the Cauchy equations and specifying the stress tensor through a constitutive relation. By expressing the shear tensor in terms of viscosity and fluid velocity, and assuming constant density and viscosity, the Cauchy equations will lead to the Navier–Stokes equations.
The incompressible case is simpler than the compressible one so for didactical purpose it should be presented before. However, the compressible case is the most general framework of Navier–Stokes equations so where not specified, Navier–Stokes equations are intended to be "compressible" Navier–Stokes equations.
Convective acceleration.
A significant feature of Cauchy equation and consequently all other continuum equations (including Euler and Navier–Stokes) is the presence of convective acceleration: the effect of time-independent acceleration of a flow with respect to space. While individual fluid particles indeed experience time-dependent acceleration, the convective acceleration of the flow field is a spatial effect, one example being fluid speeding up in a nozzle.
Incompressible flow.
The incompressible momentum Navier–Stokes equation result from the following assumptions on the Cauchy stress tensor:
Dynamic viscosity need not be constant – in incompressible flows it can depend on density and on pressure. Any equation expliciting one of these transport coefficient in the conservative variables is called an equation of state.
The divergence of the deviatoric stress is given by:
Incompressibility rules out density and pressure waves like sound or shock waves, so this simplification is not useful if these phenomena are of interest. The incompressible flow assumption typically holds well with all fluids at low Mach numbers (say up to about Mach 0.3), such as for modelling air winds at normal temperatures. For incompressible (uniform density ρ0) flows the following identity holds:
where is the specific (with the sense of "per unit mass") thermodynamic work, the internal source term. Then the incompressible Navier–Stokes equations are best visualised by dividing for the density:
Velocity profile (laminar flow)
formula_14, formula_15, formula_16
for the formula_17- direction, simplify Navier-Stokes equation
formula_18
integrate twice to find velocity profile with boundary conditions: formula_19, formula_20 formula_21, formula_20
formula_23
From this equation, sub in your two boundary conditions to get 2 equations
formula_24 <br>
formula_25
Add and solve for B
formula_26
Substitute and solve for A
formula_27
Finally you get the velocity profile
formula_28
in tensor notation:
where:
It is well worth observing the meaning of each term (compare to the Cauchy momentum equation):
The higher-order term, namely the shear stress divergence , has simply reduced to the vector laplacian term . This laplacian term can be interpreted as the difference between the velocity at a point and the mean velocity in a small surrounding volume. This implies that – for a Newtonian fluid – viscosity operates as a "diffusion of momentum", in much the same way as the heat conduction. In fact neglecting the convection term, incompressible Navier–Stokes equations lead to a vector diffusion equation (namely Stokes equations), but in general the convection term is present, so incompressible Navier–Stokes equations belong to the class of convection-diffusion equations.
In the usual case of an external field being a conservative field:
by defining the hydraulic head:
one can finally condense the whole source in one term, arriving to the incompressible Navier-Stokes equation with conservative external field:
The incompressible Navier–Stokes equations with conservative external field is the fundamental equation of hydraulics. The domain for these equations is commonly a 3 or less euclidean space, for which an orthogonal coordinate reference frame is usually set to explicit the system of scalar partial derivative equations to be solved. In 3D orthogonal coordinate systems are 3: Cartesian, cylindrical, and spherical. Expressing the Navier-Stokes vector equation in Cartesian coordinates is quite straightforward and not much influenced by the number of dimensions of the euclidean space employed, and this is the case also for the first-order terms (like the variation and convection ones) also in non-cartesian orthogonal coordinate systems. But for the higher order terms (the two coming from the divergence of the deviatoric stress that distinguish Navier–Stokes equations from Euler equations) some tensor calculus is required for deducing an expression in non-cartesian orthogonal coordinate systems.
The incompressible Navier–Stokes equation is composite, the sum of two orthogonal equations,
where and are solenoidal and irrotational projection operators satisfying and formula_34 and are the non-conservative and conservative parts of the body force. This result follows from the Helmholtz Theorem (also known as the fundamental theorem of vector calculus). The first equation is a pressureless governing equation for the velocity, while the second equation for the pressure is a functional of the velocity and is related to the pressure Poisson equation.
The explicit functional form of the projection operator in 3D is found from the Helmholtz Theorem:
with a similar structure in 2D. Thus the governing equation is an integro-differential equation similar to Coulomb and Biot-Savart law, not convenient for numerical computation.
An equivalent weak or variational form of the equation, proved to produce the same velocity solution as the Navier–Stokes equation, is given by,
for divergence-free test functions satisfying appropriate boundary conditions. Here, the projections are accomplished by the orthogonality of the solenoidal and irrotational function spaces. The discrete form of this is imminently suited to finite element computation of divergence-free flow, as we shall see in the next section. There we will be able to address the question, "How does one specify pressure-driven (Poiseuille) problems with a pressureless governing equation?"
The absence of pressure forces from the governing velocity equation demonstrates that the equation is not a dynamic one, but rather a kinematic equation where the divergence-free condition serves the role of a conservation equation. This all would seem to refute the frequent statements that the incompressible pressure enforces the divergence-free condition.
Discrete velocity.
With partitioning of the problem domain and defining basis functions on the partitioned domain, the discrete form of the governing equation is,
It is desirable to choose basis functions which reflect the essential feature of incompressible flow – the elements must be divergence-free. While the velocity is the variable of interest, the existence of the stream function or vector potential is necessary by the Helmholtz Theorem. Further, to determine fluid flow in the absence of a pressure gradient, one can specify the difference of stream function values across a 2D channel, or the line integral of the tangential component of the vector potential around the channel in 3D, the flow being given by Stokes' Theorem. Discussion will be restricted to 2D in the following.
We further restrict discussion to continuous Hermite finite elements which have at least first-derivative degrees-of-freedom. With this, one can draw a large number of candidate triangular and rectangular elements from the plate-bending literature.
These elements have derivatives as components of the gradient. In 2D, the gradient and curl of a scalar are clearly orthogonal, given by the expressions,
Adopting continuous plate-bending elements, interchanging the derivative degrees-of-freedom and changing the sign of the appropriate one gives many families of stream function elements.
Taking the curl of the scalar stream function elements gives divergence-free velocity elements. The requirement that the stream function elements be continuous assures that the normal component of the velocity is continuous across element interfaces, all that is necessary for vanishing divergence on these interfaces.
Boundary conditions are simple to apply. The stream function is constant on no-flow surfaces, with no-slip velocity conditions on surfaces.
Stream function differences across open channels determine the flow. No boundary conditions are necessary on open boundaries, though consistent values may be used with some problems. These are all Dirichlet conditions.
The algebraic equations to be solved are simple to set up, but of course are non-linear, requiring iteration of the linearized equations.
Similar considerations apply to three-dimensions, but extension from 2D is not immediate because of the vector nature of the potential, and there exists no simple relation between the gradient and the curl as was the case in 2D.
Pressure recovery.
Recovering pressure from the velocity field is easy. The discrete weak equation for the pressure gradient is,
where the test/weight functions are irrotational. Any conforming scalar finite element may be used. However, the pressure gradient field may also be of interest. In this case one can use scalar Hermite elements for the pressure. For the test/weight functions one would choose the irrotational vector elements obtained from the gradient of the pressure element.
Compressible flow.
The Navier–Stokes equations result from the following assumptions on the stress tensor:
Since the trace of the rate-of-strain tensor in three dimensions is:
The trace of the stress tensor in three dimensions becomes:
So by alternatively decomposing the stress tensor is into isotropic and deviatoric parts, as usual in fluid dynamics:
Introducing the second viscosity ,
we arrive to the linear constitutive equation in the form usually employed in thermal hydraulics:
Both bulk viscosity and dynamic viscosity need not be constant – in general, they depend on density, on each other (the viscosity is expressed in pressure), and in compressible flows also on temperature. Any equation expliciting one of these transport coefficient in the conservation variables is called an equation of state.
By computing the divergence of the stress tensor,
since the divergence of tensor is and
the divergence of tensor is , one finally arrives to the compressible (most general) Navier-Stokes momentum equation:
Bulk viscosity is assumed to be constant, otherwise it should not be taken out of the last derivative. The effect of the volume viscosity is that the mechanical pressure is not equivalent to the thermodynamic pressure:
This difference is usually neglected, sometimes by explicitly assuming = 0, but it could have an impact in sound absorption and attenuation and shock waves , see
For the special case of an incompressible flow, the pressure constrains the flow so that the volume of fluid elements is constant: isochoric flow resulting in a solenoidal velocity field with 
Other equations.
The Navier–Stokes equations are strictly a statement of the balance of momentum. To fully describe fluid flow, more information is needed, how much depending on the assumptions made. This additional information may include boundary data (no-slip, capillary surface, etc.), conservation of mass, balance of energy, and/or an equation of state.
Continuity equation.
Regardless of the flow assumptions, a statement of the conservation of mass is generally necessary. This is achieved through the mass continuity equation, given in its most general form as:
or, using the substantive derivative:
In the example below we can assume to have a Newtonian Fluid as well as having formula_2 and formula_50 both be constant
Stream function for 2D equations.
Taking the curl of the Navier–Stokes equation results in the elimination of pressure. This is especially easy to see if 2D Cartesian flow is assumed (like in the degenerate 3D case with formula_55 and no dependence of anything on "z"), where the equations reduce to:
Differentiating the first with respect to "y", the second with respect to "x" and subtracting the resulting equations will eliminate pressure and any conservative force. Defining the stream function formula_57 through
results in mass continuity being unconditionally satisfied (given the stream function is continuous), and then incompressible Newtonian 2D momentum and mass conservation condense into one equation:
where formula_60 is the (2D) biharmonic operator and formula_61 is the kinematic viscosity, formula_62. We can also express this compactly using the Jacobian determinant:
This single equation together with appropriate boundary conditions describes 2D fluid flow, taking only kinematic viscosity as a parameter. Note that the equation for creeping flow results when the left side is assumed zero.
In axisymmetric flow another stream function formulation, called the Stokes stream function, can be used to describe the velocity components of an incompressible flow with one scalar function.
The incompressible Navier–Stokes equation is a differential algebraic equation, having the inconvenient feature that there is no explicit mechanism for advancing the pressure in time. Consequently, much effort has been expended to eliminate the pressure from all or part of the computational process. The stream function formulation eliminates the pressure but only in two dimensions and at the expense of introducing higher derivatives and elimination of the velocity, which is the primary variable of interest.
Properties.
Nonlinearity.
The Navier–Stokes equations are nonlinear partial differential equations in the general case and so remain in almost every real situation. In some cases, such as one-dimensional flow and Stokes flow (or creeping flow), the equations can be simplified to linear equations. The nonlinearity makes most problems difficult or impossible to solve and is the main contributor to the turbulence that the equations model.
The nonlinearity is due to convective acceleration, which is an acceleration associated with the change in velocity over position. Hence, any convective flow, whether turbulent or not, will involve nonlinearity. An example of convective but laminar (nonturbulent) flow would be the passage of a viscous fluid (for example, oil) through a small converging nozzle. Such flows, whether exactly solvable or not, can often be thoroughly studied and understood.
Turbulence.
Turbulence is the time-dependent chaotic behavior seen in many fluid flows. It is generally believed that it is due to the inertia of the fluid as a whole: the culmination of time-dependent and convective acceleration; hence flows where inertial effects are small tend to be laminar (the Reynolds number quantifies how much the flow is affected by inertia). It is believed, though not known with certainty, that the Navier–Stokes equations describe turbulence properly.
The numerical solution of the Navier–Stokes equations for turbulent flow is extremely difficult, and due to the significantly different mixing-length scales that are involved in turbulent flow, the stable solution of this requires such a fine mesh resolution that the computational time becomes significantly infeasible for calculation or direct numerical simulation. Attempts to solve turbulent flow using a laminar solver typically result in a time-unsteady solution, which fails to converge appropriately. To counter this, time-averaged equations such as the Reynolds-averaged Navier–Stokes equations (RANS), supplemented with turbulence models, are used in practical computational fluid dynamics (CFD) applications when modeling turbulent flows. Some models include the Spalart-Allmaras, k-ω (k-omega), k-ε (k-epsilon), and SST models, which add a variety of additional equations to bring closure to the RANS equations. Large eddy simulation (LES) can also be used to solve these equations numerically. This approach is computationally more expensive—in time and in computer memory—than RANS, but produces better results because it explicitly resolves the larger turbulent scales.
Applicability.
Together with supplemental equations (for example, conservation of mass) and well formulated boundary conditions, the Navier–Stokes equations seem to model fluid motion accurately; even turbulent flows seem (on average) to agree with real world observations.
The Navier–Stokes equations assume that the fluid being studied is a continuum (it is infinitely divisible and not composed of particles such as atoms or molecules), and is not moving at relativistic velocities. At very small scales or under extreme conditions, real fluids made out of discrete molecules will produce results different from the continuous fluids modeled by the Navier–Stokes equations. Depending on the Knudsen number of the problem, the Boltzmann equation may be a suitable replacement; failing that, one may find the techniques of statistical mechanics sufficient or have to resort to molecular dynamics.
Another limitation is simply the complicated nature of the equations. Time-tested formulations exist for common fluid families, but the application of the Navier–Stokes equations to less common families tends to result in very complicated formulations and often to open research problems. For this reason, these equations are usually rewritten for Newtonian fluids where the viscosity model is linear; truly general models for the flow of other kinds of fluids (such as blood) do not, as of 2012, exist .
Application to specific problems.
The Navier–Stokes equations, even when written explicitly for specific fluids, are rather generic in nature and their proper application to specific problems can be very diverse. This is partly because there is an enormous variety of problems that may be modeled, ranging from as simple as the distribution of static pressure to as complicated as multiphase flow driven by surface tension.
Generally, application to specific problems begins with some flow assumptions and initial/boundary condition formulation, this may be followed by scale analysis to further simplify the problem.
a)
Assume steady, parallel, one dimensional, non-convective pressure-driven flow between parallel plates, the resulting scaled (dimensionless) boundary value problem is:
The boundary condition is the no slip condition. This problem is easily solved for the flow field:
From this point onward more quantities of interest can be easily obtained, such as viscous drag force or net flow rate.
b)
Difficulties may arise when the problem becomes slightly more complicated. A seemingly modest twist on the parallel flow above would be the "radial" flow between parallel plates; this involves convection and thus non-linearity. The velocity field may be represented by a function formula_66 that must satisfy:
This ordinary differential equation is what is obtained when the Navier–Stokes equations are written and the flow assumptions applied (additionally, the pressure gradient is solved for). The nonlinear term makes this a very difficult problem to solve analytically (a lengthy implicit solution may be found which involves elliptic integrals and roots of cubic polynomials). Issues with the actual existence of solutions arise for R > 1.41 (approximately; this is not the square root of 2), the parameter R being the Reynolds number with appropriately chosen scales. This is an example of flow assumptions losing their applicability, and an example of the difficulty in "high" Reynolds number flows.
Exact solutions of the Navier–Stokes equations.
Some exact solutions to the Navier–Stokes equations exist. Examples of degenerate cases — with the non-linear terms in the Navier–Stokes equations equal to zero — are Poiseuille flow, Couette flow and the oscillatory Stokes boundary layer. But also more interesting examples, solutions to the full non-linear equations, exist; for example the Taylor–Green vortex.
Note that the existence of these exact solutions does not imply they are stable: turbulence may develop at higher Reynolds numbers.
Under additional assuptions, the component parts can be separated.
</math>
where A and B are arbitrary constants. This solution is valid in the domain "r" ≥ 1 and for formula_68.
In Cartesian coordinates, when the viscosity is zero (formula_69), this is:
A three-dimensional steady-state vortex solution.
A nice steady-state example with no singularities comes from considering the flow along the lines of a Hopf fibration. Let r be a constant radius to the inner coil. One set of solutions is given by:
for arbitrary constants A and B. This is a solution in a non-viscous gas (compressible fluid) whose density, velocities and pressure goes to zero far from the origin. (Note this is not a solution to the Clay Millennium problem because that refers to incompressible fluids where formula_2 is a constant, neither does it deal with the uniqueness of the Navier–Stokes equations with respect to any turbulence properties.) It is also worth pointing out that the components of the velocity vector are exactly those from the Pythagorean quadruple parametrization. Other choices of density and pressure are possible with the same velocity field:
Wyld diagrams.
Wyld diagrams are bookkeeping graphs that correspond to the Navier–Stokes equations via a perturbation expansion of the fundamental continuum mechanics. Similar to the Feynman diagrams in quantum field theory, these diagrams are an extension of Keldysh's technique for nonequilibrium processes in fluid dynamics. In other words, these diagrams assign graphs to the (often) turbulent phenomena in turbulent fluids by allowing correlated and interacting fluid particles to obey stochastic processes associated to pseudo-random functions in probability distributions.
Representations.
 &-\frac{\partial p}{\partial r} + \mu \left[\frac{1}{r}\frac{\partial}{\partial r}\left(r \frac{\partial u_r}{\partial r}\right) +
 \frac{1}{r^2}\frac{\partial^2 u_r}{\partial \phi^2} + \frac{\partial^2 u_r}{\partial z^2} - \frac{u_r}{r^2} -
 \frac{2}{r^2}\frac{\partial u_\phi}{\partial \phi} \right] + \rho g_r \\
 \phi:\ &\rho \left(\frac{\partial u_{\phi}}{\partial t} + u_r \frac{\partial u_{\phi}}{\partial r} +
 \frac{u_{\phi}}{r} \frac{\partial u_{\phi}}{\partial \phi} + u_z \frac{\partial u_{\phi}}{\partial z} + \frac{u_r u_{\phi}}{r}\right) = {}\\
 &-\frac{1}{r}\frac{\partial p}{\partial \phi} + \mu \left[\frac{1}{r}\frac{\partial}{\partial r}\left(r \frac{\partial u_{\phi}}{\partial r}\right) +
 \frac{1}{r^2}\frac{\partial^2 u_{\phi}}{\partial \phi^2} + \frac{\partial^2 u_{\phi}}{\partial z^2} + \frac{2}{r^2}\frac{\partial u_r}{\partial \phi}-\frac{u_{\phi}}{r^2}\right] + \rho g_{\phi} \\
 z:\ &\rho \left(\frac{\partial u_z}{\partial t} + u_r \frac{\partial u_z}{\partial r} + \frac{u_{\phi}}{r} \frac{\partial u_z}{\partial \phi} +
 u_z \frac{\partial u_z}{\partial z}\right) = {}\\
 &-\frac{\partial p}{\partial z} + \mu \left[\frac{1}{r}\frac{\partial}{\partial r}\left(r \frac{\partial u_z}{\partial r}\right) +
 \frac{1}{r^2}\frac{\partial^2 u_z}{\partial \phi^2} + \frac{\partial^2 u_z}{\partial z^2}\right] + \rho g_z.
\end{align}</math>
The gravity components will generally not be constants, however for most applications either the coordinates are chosen so that the gravity components are constant or else it is assumed that gravity is counteracted by a pressure field (for example, flow in horizontal pipe is treated normally without gravity and without a vertical pressure gradient). The continuity equation is:
This cylindrical representation of the incompressible Navier–Stokes equations is the second most commonly seen (the first being Cartesian above). Cylindrical coordinates are chosen to take advantage of symmetry, so that a velocity component can disappear. A very common case is axisymmetric flow with the assumption of no tangential velocity (formula_74), and the remaining quantities are independent of formula_75:
 \frac{u_{\theta}}{r} \frac{\partial u_r}{\partial \theta} - \frac{u_{\phi}^2 + u_{\theta}^2}{r}\right) =
 -\frac{\partial p}{\partial r} + \rho g_r + \\
 &\mu \left[\frac{1}{r^2} \frac{\partial}{\partial r}\left(r^2 \frac{\partial u_r}{\partial r}\right) +
 \frac{1}{r^2 \sin(\theta)^2} \frac{\partial^2 u_r}{\partial \phi^2} +
 \frac{1}{r^2 \sin(\theta)} \frac{\partial}{\partial \theta}\left(\sin(\theta) \frac{\partial u_r}{\partial \theta}\right) - 2\frac{u_r +
 \right] \\
 \phi:\ &\rho \left(\frac{\partial u_{\phi}}{\partial t} + u_r \frac{\partial u_{\phi}}{\partial r} +
 \frac{u_{\phi}}{r \sin(\theta)} \frac{\partial u_{\phi}}{\partial \phi} + \frac{u_{\theta}}{r} \frac{\partial u_{\phi}}{\partial \theta} +
 \frac{u_r u_{\phi} + u_{\phi} u_{\theta} \cot(\theta)}{r}\right) =
 -\frac{1}{r \sin(\theta)} \frac{\partial p}{\partial \phi} + \rho g_{\phi} + \\
 &\mu \left[\frac{1}{r^2} \frac{\partial}{\partial r}\left(r^2 \frac{\partial u_{\phi}}{\partial r}\right) +
 \frac{1}{r^2 \sin(\theta)^2} \frac{\partial^2 u_{\phi}}{\partial \phi^2} +
 \frac{1}{r^2 \sin(\theta)} \frac{\partial}{\partial \theta}\left(\sin(\theta) \frac{\partial u_{\phi}}{\partial \theta}\right) +
 \frac{2 \sin(\theta) \frac{\partial u_r}{\partial \phi} + 2 \cos(\theta) \frac{\partial u_{\theta}}{\partial \phi} -
 \right] \\
 \theta:\ &\rho \left(\frac{\partial u_{\theta}}{\partial t} + u_r \frac{\partial u_{\theta}}{\partial r} +
 \frac{u_{\phi}}{r \sin(\theta)} \frac{\partial u_{\theta}}{\partial \phi} +
 \frac{u_{\theta}}{r} \frac{\partial u_{\theta}}{\partial \theta} + \frac{u_r u_{\theta} - u_{\phi}^2 \cot(\theta)}{r}\right) =
 -\frac{1}{r} \frac{\partial p}{\partial \theta} + \rho g_{\theta} + \\
 &\mu \left[\frac{1}{r^2} \frac{\partial}{\partial r}\left(r^2 \frac{\partial u_{\theta}}{\partial r}\right) +
 \frac{1}{r^2 \sin(\theta)^2} \frac{\partial^2 u_{\theta}}{\partial \phi^2} +
 \frac{1}{r^2 \sin(\theta)} \frac{\partial}{\partial \theta}\left(\sin(\theta) \frac{\partial u_{\theta}}{\partial \theta}\right) +
 \frac{2}{r^2} \frac{\partial u_r}{\partial \theta} - \frac{u_{\theta} +
 \right].
\end{align}</math>
Mass continuity will read:
These equations could be (slightly) compacted by, for example, factoring formula_78 from the viscous terms. However, doing so would undesirably alter the structure of the Laplacian and other quantities.
Navier–Stokes equations use in games.
The Navier–Stokes equations are used extensively in video games in order to model a wide variety of natural phenomena. Simulations of small-scale gaseous fluids, such as fire and smoke, are often based on the seminal paper "Real-Time Fluid Dynamics for Games" by Jos Stam, which elaborates one of the methods proposed in Stam's earlier, more famous paper "Stable Fluids" from 1999. Stam proposes stable fluid simulation using a Navier–Stokes solution method from 1968, coupled with an unconditionally stable semi-Lagrangian advection scheme, as first proposed in 1992.
More recent implementations based upon this work run on the GPU as opposed to the CPU and achieve a much higher degree of performance.
Many improvements have been proposed to Stam's original work, which suffers inherently from high numerical dissipation in both velocity and mass.
An introduction to interactive fluid simulation can be found in the 2007 ACM SIGGRAPH course, Fluid Simulation for Computer Animation.

</doc>
<doc id="334290" url="https://en.wikipedia.org/wiki?curid=334290" title="Neutralino">
Neutralino

In supersymmetry, the neutralino is a hypothetical particle. There are four neutralinos that are fermions and are electrically neutral, the lightest of which is typically stable. They are typically labeled (the lightest), , and (the heaviest) although sometimes formula_1 is also used when formula_2 is used to refer to charginos. These four states are mixtures of the bino and the neutral wino (which are the neutral electroweak gauginos), and the neutral higgsinos. As the neutralinos are Majorana fermions, each of them is identical to its antiparticle. Because these particles only interact with the weak vector bosons, they are not directly produced at hadron colliders in copious numbers. They would primarily appear as particles in cascade decays of heavier particles (decays that happen in multiple steps) usually originating from colored supersymmetric particles such as squarks or gluinos.
In R-parity conserving models, the lightest neutralino is stable and all supersymmetric cascade-decays end up decaying into this particle which leaves the detector unseen and its existence can only be inferred by looking for unbalanced momentum in a detector.
The heavier neutralinos typically decay through a neutral Z boson to a lighter neutralino or through a charged W boson to a light chargino:
The mass splittings between the different neutralinos will dictate which patterns of decays are allowed.
Up to present, neutralinos have never been observed or detected in an experiment.
Origins in supersymmetric theories.
In supersymmetry models, all Standard Model particles have partner particles with the same quantum numbers except for the quantum number spin, which differs by 1/2 from its partner particle. Since the superpartners of the Z boson (zino), the photon (photino) and the neutral higgs (higgsino) have the same quantum numbers, they can mix to form four eigenstates of the mass operator called "neutralinos". In many models the lightest of the four neutralinos turns out to be the lightest supersymmetric particle (LSP), though other particles may also take on this role.
Phenomenology.
The exact properties of each neutralino will depend on the details of the mixing (e.g. whether they are more higgsino-like or gaugino-like), but they tend to have masses at the weak scale (100 GeV – 1 TeV) and couple to other particles with strengths characteristic of the weak interaction. In this way they are phenomenologically similar to neutrinos, and so are not directly observable in particle detectors at accelerators.
In models in which R-parity is conserved and the lightest of the four neutralinos is the LSP, the lightest neutralino is stable and is eventually produced in the decay chain of all other superpartners. In such cases supersymmetric processes at accelerators are characterized by a large discrepancy in energy and momentum between the visible initial and final state particles, with this energy being carried off by a neutralino which departs the detector unnoticed.
This is an important signature to discriminate supersymmetry from Standard Model backgrounds.
Relationship to dark matter.
As a heavy, stable particle, the lightest neutralino is an excellent candidate to form the universe's cold dark matter. In many models the lightest neutralino can be produced thermally in the hot early universe and leave approximately the right relic abundance to account for the observed dark matter. A lightest neutralino of roughly 10–10000 GeV is the leading weakly interacting massive particle (WIMP) dark matter candidate.
Neutralino dark matter could be observed experimentally in nature either indirectly or directly. For indirect observation, gamma ray and neutrino telescopes look for evidence of neutralino annihilation in regions of high dark matter density such as the galactic or solar centre. For direct observation, special purpose experiments such as the Cryogenic Dark Matter Search (CDMS) seek to detect the rare impacts of WIMPs in terrestrial detectors. These experiments have begun to probe interesting supersymmetric parameter space, excluding some models for neutralino dark matter, and upgraded experiments with greater sensitivity are under development.

</doc>
<doc id="21485" url="https://en.wikipedia.org/wiki?curid=21485" title="Neutrino">
Neutrino

A neutrino ( or , in Italian ) is an electrically neutral elementary particle with half-integer spin. The neutrino (meaning "little neutral one" in Italian) is denoted by the Greek letter ν ("nu"). The mass of the neutrino is tiny compared to other subatomic particles. They are the only identified candidate for dark matter, specifically hot dark matter.
Neutrinos are leptons, along with the charged electrons, muons, and taus, and come in three flavors: electron neutrinos (), muon neutrinos (), and tau neutrinos (). Each flavor is also associated with an antiparticle, called an "antineutrino", which also has no electric charge and half-integer spin. Neutrinos are produced in a way that conserves lepton number; i.e., for every electron neutrino produced, a positron (anti-electron) is produced, and for every electron antineutrino produced, an electron is produced as well.
Neutrinos do not carry any electric charge, which means that they are not affected by the electromagnetic force that acts on charged particles, and are leptons, so they are not affected by the strong force that acts on particles inside atomic nuclei. Neutrinos are therefore affected only by the weak subatomic force and by gravity. The weak force is a very short-range interaction, and gravity is extremely weak on the subatomic scale. Thus, neutrinos typically pass through normal matter unimpeded and undetected.
Neutrinos can be created in several ways, including in certain types of radioactive decay, in nuclear reactions such as those that take place in the Sun, in nuclear reactors, when cosmic rays hit atoms and in supernovae. The majority of neutrinos in the vicinity of the Earth are from nuclear reactions in the Sun. In fact, about 65 billion () solar neutrinos per second pass through every square centimeter perpendicular to the direction of the Sun in the region of the Earth.
Neutrinos oscillate between different flavors in flight. That is, an electron neutrino produced in a beta decay reaction may arrive in a detector as a muon or tau neutrino. This oscillation requires that the different neutrino flavors have different masses, although these masses have been shown to be tiny. From cosmological measurements, it has been calculated that the sum of the three neutrino masses must be less than one millionth that of the electron.
History.
Pauli's proposal.
The neutrino was postulated first by Wolfgang Pauli in 1930 to explain how beta decay could conserve energy, momentum, and angular momentum (spin). In contrast to Niels Bohr, who proposed a statistical version of the conservation laws to explain the event, Pauli hypothesized an undetected particle that he called a "neutron" in keeping with convention employed for naming both the proton and the electron, which in 1930 were known to be respective products for alpha and beta decay. He considered that the new particle was emitted from the nucleus together with the electron or beta particle in the process of beta decay.
James Chadwick discovered a much more massive nuclear particle in 1932 and also named it a neutron, leaving two kinds of particles with the same name. Pauli earlier had used the term "neutron" for both the particle that conserved energy in beta decay, and a presumed neutral particle in the nucleus. The word "neutrino" entered the international vocabulary
through Enrico Fermi, who used it during a conference in Paris in July 1932 and at the Solvay Conference in October 1933, where also Pauli employed it. The name (the Italian equivalent of "little neutral one") was jokingly coined by Edoardo Amaldi during a conversation with Fermi at the Institute of physics of via Panisperna in Rome, in order to distinguish this light neutral particle from Chadwick's neutron. 
In Fermi's theory of beta decay, Chadwick's large neutral particle could decay to a proton, electron, and the smaller neutral particle (flavored as an electron antineutrino):
Fermi's paper, written in 1934, unified Pauli's neutrino with Paul Dirac's positron and Werner Heisenberg's neutron–proton model and gave a solid theoretical basis for future experimental work. However, the journal Nature rejected Fermi's paper, saying that the theory was "too remote from reality". He submitted the paper to an Italian journal, which accepted it, but the general lack of interest in his theory at that early date caused him to switch to experimental physics.
Nevertheless, even in 1934 there were hints that Bohr's idea — that the energy conservation laws were not followed — was incorrect. At the Solvay conference of 1934, the first measurements of the energy spectra of beta decay were reported, and these spectra were found to impose a strict limit on the energy of electrons from each type of beta decay. Such a limit was not expected if the conservation of energy was not upheld, in which case any amount of energy would be expected to be statistically available in at least a few decays. The natural explanation of the beta decay spectrum as first measured in 1934 was that only a limited (and conserved) amount of energy was available, and a new particle was sometimes taking a varying fraction of this limited energy, leaving the rest for the beta particle. Pauli made use of the occasion to publicly emphasize that the still-undetected "neutrino" must be an actual particle.
Direct detection.
In 1942, Wang Ganchang first proposed the use of beta capture to experimentally detect neutrinos. In the 20 July 1956 issue of "Science", Clyde Cowan, Frederick Reines, F. B. Harrison, H. W. Kruse, and A. D. McGuire published confirmation that they had detected the neutrino, a result that was rewarded almost forty years later with the 1995 Nobel Prize.
In this experiment, now known as the Cowan–Reines neutrino experiment, antineutrinos created in a nuclear reactor by beta decay reacted with protons to produce neutrons and positrons:
The positron quickly finds an electron, and they annihilate each other. The two resulting gamma rays (γ) are detectable. The neutron can be detected by its capture on an appropriate nucleus, releasing a gamma ray. The coincidence of both events – positron annihilation and neutron capture – gives a unique signature of an antineutrino interaction.
Neutrino flavor.
The antineutrino discovered by Cowan and Reines is the antiparticle of the electron neutrino. In 1962, Leon M. Lederman, Melvin Schwartz and Jack Steinberger showed that more than one type of neutrino exists by first detecting interactions of the muon neutrino (already hypothesised with the name "neutretto"), which earned them the 1988 Nobel Prize in Physics. When the third type of lepton, the tau, was discovered in 1975 at the Stanford Linear Accelerator Center, it too was expected to have an associated neutrino (the tau neutrino). First evidence for this third neutrino type came from the observation of missing energy and momentum in tau decays analogous to the beta decay leading to the discovery of the electron neutrino. The first detection of tau neutrino interactions was announced in summer of 2000 by the DONUT collaboration at Fermilab; its existence had already been inferred by both theoretical consistency and experimental data from the Large Electron–Positron Collider.
Solar neutrino problem.
Starting in the late 1960s, several experiments found that the number of electron neutrinos arriving from the Sun was between one third and one half the number predicted by the Standard Solar Model. This discrepancy, which became known as the solar neutrino problem, remained unresolved for some thirty years. It was resolved by discovery of neutrino oscillation and mass. (The Standard Model of particle physics had assumed that neutrinos are massless and cannot change flavor. However, if neutrinos had mass, they could change flavor, or "oscillate" between flavors).
Oscillation.
A practical method for investigating neutrino oscillations was first suggested by Bruno Pontecorvo in 1957 using an analogy with kaon oscillations; over the subsequent 10 years he developed the mathematical formalism and the modern formulation of vacuum oscillations. In 1985 Stanislav Mikheyev and Alexei Smirnov (expanding on 1978 work by Lincoln Wolfenstein) noted that flavor oscillations can be modified when neutrinos propagate through matter. This so-called Mikheyev–Smirnov–Wolfenstein effect (MSW effect) is important to understand because many neutrinos emitted by fusion in the Sun pass through the dense matter in the solar core (where essentially all solar fusion takes place) on their way to detectors on Earth.
Starting in 1998, experiments began to show that solar and atmospheric neutrinos change flavors (see Super-Kamiokande and Sudbury Neutrino Observatory). This resolved the solar neutrino problem: the electron neutrinos produced in the Sun had partly changed into other flavors which the experiments could not detect.
Although individual experiments, such as the set of solar neutrino experiments, are consistent with non-oscillatory mechanisms of neutrino flavor conversion, taken altogether, neutrino experiments imply the existence of neutrino oscillations. Especially relevant in this context are the reactor experiment KamLAND and the accelerator experiments such as MINOS. The KamLAND experiment has indeed identified oscillations as the neutrino flavor conversion mechanism involved in the solar electron neutrinos. Similarly MINOS confirms the oscillation of atmospheric neutrinos and gives a better determination of the mass squared splitting. Takaaki Kajita of Japan and Arthur B. McDonald of Canada received the 2015 Nobel Prize for Physics for their landmark finding, theoretical and experimental, that neutrinos can change flavors.
Supernova neutrinos.
Raymond Davis, Jr. and Masatoshi Koshiba were jointly awarded the 2002 Nobel Prize in Physics; Davis for his pioneer work on cosmic neutrinos and Koshiba for the first real time observation of supernova neutrinos. The detection of solar neutrinos, and of neutrinos of the SN 1987A supernova in 1987 marked the beginning of neutrino astronomy. In an average supernova, approximately 1057 (an Octodecillion) neutrinos are released.
Properties and reactions.
The neutrino has half-integer spin () and is therefore a fermion. Neutrinos interact primarily through the weak force. The discovery of neutrino flavor oscillations implies that neutrinos have mass. The existence of a neutrino mass strongly suggests the existence of a tiny neutrino magnetic moment of the order of , allowing the possibility that neutrinos may interact electromagnetically as well. An experiment done by C. S. Wu at Columbia University showed that neutrinos always have left-handed chirality. It is very hard to uniquely identify neutrino interactions among the natural background of radioactivity. For this reason, in early experiments a special reaction channel was chosen to facilitate the identification: the interaction of an antineutrino with one of the hydrogen nuclei in the water molecules. A hydrogen nucleus is a single proton, so simultaneous nuclear interactions, which would occur within a heavier nucleus, don't need to be considered for the detection experiment. Within a cubic metre of water placed right outside a nuclear reactor, only relatively few such interactions can be recorded, but the setup is now used for measuring the reactor's plutonium production rate.
Mikheyev–Smirnov–Wolfenstein effect.
Neutrinos traveling through matter, in general, undergo a process analogous to light traveling through a transparent material. This process is not directly observable because it does not produce ionizing radiation, but gives rise to the MSW effect. Only a small fraction of the neutrino's energy is transferred to the material.
Nuclear reactions.
Neutrinos can interact with a nucleus, changing it to another nucleus. This process is used in radiochemical neutrino detectors. In this case, the energy levels and spin states within the target nucleus have to be taken into account to estimate the probability for an interaction. In general the interaction probability increases with the number of neutrons and protons within a nucleus.
Induced fission.
Very much like neutrons do in nuclear reactors, neutrinos can induce fission reactions within heavy nuclei. So far, this reaction has not been measured in a laboratory, but is predicted to happen within stars and supernovae. The process affects the abundance of isotopes seen in the universe. Neutrino fission of deuterium nuclei has been observed in the Sudbury Neutrino Observatory, which uses a heavy water detector.
Types.
There are three known types ("flavors") of neutrinos: electron neutrino , muon neutrino and tau neutrino , named after their partner leptons in the Standard Model (see table at right). The current best measurement of the number of neutrino types comes from observing the decay of the Z boson. This particle can decay into any light neutrino and its antineutrino, and the more types of light neutrinos available, the shorter the lifetime of the Z boson. Measurements of the Z lifetime have shown that the number of light neutrino types is 3. The correspondence between the six quarks in the Standard Model and the six leptons, among them the three neutrinos, suggests to physicists' intuition that there should be exactly three types of neutrino. However, actual proof that there are only three kinds of neutrinos remains an elusive goal of particle physics.
The possibility of "sterile" neutrinos—relatively light neutrinos which do not participate in the weak interaction but which could be created through flavor oscillation (see below)—is unaffected by these Z-boson-based measurements, and the existence of such particles is in fact hinted by experimental data from the LSND experiment. However, the currently running MiniBooNE experiment suggested, until recently, that sterile neutrinos are not required to explain the experimental data, although the latest research into this area is on-going and anomalies in the MiniBooNE data may allow for exotic neutrino types, including sterile neutrinos. A recent re-analysis of reference electron spectra data from the Institut Laue-Langevin has also hinted at a fourth, sterile neutrino.
Recently analyzed data from the Wilkinson Microwave Anisotropy Probe of the cosmic background radiation is compatible with either three or four types of neutrinos. It is hoped that the addition of two more years of data from the probe will resolve this uncertainty.
Antineutrinos.
Antineutrinos, the antiparticles of neutrinos, are neutral particles produced in nuclear beta decay. These are emitted during beta particle emissions, in which a neutron decays into a proton, electron, and antineutrino. They have a spin of ½, and are part of the lepton family of particles. All antineutrinos observed thus far possess right-handed helicity (i.e. only one of the two possible spin states has ever been seen), while neutrinos are left-handed. Antineutrinos, like neutrinos, interact with other matter only through the gravitational and weak forces, making them very difficult to detect experimentally. Neutrino oscillation experiments indicate that antineutrinos have mass, but beta decay experiments constrain that mass to be very small. A neutrino–antineutrino interaction has been suggested in attempts to form a composite photon with the neutrino theory of light.
Because antineutrinos and neutrinos are neutral particles, it is possible that they are actually the same particle. Particles that have this property are known as Majorana particles. Majorana neutrinos have the property that the neutrino and antineutrino could be distinguished only by chirality; what experiments observe as a difference between the neutrino and antineutrino could simply be due to one particle with two possible chiralities. If neutrinos are indeed Majorana particles, neutrinoless double beta decay, as well as a range of other lepton number violating phenomena, would be allowed. Several experiments have been and are being conducted to search for this process.
Researchers around the world have begun to investigate the possibility of using antineutrinos for reactor monitoring in the context of preventing the proliferation of nuclear weapons.
Antineutrinos were first detected as a result of their interaction with protons in a large tank of water. This was installed next to a nuclear reactor as a controllable source of the antineutrinos. (See: Cowan–Reines neutrino experiment)
Only antineutrinos, not neutrinos, take part in the Glashow resonance.
Flavor oscillations.
Neutrinos are most often created or detected with a well defined flavor (electron, muon, tau). However, in a phenomenon known as neutrino flavor oscillation, neutrinos are able to oscillate among the three available flavors while they propagate through space. Specifically, this occurs because the neutrino flavor eigenstates are not the same as the neutrino mass eigenstates (simply called 1, 2, 3). This allows for a neutrino that was produced as an electron neutrino at a given location to have a calculable probability to be detected as either a muon or tau neutrino after it has traveled to another location. This quantum mechanical effect was first hinted by the discrepancy between the number of electron neutrinos detected from the Sun's core failing to match the expected numbers, dubbed as the "solar neutrino problem". In the Standard Model the existence of flavor oscillations implies nonzero differences between the neutrino masses, because the amount of mixing between neutrino flavors at a given time depends on the differences between their squared masses. There are other possibilities in which neutrino can oscillate even if they are massless. If Lorentz invariance is not an exact symmetry, neutrinos can experience Lorentz-violating oscillations.
It is possible that the neutrino and antineutrino are in fact the same particle, a hypothesis first proposed by the Italian physicist Ettore Majorana. The neutrino could transform into an antineutrino (and vice versa) by flipping the orientation of its spin state.
This change in spin would require the neutrino and antineutrino to have nonzero mass, and therefore travel slower than light, because such a spin flip, caused only by a change in point of view, can take place only if inertial frames of reference exist that move faster than the particle: such a particle has a spin of one orientation when seen from a frame which moves slower than the particle, but the opposite spin when observed from a frame that moves faster than the particle.
On July 19, 2013 the results from the T2K experiment presented at the European Physical Society Conference on High Energy Physics in Stockholm, Sweden, confirmed neutrino oscillation theory.
Speed.
Before neutrinos were found to oscillate, they were generally assumed to be massless, propagating at the speed of light. According to the theory of special relativity, the question of neutrino velocity is closely related to their mass. If neutrinos are massless, they must travel at the speed of light. However, if they have mass, they cannot reach the speed of light.
Also some Lorentz-violating variants of quantum gravity might allow faster-than-light neutrinos. A comprehensive framework for Lorentz violations is the Standard-Model Extension (SME).
In the early 1980s, first measurements of neutrino speed were done using pulsed pion beams (produced by pulsed proton beams hitting a target). The pions decayed producing neutrinos, and the neutrino interactions observed within a time window in a detector at a distance were consistent with the speed of light. This measurement was repeated in 2007 using the MINOS detectors, which found the speed of neutrinos to be, at the 99% confidence level, in the range between and . The central value of 1.000051"c" is higher than the speed of light but is also consistent with a velocity of exactly "c" or even slightly less. This measurement set an upper bound on the mass of the muon neutrino of at 99% confidence. After the detectors for the project were upgraded in 2012, MINOS refined their initial result and found agreement with the speed of light, with the difference in the arrival time of neutrinos and light of -0.0006% (±0.0012%).
A similar observation was made, on a much larger scale, with supernova 1987A (SN 1987A). 10-MeV antineutrinos from the supernova were detected within a time window that was consistent with the speed of light for the neutrinos. Currently, the question of whether or not neutrinos have mass cannot be decided; their speed is (as yet) indistinguishable from the speed of light.
In September 2011, the OPERA collaboration released calculations showing velocities of 17-GeV and 28-GeV neutrinos exceeding the speed of light in their experiments (see Faster-than-light neutrino anomaly). In November 2011, OPERA repeated its experiment with changes so that the speed could be determined individually for each detected neutrino. The results showed the same faster-than-light speed. However, in February 2012 reports came out that the results may have been caused by a loose fiber optic cable attached to one of the atomic clocks which measured the departure and arrival times of the neutrinos. An independent recreation of the experiment in the same laboratory by ICARUS found no discernible difference between the speed of a neutrino and the speed of light.
In June 2012, CERN announced that new measurements conducted by four Gran Sasso experiments (OPERA, ICARUS, Borexino and LVD) found agreement between the speed of light and the speed of neutrinos, finally refuting the initial OPERA result.
Mass.
The Standard Model of particle physics assumed that neutrinos are massless. However the experimentally established phenomenon of neutrino oscillation, which mixes neutrino flavour states with neutrino mass states (analogously to CKM mixing), requires neutrinos to have nonzero masses. Massive neutrinos were originally conceived by Bruno Pontecorvo in the 1950s. Enhancing the basic framework to accommodate their mass is straightforward by adding a right-handed Lagrangian. This can be done in two ways. If, like other fundamental Standard Model particles, mass is generated by the Dirac mechanism, then the framework would require an SU(2) singlet. This particle would have no other Standard Model interactions (apart from the Yukawa interactions with the neutral component of the Higgs doublet), so is called a sterile neutrino. Or, mass can be generated by the Majorana mechanism, which would require the neutrino and antineutrino to be the same particle.
The strongest upper limit on the masses of neutrinos comes from cosmology: the Big Bang model predicts that there is a fixed ratio between the number of neutrinos and the number of photons in the cosmic microwave background. If the total energy of all three types of neutrinos exceeded an average of per neutrino, there would be so much mass in the universe that it would collapse. This limit can be circumvented by assuming that the neutrino is unstable; however, there are limits within the Standard Model that make this difficult. A much more stringent constraint comes from a careful analysis of cosmological data, such as the cosmic microwave background radiation, galaxy surveys, and the Lyman-alpha forest. These indicate that the summed masses of the three neutrinos must be less than .
In 1998, research results at the Super-Kamiokande neutrino detector determined that neutrinos can oscillate from one flavor to another, which requires that they must have a nonzero mass. While this shows that neutrinos have mass, the absolute neutrino mass scale is still not known. This is because neutrino oscillations are sensitive only to the difference in the squares of the masses. The best estimate of the difference in the squares of the masses of mass eigenstates 1 and 2 was published by KamLAND in 2005: Δ"m" = . In 2006, the MINOS experiment measured oscillations from an intense muon neutrino beam, determining the difference in the squares of the masses between neutrino mass eigenstates 2 and 3. The initial results indicate |Δ"m"| = , consistent with previous results from Super-Kamiokande. Since |Δ"m"| is the difference of two squared masses, at least one of them has to have a value which is at least the square root of this value. Thus, there exists at least one neutrino mass eigenstate with a mass of at least .
In 2009, lensing data of a galaxy cluster were analyzed to predict a neutrino mass of about . This surprisingly high value requires that the three neutrino masses be nearly equal, with neutrino oscillations on the order of meV. The masses lie below the Mainz-Troitsk upper bound of for the electron antineutrino. The latter will be tested in 2015 in the KATRIN experiment, that searches for a mass between and .
A number of efforts are under way to directly determine the absolute neutrino mass scale in laboratory experiments. The methods applied involve nuclear beta decay (KATRIN and MARE).
On 31 May 2010, OPERA researchers observed the first tau neutrino candidate event in a muon neutrino beam, the first time this transformation in neutrinos had been observed, providing further evidence that they have mass.
In July 2010 the 3-D MegaZ DR7 galaxy survey reported that they had measured a limit of the combined mass of the three neutrino varieties to be less than . A tighter upper bound yet for this sum of masses, , was reported in March 2013 by the Planck collaboration, whereas a February 2014 result estimates the sum as 0.320 ± 0.081 eV based on discrepancies between the cosmological consequences implied by Planck's detailed measurements of the Cosmic Microwave Background and predictions arising from observing other phenomena, combined with the assumption that neutrinos are responsible for the observed weaker gravitational lensing than would be expected from massless neutrinos.
If the neutrino is a Majorana particle, the mass may be calculated by finding the half life of neutrinoless double-beta decay of certain nuclei. As of 2015, the lowest upper limit on the Majorana mass of the neutrino has been set by KamLAND-Zen: 0.12–0.25 eV.
The Nobel prize in Physics 2015 was awarded to both Takaaki Kajita and Arthur B. McDonald for their experimental discovery of neutrino oscillations, which demonstrates that neutrinos have mass.
Size.
Standard Model neutrinos are fundamental point-like particles. An effective size can be defined using their electroweak cross section (apparent size in electroweak interaction). The average electroweak characteristic size is "r"2 = "n" × 10−33 cm2 ("n" × 1 nanobarn), where "n" = 3.2 for electron neutrino, "n" = 1.7 for muon neutrino and "n" = 1.0 for tau neutrino; it depends on no other properties than mass. However, this is best understood as being relevant only to probability of scattering. Since the neutrino does not interact electromagnetically, and is defined quantum mechanically by a wavefunction, it does not have a size in the same sense as everyday objects. Furthermore, processes that produce neutrinos impart such high energies to them that they travel at almost the speed of light. Nevertheless, neutrinos are fermions, and thus obey the Pauli exclusion principle, i.e. that increasing their density forces them into progressively higher momentum states.
Chirality.
Experimental results show that (nearly) all produced and observed neutrinos have left-handed helicities (spins antiparallel to momenta), and all antineutrinos have right-handed helicities, within the margin of error. In the massless limit, it means that only one of two possible chiralities is observed for either particle. These are the only chiralities included in the Standard Model of particle interactions.
It is possible that their counterparts (right-handed neutrinos and left-handed antineutrinos) simply do not exist. If they do, their properties are substantially different from observable neutrinos and antineutrinos. It is theorized that they are either very heavy (on the order of GUT scale—see "Seesaw mechanism"), do not participate in weak interaction (so-called sterile neutrinos), or both.
The existence of nonzero neutrino masses somewhat complicates the situation. Neutrinos are produced in weak interactions as chirality eigenstates. However, chirality of a massive particle is not a constant of motion; helicity is, but the chirality operator does not share eigenstates with the helicity operator. Free neutrinos propagate as mixtures of left- and right-handed helicity states, with mixing amplitudes on the order of "m"ν/"E". This does not significantly affect the experiments, because neutrinos involved are nearly always ultrarelativistic, and thus mixing amplitudes are vanishingly small. For example, most solar neutrinos have energies on the order of –, so the fraction of neutrinos with "wrong" helicity among them cannot exceed .
Sources.
Artificial.
Reactor neutrinos.
Nuclear reactors are the major source of human-generated neutrinos. Antineutrinos are made in the beta-decay of neutron-rich daughter fragments in the fission process. Generally, the four main isotopes contributing to the antineutrino flux are , , and (i.e. via the antineutrinos emitted during beta-minus decay of their respective fission fragments). The average nuclear fission releases about of energy, of which roughly 4.5% (or about ) is radiated away as antineutrinos. For a typical nuclear reactor with a thermal power of , meaning that the core produces this much heat, and an electrical power generation of , the total power production from fissioning atoms is actually , of which is radiated away as antineutrino radiation and never appears in the engineering. This is to say, of fission energy is "lost" from this reactor and does not appear as heat available to run turbines, since antineutrinos penetrate all building materials practically without interaction.
The antineutrino energy spectrum depends on the degree to which the fuel is burned (plutonium-239 fission antineutrinos on average have slightly more energy than those from uranium-235 fission), but in general, the "detectable" antineutrinos from fission have a peak energy between about 3.5 and , with a maximum energy of about . There is no established experimental method to measure the flux of low-energy antineutrinos. Only antineutrinos with an energy above threshold of can be uniquely identified (see "neutrino detection" below). An estimated 3% of all antineutrinos from a nuclear reactor carry an energy above this threshold. Thus, an average nuclear power plant may generate over antineutrinos per second above this threshold, but also a much larger number (97%/3% = ~30 times this number) below the energy threshold, which cannot be seen with present detector technology.
Accelerator neutrinos.
Some particle accelerators have been used to make neutrino beams. The technique is to collide protons with a fixed target, producing charged pions or kaons. These unstable particles are then magnetically focused into a long tunnel where they decay while in flight. Because of the relativistic boost of the decaying particle, the neutrinos are produced as a beam rather than isotropically. Efforts to construct an accelerator facility where neutrinos are produced through muon decays are ongoing. Such a setup is generally known as a neutrino factory.
Nuclear bombs.
Nuclear bombs also produce very large quantities of neutrinos. Fred Reines and Clyde Cowan considered the detection of neutrinos from a bomb prior to their search for reactor neutrinos; a fission reactor was recommended as a better alternative by Los Alamos physics division leader J.M.B. Kellogg. Fission bombs produce antineutrinos (from the fission process), and fusion bombs produce both neutrinos (from the fusion process) and antineutrinos (from the initiating fission explosion).
Geologic.
Neutrinos are part of the natural background radiation. In particular, the decay chains of and isotopes, as well as, include beta decays which emit antineutrinos. These so-called geoneutrinos can provide valuable information on the Earth's interior. A first indication for geoneutrinos was found by the KamLAND experiment in 2005. KamLAND's main background in the geoneutrino measurement are the antineutrinos coming from reactors. Several future experiments aim at improving the geoneutrino measurement and these will necessarily have to be far away from reactors.
Atmospheric.
Atmospheric neutrinos result from the interaction of cosmic rays with atomic nuclei in the Earth's atmosphere, creating showers of particles, many of which are unstable and produce neutrinos when they decay. A collaboration of particle physicists from Tata Institute of Fundamental Research (India), Osaka City University (Japan) and Durham University (UK) recorded the first cosmic ray neutrino interaction in an underground laboratory in Kolar Gold Fields in India in 1965.
Solar.
Solar neutrinos originate from the nuclear fusion powering the Sun and other stars.
The details of the operation of the Sun are explained by the Standard Solar Model. In short: when four protons fuse to become one helium nucleus, two of them have to convert into neutrons, and each such conversion releases one electron neutrino.
The Sun sends enormous numbers of neutrinos in all directions. Each second, about 65 billion () solar neutrinos pass through every square centimeter on the part of the Earth that faces the Sun. Since neutrinos are insignificantly absorbed by the mass of the Earth, the surface area on the side of the Earth opposite the Sun receives about the same number of neutrinos as the side facing the Sun.
Supernovae.
In 1966 Colgate and White
calculated that neutrinos carry away most of the gravitational energy released by the collapse of massive stars, events now categorized as Type Ib and Ic and Type II supernovae. When such stars collapse, matter densities at the core become so high () that the degeneracy of electrons is not enough to prevent protons and electrons from combining to form a neutron and an electron neutrino. A second and more important neutrino source is the thermal energy (100 billion kelvins) of the newly formed neutron core, which is dissipated via the formation of neutrino–antineutrino pairs of all flavors.
Colgate and White's theory of supernova neutrino production was confirmed in 1987, when neutrinos from supernova 1987A were detected. The water-based detectors Kamiokande II and IMB detected 11 and 8 antineutrinos of thermal origin, respectively, while the scintillator-based Baksan detector found 5 neutrinos (lepton number = 1) of either thermal or electron-capture origin, in a burst lasting less than 13 seconds. The neutrino signal from the supernova arrived at earth several hours before the arrival of the first electromagnetic radiation, as expected from the evident fact that the latter emerges along with the shock wave. The exceptionally feeble interaction with normal matter allowed the neutrinos to pass through the churning mass of the exploding star, while the electromagnetic photons were slowed.
Because neutrinos interact so little with matter, it is thought that a supernova's neutrino emissions carry information about the innermost regions of the explosion. Much of the "visible" light comes from the decay of radioactive elements produced by the supernova shock wave, and even light from the explosion itself is scattered by dense and turbulent gases, and thus delayed. The neutrino burst is expected to reach Earth before any electromagnetic waves, including visible light, gamma rays or radio waves. The exact time delay depends on the velocity of the shock wave and on the thickness of the outer layer of the star. For a Type II supernova, astronomers expect the neutrino flood to be released seconds after the stellar core collapse, while the first electromagnetic signal may emerge hours later, after the explosion shock wave has had time to reach the surface of the star. The SNEWS project uses a network of neutrino detectors to monitor the sky for candidate supernova events; the neutrino signal will provide a useful advance warning of a star exploding in the Milky Way.
Although neutrinos pass through the outer gases of a supernova without scattering, they provide information about the deeper supernova core with evidence that here, even neutrinos scatter to a significant extent. In a supernova core the densities are those of a neutron star (which is expected to be formed in this type of supernova), becoming large enough to influence the duration of the neutrino signal by delaying some neutrinos. The length of the neutrino signal from SN 1987A, some 13 seconds, was far longer than it would take in theory for neutrinos to pass directly through the neutrino-generating core of a supernova, expected to be only 32 kilometers in diameter SN 1987A. The number of neutrinos counted was also consistent with a total neutrino energy of 2.2 x 1046 joules, which was estimated to be nearly all of the total energy of the supernova.
Supernova remnants.
The energy of supernova neutrinos ranges from a few to several tens of MeV. However, the sites where cosmic rays are accelerated are expected to produce neutrinos that are at least one million times more energetic, produced from turbulent gaseous environments left over by supernova explosions: the supernova remnants. The origin of the cosmic rays was attributed to supernovas by Walter Baade and Fritz Zwicky; this hypothesis was refined by Vitaly L. Ginzburg and Sergei I. Syrovatsky who attributed the origin to supernova remnants, and supported their claim by the crucial remark, that the cosmic ray losses of the Milky Way is compensated, if the efficiency of acceleration in supernova remnants is about 10 percent. Ginzburg and Syrovatskii's hypothesis is supported by the specific mechanism of "shock wave acceleration" happening in supernova remnants, which is consistent with the original theoretical picture drawn by Enrico Fermi, and is receiving support from observational data. The very-high-energy neutrinos are still to be seen, but this branch of neutrino astronomy is just in its infancy. The main existing or forthcoming experiments that aim at observing very-high-energy neutrinos from our galaxy are Baikal, AMANDA, IceCube, ANTARES, NEMO and Nestor. Related information is provided by very-high-energy gamma ray observatories, such as VERITAS, HESS and MAGIC. Indeed, the collisions of cosmic rays are supposed to produce charged pions, whose decay give the neutrinos, and also neutral pions, whose decay give gamma rays: the environment of a supernova remnant is transparent to both types of radiation.
Still-higher-energy neutrinos, resulting from the interactions of extragalactic cosmic rays, could be observed with the Pierre Auger Observatory or with the dedicated experiment named ANITA.
Big Bang.
It is thought that, just like the cosmic microwave background radiation left over from the Big Bang, there is a background of low-energy neutrinos in our Universe. In the 1980s it was proposed that these may be the explanation for the dark matter thought to exist in the universe. Neutrinos have one important advantage over most other dark matter candidates: it is known that they exist. However, this idea also has serious problems.
From particle experiments, it is known that neutrinos are very light. This means that they easily move at speeds close to the speed of light. For this reason, dark matter made from neutrinos is termed "hot dark matter". The problem is that being fast moving, the neutrinos would tend to have spread out evenly in the universe before cosmological expansion made them cold enough to congregate in clumps. This would cause the part of dark matter made of neutrinos to be smeared out and unable to cause the large galactic structures that we see.
Further, these same galaxies and groups of galaxies appear to be surrounded by dark matter that is not fast enough to escape from those galaxies. Presumably this matter provided the gravitational nucleus for formation. This implies that neutrinos cannot make up a significant part of the total amount of dark matter.
From cosmological arguments, relic background neutrinos are estimated to have density of 56 of each type per cubic centimeter and temperature () if they are massless, much colder if their mass exceeds . Although their density is quite high, they have not yet been observed in the laboratory, as their energy is below thresholds of most detection methods, and due to extremely low neutrino interaction cross-sections at sub-eV energies. In contrast, boron-8 solar neutrinos—which are emitted with a higher energy—have been detected definitively despite having a space density that is lower than that of relic neutrinos by some 6 orders of magnitude.
Detection.
Neutrinos cannot be detected directly, because they do not ionize the materials they are passing through (they do not carry electric charge and other proposed effects, like the MSW effect, do not produce traceable radiation). A unique reaction to identify antineutrinos, sometimes referred to as inverse beta decay, as applied by Reines and Cowan (see below), requires a very large detector in order to detect a significant number of neutrinos. All detection methods require the neutrinos to carry a minimum threshold energy. So far, there is no detection method for low-energy neutrinos, in the sense that potential neutrino interactions (for example by the MSW effect) cannot be uniquely distinguished from other causes. Neutrino detectors are often built underground in order to isolate the detector from cosmic rays and other background radiation.
Antineutrinos were first detected in the 1950s near a nuclear reactor. Reines and Cowan used two targets containing a solution of cadmium chloride in water. Two scintillation detectors were placed next to the cadmium targets. Antineutrinos with an energy above the threshold of caused charged current interactions with the protons in the water, producing positrons and neutrons. This is very much like decay, where energy is used to convert a proton into a neutron, a positron () and an electron neutrino () is emitted:
From known decay:
In the Cowan and Reines experiment, instead of an outgoing neutrino, you have an incoming antineutrino () from a nuclear reactor:
The resulting positron annihilation with electrons in the detector material created photons with an energy of about . Pairs of photons in coincidence could be detected by the two scintillation detectors above and below the target. The neutrons were captured by cadmium nuclei resulting in gamma rays of about that were detected a few microseconds after the photons from a positron annihilation event.
Since then, various detection methods have been used. Super Kamiokande is a large volume of water surrounded by photomultiplier tubes that watch for the Cherenkov radiation emitted when an incoming neutrino creates an electron or muon in the water. The Sudbury Neutrino Observatory is similar, but uses heavy water as the detecting medium, which uses the same effects, but also allows the additional reaction any-flavor neutrino photo-dissociation of deuterium, resulting in a free neutron which is then detected from gamma radiation after chlorine-capture. Other detectors have consisted of large volumes of chlorine or gallium which are periodically checked for excesses of argon or germanium, respectively, which are created by electron-neutrinos interacting with the original substance. MINOS uses a solid plastic scintillator coupled to photomultiplier tubes, while Borexino uses a liquid pseudocumene scintillator also watched by photomultiplier tubes and the proposed NOνA detector will use liquid scintillator watched by avalanche photodiodes. The IceCube Neutrino Observatory uses of the Antarctic ice sheet near the south pole with photomultiplier tubes distributed throughout the volume.
Motivation for scientific interest.
Neutrinos' low mass and neutral charge mean they interact exceedingly weakly with other particles and fields. This feature of weak interaction interests scientists because it means neutrinos can be used to probe environments that other radiation (such as light or radio waves) cannot penetrate.
Using neutrinos as a probe was first proposed in the mid-20th century as a way to detect conditions at the core of the Sun. The solar core cannot be imaged directly because electromagnetic radiation (such as light) is diffused by the great amount and density of matter surrounding the core. On the other hand, neutrinos pass through the Sun with few interactions. Whereas photons emitted from the solar core may require 40,000 years to diffuse to the outer layers of the Sun, neutrinos generated in stellar fusion reactions at the core cross this distance practically unimpeded at nearly the speed of light.
Neutrinos are also useful for probing astrophysical sources beyond the Solar System because they are the only known particles that are not significantly attenuated by their travel through the interstellar medium. Optical photons can be obscured or diffused by dust, gas, and background radiation. High-energy cosmic rays, in the form of swift protons and atomic nuclei, are unable to travel more than about 100 megaparsecs due to the Greisen–Zatsepin–Kuzmin limit (GZK cutoff). Neutrinos, in contrast, can travel even greater distances barely attenuated.
The galactic core of the Milky Way is fully obscured by dense gas and numerous bright objects. Neutrinos produced in the galactic core might be measurable by Earth-based neutrino telescopes.
Another important use of the neutrino is in the observation of supernovae, the explosions that end the lives of highly massive stars. The core collapse phase of a supernova is an extremely dense and energetic event. It is so dense that no known particles are able to escape the advancing core front except for neutrinos. Consequently, supernovae are known to release approximately 99% of their radiant energy in a short (10-second) burst of neutrinos. These neutrinos are a very useful probe for core collapse studies.
The rest mass of the neutrino is an important test of cosmological and astrophysical theories (see "Dark matter"). The neutrino's significance in probing cosmological phenomena is as great as any other method, and is thus a major focus of study in astrophysical communities.
The study of neutrinos is important in particle physics because neutrinos typically have the lowest mass, and hence are examples of the lowest-energy particles theorized in extensions of the Standard Model of particle physics.
In November 2012 American scientists used a particle accelerator to send a coherent neutrino message through 780 feet of rock. This marks the first use of neutrinos for communication, and future research may permit binary neutrino messages to be sent immense distances through even the densest materials, such as the Earth's core.

</doc>
<doc id="21272" url="https://en.wikipedia.org/wiki?curid=21272" title="Neutron">
Neutron

The neutron is a subatomic particle, symbol or , with no net electric charge and a mass slightly larger than that of a proton. Protons and neutrons, each with mass approximately one atomic mass unit, constitute the nucleus of an atom, and they are collectively referred to as nucleons. Their properties and interactions are described by nuclear physics.
The nucleus consists of Z protons, where Z is called the atomic number, and N neutrons, where N is the neutron number. The atomic number defines the chemical properties of the atom, and the neutron number determines the isotope or nuclide. The terms isotope and nuclide are often used synonymously, but they refer to chemical and nuclear properties, respectively. The atomic mass number, symbol A, equals Z+N. For example, carbon has atomic number 6, and its abundant carbon-12 isotope has 6 neutrons, whereas its rare carbon-13 isotope has 7 neutrons. Some elements occur in nature with only one stable isotope, such as fluorine (see stable nuclide). Other elements occur as many stable isotopes, such as tin with ten stable isotopes. Even though it is not a chemical element, the neutron is included in the table of nuclides.
Within the nucleus, protons and neutrons are bound together through the nuclear force, and neutrons are required for the stability of nuclei. Neutrons are produced copiously in nuclear fission and fusion. They are a primary contributor to the nucleosynthesis of chemical elements within stars through fission, fusion, and neutron capture processes.
The neutron is essential to the production of nuclear power. In the decade after the neutron was discovered in 1932, neutrons were used to effect many different types of nuclear transmutations. With the discovery of nuclear fission in 1938, it was quickly realized that, if a fission event produced neutrons, each of these neutrons might cause further fission events, etc., in a cascade known as a nuclear chain reaction. These events and findings led to the first self-sustaining nuclear reactor (Chicago Pile-1, 1942) and the first nuclear weapon (Trinity, 1945).
Free neutrons, or individual neutrons free of the nucleus, are effectively a form of ionizing radiation, and as such, are a biological hazard, depending upon dose. A small natural "neutron background" flux of free neutrons exists on Earth, caused by cosmic ray muons, and by the natural radioactivity of spontaneously fissionable elements in the Earth's crust. Dedicated neutron sources like neutron generators, research reactors and spallation sources produce free neutrons for use in irradiation and in neutron scattering experiments.
Description.
Neutrons and protons are both nucleons, which are attracted and bound together by the nuclear force to form atomic nuclei. The nucleus of the most common isotope of the hydrogen atom (with the chemical symbol "H") is a lone proton. The nuclei of the heavy hydrogen isotopes deuterium and tritium contain one proton bound to one and two neutrons, respectively. All other types of atomic nuclei are composed of two or more protons and various numbers of neutrons. The most common nuclide of the common chemical element lead, 208Pb has 82 protons and 126 neutrons, for example.
The free neutron has a mass of about (equivalent to , or ). The neutron has a mean square radius of about , or 0.8 fm, and it is a spin-½ fermion. 
The neutron has a magnetic moment with a negative value, because its orientation is opposite to the neutron's spin. The neutron's magnetic moment causes its motion to be influenced by magnetic fields. Although the neutron has no net electric charge, it does have a slight distribution of charge within it. With its positive electric charge, the proton is directly influenced by electric fields, whereas the response of the neutron to this force is much weaker.
A free neutron is unstable, decaying to a proton, electron and antineutrino with a mean lifetime of just under 15 minutes (). This radioactive decay, known as beta decay, is possible since the mass of the neutron is slightly greater than the proton. The free proton is stable. Neutrons or protons bound in a nucleus can be stable or unstable, however, depending on the nuclide. Beta decay, in which neutrons decay to protons, or vice versa, is governed by the weak force, and it requires the emission or absorption of electrons and neutrinos, or their antiparticles.
Protons and neutrons behave almost identically under the influence of the nuclear force within the nucleus. The concept of isospin, in which the proton and neutron are viewed as two quantum states of the same particle, is used to model the interactions of nucleons by the nuclear or weak forces. Because of the strength of the nuclear force at short distances, the binding energy of nucleons is more than seven orders of magnitude larger than the electromagnetic energy binding electrons in atoms. Nuclear reactions (such as nuclear fission) therefore have an energy density that is more than ten million times that of chemical reactions. Because of the mass–energy equivalence, nuclear binding energies add or subtract from the mass of nuclei. Ultimately, the ability of the nuclear force to store energy arising from the electromagnetic repulsion of nuclear components is the basis for most of the energy that makes nuclear reactors or bombs possible. In nuclear fission, the absorption of a neutron by a heavy nuclide (e.g., uranium-235) causes the nuclide to become unstable and break into light nuclides and additional neutrons. The positively charged light nuclides then repel, releasing electromagnetic potential energy.
The neutron is classified as a hadron, since it is composed of quarks, and as a baryon, since it is composed of three quarks. The finite size of the neutron and its magnetic moment indicate the neutron is a composite, rather than elementary, particle. The neutron consists of two down quarks with charge −⅓  "e" and one up quark with charge +⅔ "e", although this simple model belies the complexities of the Standard Model for nuclei. The masses of the three quarks sum to only about , whereas the neutron's mass is about , for example. Like the proton, the quarks of the neutron are held together by the strong force, mediated by gluons.
The nuclear force results from secondary effects of the more fundamental strong force.
Discovery.
The story of the discovery of the neutron and its properties is central to the extraordinary developments in atomic physics that occurred in the first half of the 20th century, leading ultimately to the atomic bomb in 1945. In the 1911 Rutherford model, the atom consisted of a small positively charged massive nucleus surrounded by a much larger cloud of negatively charged electrons. In 1920, Rutherford suggested that the nucleus consisted of positive protons and neutrally-charged particles, suggested to be a proton and an electron bound in some way. Electrons were assumed to reside within the nucleus because it was known that beta radiation consisted of electrons emitted from the nucleus. Rutherford called these uncharged particles "neutrons", by the Latin root for "neutralis" (neuter) and the Greek suffix "-on" (a suffix used in the names of subatomic particles, i.e. "electron" and "proton"). References to the word "neutron" in connection with the atom can be found in the literature as early as 1899, however.
Throughout the 1920s, physicists assumed that the atomic nucleus was composed of protons and "nuclear electrons" but there were obvious problems. It was difficult to reconcile the proton–electron model for nuclei with the Heisenberg uncertainty relation of quantum mechanics. The Klein paradox, discovered by Oskar Klein in 1928, presented further quantum mechanical objections to the notion of an electron confined within a nucleus. Observed properties of atoms and molecules were inconsistent with the nuclear spin expected from proton–electron hypothesis. Since both protons and electrons carry an intrinsic spin of ½ "ħ", there is no way to arrange an odd number of spins ±½ "ħ" to give a spin integer multiple of "ħ". Nuclei with integer spin are common, e.g., 14N.
In 1931, Walther Bothe and Herbert Becker found that if alpha particle radiation from polonium fell on beryllium, boron, or lithium, an unusually penetrating radiation was produced. The radiation was not influenced by an electric field, so Bothe and Becker assumed it was gamma radiation. The following year Irène Joliot-Curie and Frédéric Joliot in Paris showed that if this "gamma" radiation fell on paraffin, or any other hydrogen-containing compound, it ejected protons of very high energy. Neither Rutherford nor James Chadwick at the Cavendish Laboratory in Cambridge were convinced by the gamma ray interpretation. Chadwick quickly performed a series of experiments that showed that the new radiation consisted of uncharged particles with about the same mass as the proton. These particles were neutrons. Chadwick won the Nobel Prize in Physics for this discovery in 1935.
Models for atomic nucleus consisting of protons and neutrons were quickly developed by Werner Heisenberg and others. The proton–neutron model explained the puzzle of nuclear spins. The origins of beta radiation were explained by Enrico Fermi in 1934 by the process of beta decay, in which the neutron decays to a proton by "creating" an electron and a (as yet undiscovered) neutrino. In 1935 Chadwick and his doctoral student Maurice Goldhaber, reported the first accurate measurement of the mass of the neutron.
By 1934, Fermi had bombarded heavier elements with neutrons to induce radioactivity in elements of high atomic number. In 1938, Fermi received the Nobel Prize in Physics "for his demonstrations of the existence of new radioactive elements produced by neutron irradiation, and for his related discovery of nuclear reactions brought about by slow neutrons". In 1938 Otto Hahn, Lise Meitner, and Fritz Strassmann discovered nuclear fission, or the fractionation of uranium nuclei into light elements, induced by neutron bombardment. In 1945 Hahn received the 1944 Nobel Prize in Chemistry "for his discovery of the fission of heavy atomic nuclei." The discovery of nuclear fission would lead to the development of nuclear power and the atomic bomb by the end of World War II.
Beta decay and the stability of the nucleus.
Under the Standard Model of particle physics, the only possible decay mode for the neutron that conserves baryon number is for one of the neutron's quarks to change flavour via the weak interaction. The decay of one of the neutron's down quarks into a lighter up quark can be achieved by the emission of a W boson. By this process, the Standard Model description of beta decay, the neutron decays into a proton (which contains one down and two up quarks), an electron, and an electron antineutrino.
Since interacting protons have a mutual electromagnetic repulsion that is stronger than their attractive nuclear interaction, neutrons are a necessary constituent of any atomic nucleus that contains more than one proton (see diproton and neutron–proton ratio). Neutrons bind with protons and one another in the nucleus via the nuclear force, effectively moderating the repulsive forces between the protons and stabilizing the nucleus.
Free neutron decay.
Outside the nucleus, free neutrons are unstable and have a mean lifetime of (about 14 minutes, 42 seconds); therefore the half-life for this process (which differs from the mean lifetime by a factor of ) is (about 10 minutes, 11 seconds). Beta decay of the neutron, described above, can be denoted by the radioactive decay:
where , , and denote the
proton, electron and electron antineutrino, respectively.
For the free neutron the decay energy for this process (based on the masses of the neutron, proton, and electron) is 0.782343 MeV. The maximal energy of the beta decay electron (in the process wherein the neutrino receives a vanishingly small amount of kinetic energy) has been measured at 0.782 ± .013 MeV. The latter number is not well-enough measured to determine the comparatively tiny rest mass of the neutrino (which must in theory be subtracted from the maximal electron kinetic energy) as well as neutrino mass is constrained by many other methods.
A small fraction (about one in 1000) of free neutrons decay with the same products, but add an extra particle in the form of an emitted gamma ray:
This gamma ray may be thought of as a sort of "internal bremsstrahlung" that arises as the emitted beta particle interacts with the charge of the proton in an electromagnetic way. Internal bremsstrahlung gamma ray production is also a minor feature of beta decays of bound neutrons (as discussed below).
A very small minority of neutron decays (about four per million) are so-called "two-body (neutron) decays", in which a proton, electron and antineutrino are produced as usual, but the electron fails to gain the 13.6 eV necessary energy to escape the proton, and therefore simply remains bound to it, as a neutral hydrogen atom (one of the "two bodies"). In this type of free neutron decay, in essence all of the neutron decay energy is carried off by the antineutrino (the other "body").
The transformation of a free proton to a neutron (plus a positron and a neutrino) is energetically impossible, since a free neutron has a greater mass than a free proton.
Bound neutron decay.
While a free neutron has a half life of about 10.2 min, most neutrons within nuclei are stable. According to the nuclear shell model, the protons and neutrons of a nuclide are a quantum mechanical system organized into discrete energy levels with unique quantum numbers. For a neutron to decay, the resulting proton requires an available state at lower energy than the initial neutron state. In stable nuclei the possible lower energy states are all filled, meaning they are each occupied by two protons with spin up and spin down. The Pauli exclusion principle therefore disallows the decay of a neutron to a proton within stable nuclei. The situation is similar to electrons of an atom, where electrons have distinct atomic orbitals and are prevented from decaying to lower energy states, with the emission of a photon, by the exclusion principle.
Neutrons in unstable nuclei can decay by beta decay as described above. In this case, an energetically allowed quantum state is available for the proton resulting from the decay. One example of this decay is carbon-14 (6 protons, 8 neutrons) that decays to nitrogen-14 (7 protons, 7 neutrons) with a half-life of about 5,730 years.
Inside a nucleus, a proton can transform into a neutron via inverse beta decay, if an energetically allowed quantum state is available for the neutron. This transformation occurs by emission of an antielectron (also called positron) and an electron neutrino:
The transformation of a proton to a neutron inside of a nucleus is also possible through electron capture:
Positron capture by neutrons in nuclei that contain an excess of neutrons is also possible, but is hindered because positrons are repelled by the positive nucleus, and quickly annihilate when they encounter electrons.
Competition of beta decay types.
Three types of beta decay in competition are illustrated by the single isotope copper-64 (29 protons, 35 neutrons), which has a half-life of about 12.7 hours. This isotope has one unpaired proton and one unpaired neutron, so either the proton or the neutron can decay. This particular nuclide (though not all nuclides in this situation) is almost equally likely to decay through proton decay by positron emission (18%) or electron capture (43%), as through neutron decay by electron emission (39%).
Intrinsic properties.
Electric charge.
The total electric charge of the neutron is . This zero value has 
been tested experimentally, and the present experimental limit for the charge of the neutron is
,   or . This value is consistent with zero, given the experimental uncertainties (indicated in parentheses). By comparison, the charge of the proton is, of course, .
Electric dipole moment.
The Standard Model of particle physics predicts a tiny separation of positive and negative charge within the neutron leading to a permanent electric dipole moment. The predicted value is, however, well below the current sensitivity of experiments. From several unsolved puzzles in particle physics, it is clear that the Standard Model is not the final and full description of all particles and their interactions. New theories going beyond the Standard Model generally lead to much larger predictions for the electric dipole moment of the neutron. Currently, there are at least four experiments trying to measure for the first time a finite neutron electric dipole moment, including:
Magnetic moment.
Even though the neutron is a neutral particle, the magnetic moment of a neutron is not zero. Since the neutron is a neutral particle, it is not affected by electric fields, but with its magnetic moment it is affected by magnetic fields. The magnetic moment of the neutron is an indication of its quark substructure and internal charge distribution. 
The value for the neutron's magnetic moment was first directly measured by Luis Alvarez and Felix Bloch at Berkeley, California in 1940, using an extension of the magnetic resonance methods developed by Rabi. Alvarez and Bloch determined the magnetic moment of the neutron to be , where "μ"N is the nuclear magneton.
Structure and geometry of charge distribution.
An article published in 2007 featuring a model-independent analysis concluded that the neutron has a negatively charged exterior, a positively charged middle, and a negative core. In a simplified classical view, the negative "skin" of the neutron assists it to be attracted to the protons with which it interacts in the nucleus. (However, the main attraction between neutrons and protons is via the nuclear force, which does not involve charge.)
The simplified classical view of the neutron's charge distribution also "explains" the fact that the neutron magnetic dipole points in the opposite direction from its spin angular momentum vector (as compared to the proton). This gives the neutron, in effect, a magnetic moment which resembles a negatively charged particle. This can be reconciled classically with a neutral neutron composed of a charge distribution in which the negative sub-parts of the neutron have a larger average radius of distribution, and therefore contribute more to the particle's magnetic dipole moment, than do the positive parts that are, on average, nearer the core.
Mass.
The mass of a neutron cannot be directly determined by mass spectrometry due to lack of electric charge. However, since the mass of protons and deuterons can be measured by mass spectrometry, the mass of a neutron can be deduced by subtracting proton mass from deuteron mass, with the difference being the mass of the neutron plus the binding energy of deuterium (expressed as a positive emitted energy). The latter can be directly measured by measuring the energy (formula_1) of the single gamma photon emitted when neutrons are captured by protons (this is exothermic and happens with zero-energy neutrons), plus the small recoil kinetic energy (formula_2) of the deuteron (about 0.06% of the total energy). 
The energy of the gamma ray can be measured to high precision by X-ray diffraction techniques, as was first done by Bell and Elliot in 1948. The best modern (1986) values for neutron mass by this technique are provided by Greene, et al. These give a neutron mass of:
The value for the neutron mass in MeV is less accurately known, due to less accuracy in the known conversion of u to MeV:
Another method to determine the mass of a neutron starts from the beta decay of the neutron, when the momenta of the resulting proton and electron are measured.
Anti-neutron.
The antineutron is the antiparticle of the neutron. It was discovered by Bruce Cork in the year 1956, a year after the antiproton was discovered. CPT-symmetry puts strong constraints on the relative properties of particles and antiparticles, so studying antineutrons yields provide stringent tests on CPT-symmetry. The fractional difference in the masses of the neutron and antineutron is . Since the difference is only about two standard deviations away from zero, this does not give any convincing evidence of CPT-violation.
Neutron compounds.
Dineutrons and tetraneutrons.
The existence of stable clusters of 4 neutrons, or tetraneutrons, has been hypothesised by a team led by Francisco-Miguel Marqués at the CNRS Laboratory for Nuclear Physics based on observations of the disintegration of beryllium-14 nuclei. This is particularly interesting because current theory suggests that these clusters should not be stable.
The dineutron is another hypothetical particle. In 2012, Artemis Spyrou from Michigan State University and coworkers reported that they observed, for the first time, the dineutron emission in the decay of 16Be. The dineutron character is evidenced by a small emission angle between the two neutrons. The authors measured the two-neutron separation energy to be 1.35(10) MeV, in good agreement with shell model calculations, using standard interactions for this mass region.
Neutronium and neutron stars.
At extremely high pressures and temperatures, nucleons and electrons are believed to collapse into bulk neutronic matter, called neutronium. This is presumed to happen in neutron stars.
The extreme pressure inside a neutron star may deform the neutrons into a cubic symmetry, allowing tighter packing of neutrons.
Detection.
The common means of detecting a charged particle by looking for a track of ionization (such as in a cloud chamber) does not work for neutrons directly. Neutrons that elastically scatter off atoms can create an ionization track that is detectable, but the experiments are not as simple to carry out; other means for detecting neutrons, consisting of allowing them to interact with atomic nuclei, are more commonly used. The commonly used methods to detect neutrons can therefore be categorized according to the nuclear processes relied upon, mainly neutron capture or elastic scattering. A good discussion on neutron detection is found in chapter 14 of the book "Radiation Detection and Measurement" by Glenn F. Knoll (John Wiley & Sons, 1979).
Neutron detection by neutron capture.
A common method for detecting neutrons involves converting the energy released from neutron capture reactions into electrical signals. Certain nuclides have a high neutron capture cross section, which is the probability of absorbing a neutron. Upon neutron capture, the compound nucleus emits more easily detectable radiation, for example an alpha particle, which is then detected. The nuclides , , , , , and are useful for this purpose. 
Neutron detection by elastic scattering.
Neutrons can elastically scatter off nuclei, causing the struck nucleus to recoil. Kinematically, a neutron can transfer more energy to light nuclei such as hydrogen or helium than to heavier nuclei. Detectors relying on elastic scattering are called fast neutron detectors. Recoiling nuclei can ionize and excite further atoms through collisions. Charge and/or scintillation light produced in this way can be collected to produce a detected signal. A major challenge in fast neutron detection is discerning such signals from erroneous signals produced by gamma radiation in the same detector.
Fast neutron detectors have the advantage of not requiring a moderator, and therefore being capable of measuring the neutron's energy, time of arrival, and in certain cases direction of incidence.
Sources and production.
Free neutrons are unstable, although they have the longest half-life of any unstable sub-atomic particle by several orders of magnitude. Their half-life is still only about 10 minutes, however, so they can be obtained only from sources that produce them freshly.
Natural neutron background. A small natural background flux of free neutrons exists everywhere on Earth. In the atmosphere and deep into the ocean, the "neutron background" is caused by muons produced by cosmic ray interaction with the atmosphere. These high energy muons are capable of penetration to considerable depths in water and soil. There, in striking atomic nuclei, among other reactions they induce spallation reactions in which a neutron is liberated from the nucleus. Within the Earth's crust a second source is neutrons produced primarily by spontaneous fission of uranium and thorium present in crustal minerals. The neutron background is not strong enough to be a biological hazard, but it is of importance to very high resolution particle detectors that are looking for very rare events, such as (hypothesized) interactions that might be caused by particles of dark matter. Recent research has shown that even thunderstorms can produce neutrons with energies of up to several tens of MeV.
Even stronger neutron background radiation is produced at the surface of Mars, where the atmosphere is thick enough to generate neutrons from cosmic ray muon production and neutron-spallation, but not thick enough to provide significant protection from the neutrons produced. These neutrons not only produce a Martian surface neutron radiation hazard from direct downward-going neutron radiation but may also produce a significant hazard from reflection of neutrons from the Martian surface, which will produce reflected neutron radiation penetrating upward into a Martian craft or habitat from the floor.
Sources of neutrons for research. These include certain types of radioactive decay (spontaneous fission and neutron emission), and from certain nuclear reactions. Convenient nuclear reactions include tabletop reactions such as natural alpha and gamma bombardment of certain nuclides, often beryllium or deuterium, and induced nuclear fission, such as occurs in nuclear reactors. In addition, high-energy nuclear reactions (such as occur in cosmic radiation showers or accelerator collisions) also produce neutrons from disintigration of target nuclei. Small (tabletop) particle accelerators optimized to produce free neutrons in this way, are called neutron generators.
In practice, the most commonly used small laboratory sources of neutrons use radioactive decay to power neutron production. One noted neutron-producing radioisotope, californium-252 decays (half-life 2.65 years) by spontaneous fission 3% of the time with production of 3.7 neutrons per fission, and is used alone as a neutron source from this process. Nuclear reaction sources (that involve two materials) powered by radioisotopes use an alpha decay source plus a beryllium target, or else a source of high-energy gamma radiation from a source that undergoes beta decay followed by gamma decay, which produces photoneutrons on interaction of the high energy gamma ray with ordinary stable beryllium, or else with the deuterium in heavy water. A popular source of the latter type is radioactive antimony-124 plus beryllium, a system with a half-life of 60.9 days, which can be constructed from natural antimony (which is 42.8% stable antimony-123) by activating it with neutrons in a nuclear reactor, then transported to where the neutron source is needed.
Nuclear fission reactors naturally produce free neutrons; their role is to sustain the energy-producing chain reaction. The intense neutron radiation can also be used to produce various radioisotopes through the process of neutron activation, which is a type of neutron capture.
Experimental nuclear fusion reactors produce free neutrons as a waste product. However, it is these neutrons that possess most of the energy, and converting that energy to a useful form has proved a difficult engineering challenge. Fusion reactors that generate neutrons are likely to create radioactive waste, but the waste is composed of neutron-activated lighter isotopes, which have relatively short (50–100 years) decay periods as compared to typical half-lives of 10,000 years for fission waste, which is long due primarily to the long half-life of alpha-emitting transuranic actinides.
Neutron beams and modification of beams after production.
Free neutron beams are obtained from neutron sources by neutron transport. For access to intense neutron sources, researchers must go to a specialist neutron facility that operates a research reactor or a spallation source.
The neutron's lack of total electric charge makes it difficult to steer or accelerate them. Charged particles can be accelerated, decelerated, or deflected by electric or magnetic fields. These methods have little effect on neutrons. However, some effects may be attained by use of inhomogeneous magnetic fields because of the neutron's magnetic moment. Neutrons can be controlled by methods that include moderation, reflection, and velocity selection. Thermal neutrons can be polarized by transmission through magnetic materials in a method analogous to the Faraday effect for photons. Cold neutrons of wavelengths of 6–7 angstroms can be produced in beams of a high degree of polarization, by use of magnetic mirrors and magnetized interference filters.
Applications.
The neutron plays an important role in many nuclear reactions. For example, neutron capture often results in neutron activation, inducing radioactivity. In particular, knowledge of neutrons and their behavior has been important in the development of nuclear reactors and nuclear weapons. The fissioning of elements like uranium-235 and plutonium-239 is caused by their absorption of neutrons.
"Cold", "thermal" and "hot" neutron radiation is commonly employed in neutron scattering facilities, where the radiation is used in a similar way one uses X-rays for the analysis of condensed matter. Neutrons are complementary to the latter in terms of atomic contrasts by different scattering cross sections; sensitivity to magnetism; energy range for inelastic neutron spectroscopy; and deep penetration into matter.
The development of "neutron lenses" based on total internal reflection within hollow glass capillary tubes or by reflection from dimpled aluminum plates has driven ongoing research into neutron microscopy and neutron/gamma ray tomography.
A major use of neutrons is to excite delayed and prompt gamma rays from elements in materials. This forms the basis of neutron activation analysis (NAA) and prompt gamma neutron activation analysis (PGNAA). NAA is most often used to analyze small samples of materials in a nuclear reactor whilst PGNAA is most often used to analyze subterranean rocks around bore holes and industrial bulk materials on conveyor belts.
Another use of neutron emitters is the detection of light nuclei, in particular the hydrogen found in water molecules. When a fast neutron collides with a light nucleus, it loses a large fraction of its energy. By measuring the rate at which slow neutrons return to the probe after reflecting off of hydrogen nuclei, a neutron probe may determine the water content in soil.
Medical therapies.
Because neutron radiation is both penetrating and ionizing, it can be exploited for medical treatments. Neutron radiation can have the unfortunate side-effect of leaving the affected area radioactive, however. Neutron tomography is therefore not a viable medical application.
Fast neutron therapy utilizes high energy neutrons typically greater than 20 MeV to treat cancer. Radiation therapy of cancers is based upon the biological response of cells to ionizing radiation. If radiation is delivered in small sessions to damage cancerous areas, normal tissue will have time to repair itself, while tumor cells often cannot. Neutron radiation can deliver energy to a cancerous region at a rate an order of magnitude larger than gamma radiation
Beams of low energy neutrons are used in boron capture therapy to treat cancer. In boron capture therapy, the patient is given a drug that contains boron and that preferentially accumulates in the tumor to be targeted. The tumor is then bombarded with very low energy neutrons (although often higher than thermal energy) which are captured by the boron-10 isotope in the boron, which produces an excited state of boron-11 that then decays to produce lithium-7 and an alpha particle that have sufficient energy to kill the malignant cell, but insufficient range to damage nearby cells. For such a therapy to be applied to the treatment of cancer, a neutron source having an intensity of the order of billion (109) neutrons per second per cm2 is preferred. Such fluxes require a research nuclear reactor.
Protection.
Exposure to free neutrons can be hazardous, since the interaction of neutrons with molecules in the body can cause disruption to molecules and atoms, and can also cause reactions that give rise to other forms of radiation (such as protons). The normal precautions of radiation protection apply: Avoid exposure, stay as far from the source as possible, and keep exposure time to a minimum. Some particular thought must be given to how to protect from neutron exposure, however. For other types of radiation, e.g., alpha particles, beta particles, or gamma rays, material of a high atomic number and with high density make for good shielding; frequently, lead is used. However, this approach will not work with neutrons, since the absorption of neutrons does not increase straightforwardly with atomic number, as it does with alpha, beta, and gamma radiation. Instead one needs to look at the particular interactions neutrons have with matter (see the section on detection above). For example, hydrogen-rich materials are often used to shield against neutrons, since ordinary hydrogen both scatters and slows neutrons. This often means that simple concrete blocks or even paraffin-loaded plastic blocks afford better protection from neutrons than do far more dense materials. After slowing, neutrons may then be absorbed with an isotope that has high affinity for slow neutrons without causing secondary capture radiation, such as lithium-6.
Hydrogen-rich ordinary water affects neutron absorption in nuclear fission reactors: Usually, neutrons are so strongly absorbed by normal water that fuel enrichment with fissionable isotope is required. The deuterium in heavy water has a very much lower absorption affinity for neutrons than does protium (normal light hydrogen). Deuterium is, therefore, used in CANDU-type reactors, in order to slow (moderate) neutron velocity, to increase the probability of nuclear fission compared to neutron capture.
Neutron temperature.
Thermal neutrons.
A "thermal neutron" is a free neutron that is Boltzmann distributed with kT = () at room temperature. This gives characteristic (not average, or median) speed of 2.2 km/s. The name 'thermal' comes from their energy being that of the room temperature gas or material they are permeating. (see "kinetic theory" for energies and speeds of molecules). After a number of collisions (often in the range of 10–20) with nuclei, neutrons arrive at this energy level, provided that they are not absorbed.
In many substances, thermal neutron reactions show a much larger effective cross-section than reactions involving faster neutrons, and thermal neutrons can therefore be absorbed more readily (i.e., with higher probability) by any atomic nuclei that they collide with, creating a heavier — and often unstable — isotope of the chemical element as a result.
Most fission reactors use a neutron moderator to slow down, or "thermalize" the neutrons that are emitted by nuclear fission so that they are more easily captured, causing further fission. Others, called fast breeder reactors, use fission energy neutrons directly.
Cold neutrons.
"Cold neutrons" are thermal neutrons that have been equilibrated in a very cold substance such as liquid deuterium. Such a "cold source" is placed in the moderator of a research reactor or spallation source. Cold neutrons are particularly valuable for neutron scattering experiments.
Ultracold neutrons.
Ultracold neutrons are produced by inelastically scattering cold neutrons in substances with a temperature of a few kelvins, such as solid deuterium or superfluid helium. An alternative production method is the mechanical deceleration of cold neutrons.
Fission energy neutrons.
A "fast neutron" is a free neutron with a kinetic energy level close to (), hence a speed of ~ (~ 5% of the speed of light). They are named "fission energy" or "fast" neutrons to distinguish them from lower-energy thermal neutrons, and high-energy neutrons produced in cosmic showers or accelerators. Fast neutrons are produced by nuclear processes such as nuclear fission. Neutrons produced in fission, as noted above, have a Maxwell–Boltzmann distribution of kinetic energies from 0 to ~14 MeV, a mean energy of 2 MeV (for U-235 fission neutrons), and a mode of only 0.75 MeV, which means that more than half of them do not qualify as fast (and thus have almost no chance of initiating fission in fertile materials, such as U-238 and Th-232).
Fast neutrons can be made into thermal neutrons via a process called moderation. This is done with a neutron moderator. In reactors, typically heavy water, light water, or graphite are used to moderate neutrons.
Fusion neutrons.
D–T (deuterium–tritium) fusion is the fusion reaction that produces the most energetic neutrons, with 14.1 MeV of kinetic energy and traveling at 17% of the speed of light. D–T fusion is also the easiest fusion reaction to ignite, reaching near-peak rates even when the deuterium and tritium nuclei have only a thousandth as much kinetic energy as the 14.1 MeV that will be produced.
14.1 MeV neutrons have about 10 times as much energy as fission neutrons, and are very effective at fissioning even non-fissile heavy nuclei, and these high-energy fissions produce more neutrons on average than fissions by lower-energy neutrons. This makes D–T fusion neutron sources such as proposed tokamak power reactors useful for transmutation of transuranic waste. 14.1 MeV neutrons can also produce neutrons by knocking them loose from nuclei.
On the other hand, these very high energy neutrons are less likely to simply be captured without causing fission or spallation. For these reasons, nuclear weapon design extensively utilizes D–T fusion 14.1 MeV neutrons to cause more fission. Fusion neutrons are able to cause fission in ordinarily non-fissile materials, such as depleted uranium (uranium-238), and these materials have been used in the jackets of thermonuclear weapons. Fusion neutrons also can cause fission in substances that are unsuitable or difficult to make into primary fission bombs, such as reactor grade plutonium. This physical fact thus causes ordinary non-weapons grade materials to become of concern in certain nuclear proliferation discussions and treaties.
Other fusion reactions produce much less energetic neutrons. D–D fusion produces a 2.45 MeV neutron and helium-3 half of the time, and produces tritium and a proton but no neutron the other half of the time. D–3He fusion produces no neutron.
Intermediate-energy neutrons.
A fission energy neutron that has slowed down but not yet reached thermal energies is called an epithermal neutron.
Cross sections for both capture and fission reactions often have multiple resonance peaks at specific energies in the epithermal energy range.
These are of less significance in a fast neutron reactor, where most neutrons are absorbed before slowing down to this range, or in a well-moderated thermal reactor, where epithermal neutrons interact mostly with moderator nuclei, not with either fissile or fertile actinide nuclides.
However, in a partially moderated reactor with more interactions of epithermal neutrons with heavy metal nuclei, there are greater possibilities for transient changes in reactivity that might make reactor control more difficult.
Ratios of capture reactions to fission reactions are also worse (more captures without fission) in most nuclear fuels such as plutonium-239, making epithermal-spectrum reactors using these fuels less desirable, as captures not only waste the one neutron captured but also usually result in a nuclide that is not fissile with thermal or epithermal neutrons, though still fissionable with fast neutrons. The exception is uranium-233 of the thorium cycle, which has good capture-fission ratios at all neutron energies.
High-energy neutrons.
These neutrons have much more energy than fission energy neutrons and are generated as secondary particles by particle accelerators or in the atmosphere from cosmic rays. They can have energies as high as tens of joules per neutron. These neutrons are extremely efficient at ionization and far more likely to cause cell death than X-rays or protons.

</doc>
<doc id="1886874" url="https://en.wikipedia.org/wiki?curid=1886874" title="Neutron capture">
Neutron capture

Neutron capture is a nuclear reaction in which an atomic nucleus and one or more neutrons collide and merge to form a heavier nucleus. Since neutrons have no electric charge they can enter a nucleus more easily than positively charged protons, which are repelled electrostatically.
Neutron capture plays an important role in the cosmic nucleosynthesis of heavy elements. In stars it can proceed in two ways: as a rapid (r-process) or a slow process (s-process). Nuclei of masses greater than 56 cannot be formed by thermonuclear reactions (i.e. by nuclear fusion), but can be formed by neutron capture.
Neutron capture at small neutron flux.
At small neutron flux, as in a nuclear reactor, a single neutron is captured by a nucleus. For example, when natural gold (197Au) is irradiated by neutrons, the isotope 198Au is formed in a highly excited state, and quickly decays to the ground state of 198Au by the emission of γ rays. In this process, the mass number increases by one. This is written as a formula in the form 197Au+n → 198Au+γ, or in short form 197Au(n,γ)198Au. If thermal neutrons are used, the process is called thermal capture.
The isotope 198Au is a beta emitter that decays into the mercury isotope 198Hg. In this process the atomic number rises by one.
Neutron capture at high neutron flux.
The r-process happens inside stars if the neutron flux density is so high that the atomic nucleus has no time to decay via beta emission in between neutron captures. The mass number therefore rises by a large amount while the atomic number (i.e., the element) stays the same. Only afterwards, the highly unstable nuclei decay via many β− decays to stable or unstable nuclei of high atomic number.
Capture cross section.
The absorption neutron cross-section of an isotope of a chemical element is the effective cross sectional area that an atom of that isotope presents to absorption, and is a measure of the probability of neutron capture. It is usually measured in barns (b).
Absorption cross section is often highly dependent on neutron energy. Two of the most commonly specified measures are the cross-section for thermal neutron absorption, and resonance integral which considers the contribution of absorption peaks at certain neutron energies specific to a particular nuclide, usually above the thermal range, but encountered as neutron moderation slows the neutron down from an original high energy.
The thermal energy of the nucleus also has an effect; as temperatures rise, Doppler broadening increases the chance of catching a resonance peak. In particular, the increase in uranium-238's ability to absorb neutrons at higher temperatures (and to do so without fissioning) is a negative feedback mechanism that helps keep nuclear reactors under control.
Thermochemical significance.
Neutron capture is involved in the formation of isotopes of chemical elements. As a consequence of this fact the energy of neutron capture intervenes in the standard enthalpy of formation of isotopes.
Uses.
Neutron activation analysis can be used to remotely detect the chemical composition of materials. This is because different elements release different characteristic radiation when they absorb neutrons. This makes it useful in many fields related to mineral exploration and security.
Neutron absorbers.
The most important neutron absorber is 10B as 10B4C in control rods, or boric acid as a coolant water additive in PWRs. Other important neutron absorbers that are used in nuclear reactors are xenon, cadmium, hafnium, gadolinium, cobalt, samarium, titanium, dysprosium, erbium, europium, molybdenum and ytterbium; all of which usually consist of mixtures of various isotopes—some of which are excellent neutron-absorbers. These also occur in combinations such as Mo2B5, hafnium diboride, titanium diboride, dysprosium titanate and gadolinium titanate.
Hafnium, one of the last stable elements to be discovered, presents an interesting case. Even though hafnium is a heavier element, its electron configuration makes it practically identical with the element zirconium, and they are always found in the same ores. However, their nuclear properties are different in a profound way. Hafnium absorbs neutrons avidly (Hf absorbs 600 times more than Zr), and it can be used in reactor control rods, whereas natural zirconium is practically transparent to neutrons. So, zirconium is a very desirable construction material for reactor internal parts, including the metallic cladding of the fuel rods which contain either uranium, plutonium, or mixed oxides of the two elements (MOX fuel).
Hence, it is quite important to be able to separate the zirconium from the hafnium in their naturally-occurring alloy. This can only be done inexpensively by using modern chemical ion-exchange resins. Similar resins are also used in reprocessing nuclear fuel rods, when it is necessary to separate uranium and plutonium, and sometimes thorium.

</doc>
<doc id="17486518" url="https://en.wikipedia.org/wiki?curid=17486518" title="Neutron electric dipole moment">
Neutron electric dipole moment

The neutron electric dipole moment (nEDM) is a measure for the distribution of positive and negative charge inside the neutron. A finite electric dipole moment can only exist if the centers of the negative and positive charge distribution inside the particle do not coincide. So far, no neutron EDM has been found. The current best upper limit amounts to .
Theory.
A permanent electric dipole moment of a fundamental particle violates both parity (P) and time reversal symmetry (T). This can be understood by examining the neutron with its magnetic dipole moment and hypothetical electric dipole moment. Under time reversal, the magnetic dipole moment changes its direction, whereas the electric dipole moment stays unchanged. Under parity, the electric dipole moment changes its direction but not the magnetic dipole moment. As the resulting system under P and T is not symmetric with respect to the initial system, these symmetries are violated in the case of the existence of an EDM. Having also CPT symmetry, the combined symmetry CP is violated as well.
Standard Model prediction.
As it is depicted above, in order to generate a finite nEDM one needs processes that violate CP symmetry. CP violation has been observed in weak interactions and is included in the Standard Model of particle physics via the CP-violating phase in the CKM matrix. However, the amount of CP violation is very small and therefore also the contribution to the nEDM: .
Matter–antimatter asymmetry.
From the asymmetry between matter and antimatter in the universe, one suspects that there must be a sizeable amount of CP-violation. Measuring a neutron electric dipole moment at a much higher level than predicted by the Standard Model would therefore directly confirm this suspicion and improve our understanding of CP-violating processes.
Strong CP problem.
As the neutron is built up of quarks, it is also susceptible to CP violation stemming from strong interactions. Quantum chromodynamics – the theoretical description of the strong force – naturally includes a term which breaks CP-symmetry. The strength of this term is characterized by the angle "θ". The current limit on the nEDM constrains this angle to be less than 10−10 rad. This fine-tuning of the "θ"-angle, which is naturally expected to be of order 1, is the strong CP problem.
SUSY CP problem.
Supersymmetric extensions to the Standard Model, such as the Minimal Supersymmetric Standard Model, generally lead to a large CP-violation. Typical predictions for the neutron EDM arising from the theory range between and . As in the case of the strong interaction, the limit on the neutron EDM is already constraining the CP violating phases. The fine-tuning is, however, not as severe yet.
Experimental technique.
In order to extract the neutron EDM, one measures the Larmor precession of the neutron spin in the presence of parallel and antiparallel magnetic and electric fields. The precession frequency for each of the two cases is given by
the addition or subtraction of the frequencies stemming from the precession of the magnetic moment around the magnetic field and the precession of the electric dipole moment around the electric field. From the difference of those two frequencies one readily obtains a measure of the neutron EDM:
The biggest challenge of the experiment (and at the same time the source of the biggest systematic false effects) is to ensure that the magnetic field does not change during these two measurements.
History.
The first experiments searching for the electric dipole moment of the neutron used beams of thermal (and later cold) neutrons to conduct the measurement. It started with the experiment by Smith, Purcell and Ramsey in 1951 (and published in 1957) obtaining a limit of . Beams of neutrons were used until 1977 for nEDM experiments. At this point, systematic effects related to the high velocities of the neutrons in the beam became insurmountable. The final limit obtained with a neutron beam amounts to .
After that, experiments with ultracold neutrons took over. It started in 1980 with an experiment at the Leningrad Nuclear Physics Institute obtaining a limit of . This experiment and especially the experiment starting in 1984 at the Institut Laue-Langevin pushed the limit down by another two orders of magnitude yielding the above quoted best upper limit in 2006.
During these 50 years of experiments, six orders of magnitude have been covered thereby putting stringent constraints on theoretical models.
Current experiments.
Currently, there are at least five experiments aiming at improving the current limit (or measuring for the first time) on the neutron EDM with a sensitivity down to over the next 10 years, thereby covering the range of prediction coming from supersymmetric extensions to the Standard Model.

</doc>
<doc id="391283" url="https://en.wikipedia.org/wiki?curid=391283" title="Neutron emission">
Neutron emission

Neutron emission is a type of radioactive decay of atoms containing excess neutrons, in which a neutron is simply ejected from the nucleus. Neutron emission is one of the ways an atom reaches its stability. An atom is unstable, therefore radioactive, when the forces in the nucleus are unbalanced. The instability of the nucleus results from the nuclei having extra neutrons or extra protons. Two examples of isotopes that emit neutrons are beryllium-13 (mean life ) and helium-5 (). Commonly, it is abbreviated with a lower case "n". 
As only a neutron is lost in this process, the atom does not gain or lose any protons, and so it does not become an atom of a different element. Instead, the atom will become a new isotope of the original element, such as beryllium-13 becoming beryllium-12 after emitting one of its neutrons.
Neutron emission in fission.
Neutron emission usually happens from nuclei that are in an excited state, such as the excited 17O* produced from the beta decay of 17N. The neutron emission process itself is controlled by the nuclear force and therefore is extremely fast, sometimes referred to as "nearly instantaneous". This process allows unstable atoms to become more stable. The ejection of the neutron may be as a product of the movement of many nucleons, but it is ultimately mediated by the repulsive action of the nuclear force that exists at extremely short-range distances between nucleons. The life time of an ejected neutron inside the nucleus before it is emitted is usually comparable to the flight time of a typical neutron before it leaves the small nuclear "potential well", or about 10−23 seconds.
Induced fission.
A synonym for such neutron emission is "prompt neutron" production, of the type that is best known to occur simultaneously with induced nuclear fission. Induced fission happens only when a nucleus is bombarded with neutrons, gamma rays, or other carriers of energy. Many heavy isotopes, most notably californium-252, also emit prompt neutrons among the products of a similar spontaneous radioactive decay process, spontaneous fission.
Spontaneous fission.
Spontaneous fission happens when an atom's nucleus splits into two smaller nuclei and generally one or more neutron.
Delayed neutrons in reactor control.
Most neutron emission outside prompt neutron production associated with fission (either induced or spontaneous), is from neutron-heavy isotopes produced as fission products. These neutrons are sometimes emitted with a delay, giving them the term delayed neutrons, but the actual delay in their production is a delay waiting for the beta decay of fission products to produce the excited-state nuclear precursors that immediately undergo prompt neutron emission. Thus, the delay in neutron emission is not from the neutron-production process, but rather its precursor beta decay, which is controlled by the weak force, and thus requires a far longer time. The beta decay half lives for the precursors to delayed neutron-emitter radioisotopes, are typically fractions of a second to tens of seconds.
Nevertheless, the delayed neutrons emitted by neutron-rich fission products aid control of nuclear reactors by making reactivity change far more slowly than it would if it were controlled by prompt neutrons alone. About 0.65% of neutrons are released in a nuclear chain reaction in a delayed way due to the mechanism of neutron emission, and it is this fraction of neutrons that allows a nuclear reactor to be controlled on human reaction time-scales, without proceeding to a prompt critical state, and runaway melt down.

</doc>
<doc id="244611" url="https://en.wikipedia.org/wiki?curid=244611" title="Newton's law of universal gravitation">
Newton's law of universal gravitation

Newton's law of universal gravitation states that any two bodies in the Universe attract each other with a force that is directly proportional to the product of their masses and inversely proportional to the square of the distance between them. This is a general physical law derived from empirical observations by what Isaac Newton called induction. It is a part of classical mechanics and was formulated in Newton's work "Philosophiæ Naturalis Principia Mathematica" ("the "Principia""), first published on 5 July 1687. (When Newton's book was presented in 1686 to the Royal Society, Robert Hooke made a claim that Newton had obtained the inverse square law from him; see the History section below.)
In modern language, the law states: Every point mass attracts every single other point mass by a force pointing along the line intersecting both points. The force is proportional to the product of the two masses and inversely proportional to the square of the distance between them. The first test of Newton's theory of gravitation between masses in the laboratory was the Cavendish experiment conducted by the British scientist Henry Cavendish in 1798. It took place 111 years after the publication of Newton's "Principia" and 71 years after his death.
Newton's law of gravitation resembles Coulomb's law of electrical forces, which is used to calculate the magnitude of electrical force arising between two charged bodies. Both are inverse-square laws, where force is inversely proportional to the square of the distance between the bodies. Coulomb's law has the product of two charges in place of the product of the masses, and the electrostatic constant in place of the gravitational constant.
Newton's law has since been superseded by Einstein's theory of general relativity, but it continues to be used as an excellent approximation of the effects of gravity in most applications. Relativity is required only when there is a need for extreme precision, or when dealing with very strong gravitational fields, such as those found near extremely massive and dense objects, or at very close distances (such as Mercury's orbit around the sun).
History.
Early history.
A recent assessment (by Ofer Gal) about the early history of the inverse square law is "by the late 1660s", the assumption of an "inverse proportion between gravity and the square of distance was rather common and had been advanced by a number of different people for different reasons". The same author does credit Hooke with a significant and even seminal contribution, but he treats Hooke's claim of priority on the inverse square point as uninteresting since several individuals besides Newton and Hooke had at least suggested it, and he points instead to the idea of "compounding the celestial motions" and the conversion of Newton's thinking away from "centrifugal" and towards "centripetal" force as Hooke's significant contributions.
Plagiarism dispute.
In 1686, when the first book of Newton's "Principia" was presented to the Royal Society, Robert Hooke accused Newton of plagiarism by claiming that he had taken from him the "notion" of "the rule of the decrease of Gravity, being reciprocally as the squares of the distances from the Center". At the same time (according to Edmond Halley's contemporary report) Hooke agreed that "the Demonstration of the Curves generated thereby" was wholly Newton's.
In this way the question arose as to what, if anything, Newton owed to Hooke. This is a subject extensively discussed since that time and on which some points continue to excite some controversy.
Hooke's work and claims.
Robert Hooke published his ideas about the "System of the World" in the 1660s, when he read to the Royal Society on March 21, 1666, a paper "On gravity", "concerning the inflection of a direct motion into a curve by a supervening attractive principle", and he published them again in somewhat developed form in 1674, as an addition to "An Attempt to Prove the Motion of the Earth from Observations". Hooke announced in 1674 that he planned to "explain a System of the World differing in many particulars from any yet known", based on three "Suppositions": that "all Celestial Bodies whatsoever, have an attraction or gravitating power towards their own Centers" [and] "they do also attract all the other Celestial Bodies that are within the sphere of their activity"; that "all bodies whatsoever that are put into a direct and simple motion, will so continue to move forward in a straight line, till they are by some other effectual powers deflected and bent..."; and that "these attractive powers are so much the more powerful in operating, by how much the nearer the body wrought upon is to their own Centers". Thus Hooke clearly postulated mutual attractions between the Sun and planets, in a way that increased with nearness to the attracting body, together with a principle of linear inertia.
Hooke's statements up to 1674 made no mention, however, that an inverse square law applies or might apply to these attractions. Hooke's gravitation was also not yet universal, though it approached universality more closely than previous hypotheses. He also did not provide accompanying evidence or mathematical demonstration. On the latter two aspects, Hooke himself stated in 1674: "Now what these several degrees [of attraction] are I have not yet experimentally verified"; and as to his whole proposal: "This I only hint at present", "having my self many other things in hand which I would first compleat, and therefore cannot so well attend it" (i.e. "prosecuting this Inquiry"). It was later on, in writing on 6 January 1679|80 to Newton, that Hooke communicated his "supposition ... that the Attraction always is in a duplicate proportion to the Distance from the Center Reciprocall, and Consequently that the Velocity will be in a subduplicate proportion to the Attraction and Consequently as Kepler Supposes Reciprocall to the Distance." (The inference about the velocity was incorrect.)
Hooke's correspondence of 1679-1680 with Newton mentioned not only this inverse square supposition for the decline of attraction with increasing distance, but also, in Hooke's opening letter to Newton, of 24 November 1679, an approach of "compounding the celestial motions of the planets of a direct motion by the tangent & an attractive motion towards the central body".
Newton's work and claims.
Newton, faced in May 1686 with Hooke's claim on the inverse square law, denied that Hooke was to be credited as author of the idea. Among the reasons, Newton recalled that the idea had been discussed with Sir Christopher Wren previous to Hooke's 1679 letter. Newton also pointed out and acknowledged prior work of others, including Bullialdus, (who suggested, but without demonstration, that there was an attractive force from the Sun in the inverse square proportion to the distance), and Borelli (who suggested, also without demonstration, that there was a centrifugal tendency in counterbalance with a gravitational attraction towards the Sun so as to make the planets move in ellipses). D T Whiteside has described the contribution to Newton's thinking that came from Borelli's book, a copy of which was in Newton's library at his death.
Newton further defended his work by saying that had he first heard of the inverse square proportion from Hooke, he would still have some rights to it in view of his demonstrations of its accuracy. Hooke, without evidence in favor of the supposition, could only guess that the inverse square law was approximately valid at great distances from the center. According to Newton, while the 'Principia' was still at pre-publication stage, there were so many a-priori reasons to doubt the accuracy of the inverse-square law (especially close to an attracting sphere) that "without my (Newton's) Demonstrations, to which Mr Hooke is yet a stranger, it cannot believed by a judicious Philosopher to be any where accurate."
This remark refers among other things to Newton's finding, supported by mathematical demonstration, that if the inverse square law applies to tiny particles, then even a large spherically symmetrical mass also attracts masses external to its surface, even close up, exactly as if all its own mass were concentrated at its center. Thus Newton gave a justification, otherwise lacking, for applying the inverse square law to large spherical planetary masses as if they were tiny particles. In addition, Newton had formulated in Propositions 43-45 of Book 1, and associated sections of Book 3, a sensitive test of the accuracy of the inverse square law, in which he showed that only where the law of force is accurately as the inverse square of the distance will the directions of orientation of the planets' orbital ellipses stay constant as they are observed to do apart from small effects attributable to inter-planetary perturbations.
In regard to evidence that still survives of the earlier history, manuscripts written by Newton in the 1660s show that Newton himself had arrived by 1669 at proofs that in a circular case of planetary motion, "endeavour to recede" (what was later called centrifugal force) had an inverse-square relation with distance from the center. After his 1679-1680 correspondence with Hooke, Newton adopted the language of inward or centripetal force. According to Newton scholar J. Bruce Brackenridge, although much has been made of the change in language and difference of point of view, as between centrifugal or centripetal forces, the actual computations and proofs remained the same either way. They also involved the combination of tangential and radial displacements, which Newton was making in the 1660s. The lesson offered by Hooke to Newton here, although significant, was one of perspective and did not change the analysis. This background shows there was basis for Newton to deny deriving the inverse square law from Hooke.
Newton's acknowledgment.
On the other hand, Newton did accept and acknowledge, in all editions of the 'Principia', that Hooke (but not exclusively Hooke) had separately appreciated the inverse square law in the solar system. Newton acknowledged Wren, Hooke and Halley in this connection in the Scholium to Proposition 4 in Book 1. Newton also acknowledged to Halley that his correspondence with Hooke in 1679-80 had reawakened his dormant interest in astronomical matters, but that did not mean, according to Newton, that Hooke had told Newton anything new or original: "yet am I not beholden to him for any light into that business but only for the diversion he gave me from my other studies to think on these things & for his dogmaticalness in writing as if he had found the motion in the Ellipsis, which inclined me to try it ..."
Modern controversy.
Since the time of Newton and Hooke, scholarly discussion has also touched on the question of whether Hooke's 1679 mention of 'compounding the motions' provided Newton with something new and valuable, even though that was not a claim actually voiced by Hooke at the time. As described above, Newton's manuscripts of the 1660s do show him actually combining tangential motion with the effects of radially directed force or endeavour, for example in his derivation of the inverse square relation for the circular case. They also show Newton clearly expressing the concept of linear inertia—for which he was indebted to Descartes' work, published in 1644 (as Hooke probably was). These matters do not appear to have been learned by Newton from Hooke.
Nevertheless, a number of authors have had more to say about what Newton gained from Hooke and some aspects remain controversial. The fact that most of Hooke's private papers had been destroyed or have disappeared does not help to establish the truth.
Newton's role in relation to the inverse square law was not as it has sometimes been represented. He did not claim to think it up as a bare idea. What Newton did was to show how the inverse-square law of attraction had many necessary mathematical connections with observable features of the motions of bodies in the solar system; and that they were related in such a way that the observational evidence and the mathematical demonstrations, taken together, gave reason to believe that the inverse square law was not just approximately true but exactly true (to the accuracy achievable in Newton's time and for about two centuries afterwards – and with some loose ends of points that could not yet be certainly examined, where the implications of the theory had not yet been adequately identified or calculated).
About thirty years after Newton's death in 1727, Alexis Clairaut, a mathematical astronomer eminent in his own right in the field of gravitational studies, wrote after reviewing what Hooke published, that "One must not think that this idea ... of Hooke diminishes Newton's glory"; and that "the example of Hooke" serves "to show what a distance there is between a truth that is glimpsed and a truth that is demonstrated".
Modern form.
In modern language, the law states the following:
Assuming SI units, "F" is measured in newtons (N), "m"1 and "m"2 in kilograms (kg), "r" in meters (m), and the constant "G" is approximately equal to .
The value of the constant "G" was first accurately determined from the results of the Cavendish experiment conducted by the British scientist Henry Cavendish in 1798, although Cavendish did not himself calculate a numerical value for "G". This experiment was also the first test of Newton's theory of gravitation between masses in the laboratory. It took place 111 years after the publication of Newton's "Principia" and 71 years after Newton's death, so none of Newton's calculations could use the value of "G"; instead he could only calculate a force relative to another force.
Bodies with spatial extent.
If the bodies in question have spatial extent (rather than being theoretical point masses), then the gravitational force between them is calculated by summing the contributions of the notional point masses which constitute the bodies. In the limit, as the component point masses become "infinitely small", this entails integrating the force (in vector form, see below) over the extents of the two bodies.
In this way it can be shown that an object with a spherically-symmetric distribution of mass exerts the same gravitational attraction on external bodies as if all the object's mass were concentrated at a point at its centre. (This is not generally true for non-spherically-symmetrical bodies.)
For points "inside" a spherically-symmetric distribution of matter, Newton's Shell theorem can be used to find the gravitational force. The theorem tells us how different parts of the mass distribution affect the gravitational force measured at a point located a distance r0 from the center of the mass distribution:
As a consequence, for example, within a shell of uniform thickness and density there is "no net" gravitational acceleration anywhere within the hollow sphere.
Furthermore, inside a uniform sphere the gravity increases linearly with the distance from the center; the increase due to the additional mass is 1.5 times the decrease due to the larger distance from the center. Thus, if a spherically symmetric body has a uniform core and a uniform mantle with a density that is less than 2/3 of that of the core, then the gravity initially decreases outwardly beyond the boundary, and if the sphere is large enough, further outward the gravity increases again, and eventually it exceeds the gravity at the core/mantle boundary. The gravity of the Earth may be highest at the core/mantle boundary.
Vector form.
Newton's law of universal gravitation can be written as a vector equation to account for the direction of the gravitational force as well as its magnitude. In this formula, quantities in bold represent vectors.
where
It can be seen that the vector form of the equation is the same as the scalar form given earlier, except that F is now a vector quantity, and the right hand side is multiplied by the appropriate unit vector. Also, it can be seen that F12 = −F21.
Gravitational field.
The gravitational field is a vector field that describes the gravitational force which would be applied on an object in any given point in space, per unit mass. It is actually equal to the gravitational acceleration at that point.
It is a generalization of the vector form, which becomes particularly useful if more than 2 objects are involved (such as a rocket between the Earth and the Moon). For 2 objects (e.g. object 2 is a rocket, object 1 the Earth), we simply write r instead of r12 and "m" instead of "m"2 and define the gravitational field g(r) as:
so that we can write:
This formulation is dependent on the objects causing the field. The field has units of acceleration; in SI, this is m/s2.
Gravitational fields are also conservative; that is, the work done by gravity from one position to another is path-independent. This has the consequence that there exists a gravitational potential field "V"(r) such that
If "m"1 is a point mass or the mass of a sphere with homogeneous mass distribution, the force field g(r) outside the sphere is isotropic, i.e., depends only on the distance "r" from the center of the sphere. In that case
the gravitational field is on, inside and outside of symmetric masses.
As per Gauss Law, field in a symmetric body can be found by the mathematical equation:
where formula_8 is a closed surface and formula_9 is the mass enclosed by the surface.
Hence, for a hollow sphere of radius formula_10 and total mass formula_11,
For a uniform solid sphere of radius formula_10 and total mass formula_11,
Problematic aspects.
Newton's description of gravity is sufficiently accurate for many practical purposes and is therefore widely used. Deviations from it are small when the dimensionless quantities "φ"/"c"2 and "(v/c)2" are both much less than one, where "φ" is the gravitational potential, "v" is the velocity of the objects being studied, and "c" is the speed of light.
For example, Newtonian gravity provides an accurate description of the Earth/Sun system, since
where "r"orbit is the radius of the Earth's orbit around the Sun.
In situations where either dimensionless parameter is large, then
general relativity must be used to describe the system. General relativity reduces to Newtonian gravity in the limit of small potential and low velocities, so Newton's law of gravitation is often said to be the low-gravity limit of general relativity.
Observations conflicting with Newton's formula.
The observed fact that the "gravitational mass" and the "inertial mass" is the same for all objects is unexplained within Newton's Theories. General Relativity takes this as a basic principle. See the Equivalence Principle. In point of fact, the experiments of Galileo Galilei, decades before Newton, established that objects that have the same air or fluid resistance are accelerated by the force of the Earth's gravity equally, regardless of their different "inertial" masses. Yet, the forces and energies that are required to accelerate various masses is completely dependent upon their different "inertial" masses, as can be seen from Newton's Second Law of Motion, F = ma.
Newton's reservations.
While Newton was able to formulate his law of gravity in his monumental work, he was deeply uncomfortable with the notion of "action at a distance" which his equations implied. In 1692, in his third letter to Bentley, he wrote: "That one body may act upon another at a distance through a vacuum without the mediation of anything else, by and through which their action and force may be conveyed from one another, is to me so great an absurdity that, I believe, no man who has in philosophic matters a competent faculty of thinking could ever fall into it."
He never, in his words, "assigned the cause of this power". In all other cases, he used the phenomenon of motion to explain the origin of various forces acting on bodies, but in the case of gravity, he was unable to experimentally identify the motion that produces the force of gravity (although he invented two mechanical hypotheses in 1675 and 1717). Moreover, he refused to even offer a hypothesis as to the cause of this force on grounds that to do so was contrary to sound science. He lamented that "philosophers have hitherto attempted the search of nature in vain" for the source of the gravitational force, as he was convinced "by many reasons" that there were "causes hitherto unknown" that were fundamental to all the "phenomena of nature". These fundamental phenomena are still under investigation and, though hypotheses abound, the definitive answer has yet to be found. And in Newton's 1713 "General Scholium" in the second edition of "Principia": "I have not yet been able to discover the cause of these properties of gravity from phenomena and I feign no hypotheses... It is enough that gravity does really exist and acts according to the laws I have explained, and that it abundantly serves to account for all the motions of celestial bodies."
Einstein's solution.
These objections were explained by Einstein's theory of general relativity, in which gravitation is an attribute of curved spacetime instead of being due to a force propagated between bodies. In Einstein's theory, energy and momentum distort spacetime in their vicinity, and other particles move in trajectories determined by the geometry of spacetime. This allowed a description of the motions of light and mass that was consistent with all available observations. In general relativity, the gravitational force is a fictitious force due to the curvature of spacetime, because the gravitational acceleration of a body in free fall is due to its world line being a geodesic of spacetime.
Extensions.
Newton was the first to consider in his Principia an extended expression of his law of gravity including an inverse-cube term of the form 
attempting to explain the Moon's apsidal motion. Other extensions were proposed by Laplace (around 1790) and Decombes (1913):
In recent years quests for non-inverse square terms in the law of gravity have been carried out by neutron interferometry.
Solutions of Newton's law of universal gravitation.
The "n"-body problem is an ancient, classical problem of predicting the individual motions of a group of celestial objects interacting with each other gravitationally. Solving this problem — from the time of the Greeks and on — has been motivated by the desire to understand the motions of the Sun, planets and the visible stars. In the 20th century, understanding the dynamics of globular cluster star systems became an important "n"-body problem too. The "n"-body problem in general relativity is considerably more difficult to solve.
The classical physical problem can be informally stated as: "given the quasi-steady orbital properties" ("instantaneous position, velocity and time") "of a group of celestial bodies, predict their interactive forces; and consequently, predict their true orbital motions for all future times".
The two-body problem has been completely solved, as has the "Restricted 3-Body Problem".

</doc>
<doc id="55212" url="https://en.wikipedia.org/wiki?curid=55212" title="Newton's laws of motion">
Newton's laws of motion

Newton's laws of motion are three physical laws that, together, laid the foundation for classical mechanics. They describe the relationship between a body and the forces acting upon it, and its motion in response to those forces. They have been expressed in several different ways, over nearly three centuries, and can be summarised as follows.
The three laws of motion were first compiled by Isaac Newton in his "Philosophiæ Naturalis Principia Mathematica" ("Mathematical Principles of Natural Philosophy"), first published in 1687. Newton used them to explain and investigate the motion of many physical objects and systems. For example, in the third volume of the text, Newton showed that these laws of motion, combined with his law of universal gravitation, explained Kepler's laws of planetary motion.
Overview.
Newton's laws are applied to objects which are idealised as single point masses, in the sense that the size and shape of the object's body are neglected to focus on its motion more easily. This can be done when the object is small compared to the distances involved in its analysis, or the deformation and rotation of the body are of no importance. In this way, even a planet can be idealised as a particle for analysis of its orbital motion around a star.
In their original form, Newton's laws of motion are not adequate to characterise the motion of rigid bodies and deformable bodies. Leonhard Euler in 1750 introduced a generalisation of Newton's laws of motion for rigid bodies called Euler's laws of motion, later applied as well for deformable bodies assumed as a continuum. If a body is represented as an assemblage of discrete particles, each governed by Newton's laws of motion, then Euler's laws can be derived from Newton's laws. Euler's laws can, however, be taken as axioms describing the laws of motion for extended bodies, independently of any particle structure.
Newton's laws hold only with respect to a certain set of frames of reference called Newtonian or inertial reference frames. Some authors interpret the first law as defining what an inertial reference frame is; from this point of view, the second law only holds when the observation is made from an inertial reference frame, and therefore the first law cannot be proved as a special case of the second. Other authors do treat the first law as a corollary of the second. The explicit concept of an inertial frame of reference was not developed until long after Newton's death.
In the given interpretation mass, acceleration, momentum, and (most importantly) force are assumed to be externally defined quantities. This is the most common, but not the only interpretation of the way one can consider the laws to be a definition of these quantities.
Newtonian mechanics has been superseded by special relativity, but it is still useful as an approximation when the speeds involved are much slower than the speed of light.
Newton's first law.
The first law states that if the net force (the vector sum of all forces acting on an object) is zero, then the velocity of the object is constant. Velocity is a vector quantity which expresses both the object's speed and the direction of its motion; therefore, the statement that the object's velocity is constant is a statement that both its speed and the direction of its motion are constant.
The first law can be stated mathematically as
Consequently,
This is known as "uniform motion". An object "continues" to do whatever it happens to be doing unless a force is exerted upon it. If it is at rest, it continues in a state of rest (demonstrated when a tablecloth is skilfully whipped from under dishes on a tabletop and the dishes remain in their initial state of rest). If an object is moving, it continues to move without turning or changing its speed. This is evident in space probes that continually move in outer space. Changes in motion must be imposed against the tendency of an object to retain its state of motion. In the absence of net forces, a moving object tends to move along a straight line path indefinitely.
Newton placed the first law of motion to establish frames of reference for which the other laws are applicable. The first law of motion postulates the existence of at least one frame of reference called a Newtonian or inertial reference frame, relative to which the motion of a particle not subject to forces is a straight line at a constant speed. Newton's first law is often referred to as the "law of inertia". Thus, a condition necessary for the uniform motion of a particle relative to an inertial reference frame is that the total net force acting on it is zero. In this sense, the first law can be restated as:
Newton's laws are valid only in an inertial reference frame. Any reference frame that is in uniform motion with respect to an inertial frame is also an inertial frame, i.e. Galilean invariance or the principle of Newtonian relativity.
Newton's second law.
The second law states that the net force on an object is equal to the rate of change (that is, the "derivative") of its linear momentum p in an inertial reference frame:
The second law can also be stated in terms of an object's acceleration. Since Newton's second law is only valid for constant-mass systems, mass can be taken outside the differentiation operator by the constant factor rule in differentiation. Thus,
where F is the net force applied, "m" is the mass of the body, and a is the body's acceleration. Thus, the net force applied to a body produces a proportional acceleration. In other words, if a body is accelerating, then there is a force on it.
Consistent with the first law, the time derivative of the momentum is non-zero when the momentum changes direction, even if there is no change in its magnitude; such is the case with uniform circular motion. The relationship also implies the conservation of momentum: when the net force on the body is zero, the momentum of the body is constant. Any net force is equal to the rate of change of the momentum.
Any mass that is gained or lost by the system will cause a change in momentum that is not the result of an external force. A different equation is necessary for variable-mass systems (see below).
Newton's second law requires modification if the effects of special relativity are to be taken into account, because at high speeds the approximation that momentum is the product of rest mass and velocity is not accurate.
Impulse.
An impulse J occurs when a force F acts over an interval of time Δ"t", and it is given by
Since force is the time derivative of momentum, it follows that
This relation between impulse and momentum is closer to Newton's wording of the second law.
Impulse is a concept frequently used in the analysis of collisions and impacts.
Variable-mass systems.
Variable-mass systems, like a rocket burning fuel and ejecting spent gases, are not closed and cannot be directly treated by making mass a function of time in the second law; that is, the following formula is wrong:
The falsehood of this formula can be seen by noting that it does not respect Galilean invariance: a variable-mass object with F = 0 in one frame will be seen to have F ≠ 0 in another frame.
The correct equation of motion for a body whose mass "m" varies with time by either ejecting or accreting mass is obtained by applying the second law to the entire, constant-mass system consisting of the body and its ejected/accreted mass; the result is
where u is the velocity of the escaping or incoming mass relative to the body. From this equation one can derive the equation of motion for a varying mass system, for example, the Tsiolkovsky rocket equation.
Under some conventions, the quantity u d"m"/d"t" on the left-hand side, which represents the advection of momentum, is defined as a force (the force exerted on the body by the changing mass, such as rocket exhaust) and is included in the quantity F. Then, by substituting the definition of acceleration, the equation becomes F = "m"a.
Newton's third law.
The third law states that all forces between two objects exist in equal magnitude and opposite direction: if one object "A" exerts a force F"A" on a second object "B", then "B" simultaneously exerts a force F"B" on "A", and the two forces are equal and opposite: F"A" = −F"B". The third law means that all forces are "interactions" between different bodies, and thus that there is no such thing as a unidirectional force or a force that acts on only one body. This law is sometimes referred to as the "action-reaction law", with F"A" called the "action" and F"B" the "reaction". The action and the reaction are simultaneous, and it does not matter which is called the "action" and which is called "reaction"; both forces are part of a single interaction, and neither force exists without the other.
The two forces in Newton's third law are of the same type (e.g., if the road exerts a forward frictional force on an accelerating car's tires, then it is also a frictional force that Newton's third law predicts for the tires pushing backward on the road).
From a conceptual standpoint, Newton's third law is seen when a person walks: they push against the floor, and the floor pushes against the person. Similarly, the tires of a car push against the road while the road pushes back on the tires—the tires and road simultaneously push against each other. In swimming, a person interacts with the water, pushing the water backward, while the water simultaneously pushes the person forward—both the person and the water push against each other. The reaction forces account for the motion in these examples. These forces depend on friction; a person or car on ice, for example, may be unable to exert the action force to produce the needed reaction force.
History.
Newton's 1st Law.
From the original Latin of Newton's "Principia":
Translated to English, this reads:
The ancient Greek philosopher Aristotle had the view that all objects have a natural place in the universe: that heavy objects (such as rocks) wanted to be at rest on the Earth and that light objects like smoke wanted to be at rest in the sky and the stars wanted to remain in the heavens. He thought that a body was in its natural state when it was at rest, and for the body to move in a straight line at a constant speed an external agent was needed continually to propel it, otherwise it would stop moving. Galileo Galilei, however, realised that a force is necessary to change the velocity of a body, i.e., acceleration, but no force is needed to maintain its velocity. In other words, Galileo stated that, in the "absence" of a force, a moving object will continue moving. The tendency of objects to resist changes in motion was what Galileo called "inertia". This insight was refined by Newton, who made it into his first law, also known as the "law of inertia"—no force means no acceleration, and hence the body will maintain its velocity. As Newton's first law is a restatement of the law of inertia which Galileo had already described, Newton appropriately gave credit to Galileo.
The law of inertia apparently occurred to several different natural philosophers and scientists independently, including Thomas Hobbes in his "Leviathan". The 17th century philosopher and mathematician René Descartes also formulated the law, although he did not perform any experiments to confirm it.
Newton's 2nd Law.
Newton's original Latin reads:
This was translated quite closely in Motte's 1729 translation as:
According to modern ideas of how Newton was using his terminology, this is understood, in modern terms, as an equivalent of:
This may be expressed by the formula F = p', where p' is the time derivative of the momentum p. This equation can be seen clearly in the Wren Library of Trinity College, Cambridge, in a glass case in which Newton's manuscript is open to the relevant page.
Motte's 1729 translation of Newton's Latin continued with Newton's commentary on the second law of motion, reading:
The sense or senses in which Newton used his terminology, and how he understood the second law and intended it to be understood, have been extensively discussed by historians of science, along with the relations between Newton's formulation and modern formulations.
Newton's 3rd Law.
Translated to English, this reads:
Newton's Scholium (explanatory comment) to this law:
In the above, as usual, "motion" is Newton's name for momentum, hence his careful distinction between motion and velocity.
Newton used the third law to derive the law of conservation of momentum; from a deeper perspective, however, conservation of momentum is the more fundamental idea (derived via Noether's theorem from Galilean invariance), and holds in cases where Newton's third law appears to fail, for instance when force fields as well as particles carry momentum, and in quantum mechanics. 
Importance and range of validity.
Newton's laws were verified by experiment and observation for over 200 years, and they are excellent approximations at the scales and speeds of everyday life. Newton's laws of motion, together with his law of universal gravitation and the mathematical techniques of calculus, provided for the first time a unified quantitative explanation for a wide range of physical phenomena.
These three laws hold to a good approximation for macroscopic objects under everyday conditions. However, Newton's laws (combined with universal gravitation and classical electrodynamics) are inappropriate for use in certain circumstances, most notably at very small scales, very high speeds (in special relativity, the Lorentz factor must be included in the expression for momentum along with rest mass and velocity) or very strong gravitational fields. Therefore, the laws cannot be used to explain phenomena such as conduction of electricity in a semiconductor, optical properties of substances, errors in non-relativistically corrected GPS systems and superconductivity. Explanation of these phenomena requires more sophisticated physical theories, including general relativity and quantum field theory.
In quantum mechanics concepts such as force, momentum, and position are defined by linear operators that operate on the quantum state; at speeds that are much lower than the speed of light, Newton's laws are just as exact for these operators as they are for classical objects. At speeds comparable to the speed of light, the second law holds in the original form F = dp/d"t", where F and p are four-vectors.
Relationship to the conservation laws.
In modern physics, the laws of conservation of momentum, energy, and angular momentum are of more general validity than Newton's laws, since they apply to both light and matter, and to both classical and non-classical physics.
This can be stated simply, "Momentum, energy and angular momentum cannot be created or destroyed."
Because force is the time derivative of momentum, the concept of force is redundant and subordinate to the conservation of momentum, and is not used in fundamental theories (e.g., quantum mechanics, quantum electrodynamics, general relativity, etc.). The standard model explains in detail how the three fundamental forces known as gauge forces originate out of exchange by virtual particles. Other forces such as gravity and fermionic degeneracy pressure also arise from the momentum conservation. Indeed, the conservation of 4-momentum in inertial motion via curved space-time results in what we call gravitational force in general relativity theory. Application of space derivative (which is a momentum operator in quantum mechanics) to overlapping wave functions of pair of fermions (particles with half-integer spin) results in shifts of maxima of compound wavefunction away from each other, which is observable as "repulsion" of fermions.
Newton stated the third law within a world-view that assumed instantaneous action at a distance between material particles. However, he was prepared for philosophical criticism of this action at a distance, and it was in this context that he stated the famous phrase "I feign no hypotheses". In modern physics, action at a distance has been completely eliminated, except for subtle effects involving quantum entanglement. However, in modern engineering in all practical applications involving the motion of vehicles and satellites, the concept of action at a distance is used extensively.
The discovery of the second law of thermodynamics by Carnot in the 19th century showed that every physical quantity is not conserved over time, thus disproving the validity of inducing the opposite metaphysical view from Newton's laws. Hence, a "steady-state" worldview based solely on Newton's laws and the conservation laws does not take entropy into account.

</doc>
<doc id="150159" url="https://en.wikipedia.org/wiki?curid=150159" title="Noether's theorem">
Noether's theorem

Noether's (first) theorem states that every differentiable symmetry of the action of a physical system has a corresponding conservation law. The theorem was proven by German mathematician Emmy Noether in 1915 and published in 1918. The action of a physical system is the integral over time of a Lagrangian function (which may or may not be an integral over space of a Lagrangian density function), from which the system's behavior can be determined by the principle of least action.
Noether's theorem is used in theoretical physics and the calculus of variations. A generalization of the formulations on constants of motion in Lagrangian and Hamiltonian mechanics (developed in 1788 and 1833, respectively), it does not apply to systems that cannot be modeled with a Lagrangian alone (e.g. systems with a Rayleigh dissipation function). In particular, dissipative systems with continuous symmetries need not have a corresponding conservation law.
Basic illustrations and background.
As an illustration, if a physical system behaves the same regardless of how it is oriented in space, its Lagrangian is rotationally symmetric: from this symmetry, Noether's theorem dictates that the angular momentum of the system be conserved, as a consequence of its laws of motion. The physical system itself need not be symmetric; a jagged asteroid tumbling in space conserves angular momentum despite its asymmetry. It is the laws of its motion that are symmetric.
As another example, if a physical process exhibits the same outcomes regardless of place or time, then its Lagrangian is symmetric under continuous translations in space and time: by Noether's theorem, these symmetries account for the conservation laws of linear momentum and energy within this system, respectively.
Noether's theorem is important, both because of the insight it gives into conservation laws, and also as a practical calculational tool. It allows investigators to determine the conserved quantities (invariants) from the observed symmetries of a physical system. Conversely, it allows researchers to consider whole classes of hypothetical Lagrangians with given invariants, to describe a physical system. As an illustration, suppose that a physical theory is proposed which conserves a quantity "X". A researcher can calculate the types of Lagrangians that conserve "X" through a continuous symmetry. Due to Noether's theorem, the properties of these Lagrangians provide further criteria to understand the implications and judge the fitness of the new theory.
There are numerous versions of Noether's theorem, with varying degrees of generality. The original version only applied to ordinary differential equations (particles) and not partial differential equations (fields). The original versions also assume that the Lagrangian only depends upon the first derivative, while later versions generalize the theorem to Lagrangians depending on the "n"th derivative. There are natural quantum counterparts of this theorem, expressed in the Ward–Takahashi identities. Generalizations of Noether's theorem to superspaces are also available.
Informal statement of the theorem.
All fine technical points aside, Noether's theorem can be stated informally
A more sophisticated version of the theorem involving fields states that:
The word "symmetry" in the above statement refers more precisely to the covariance of the form that a physical law takes with respect to a one-dimensional Lie group of transformations satisfying certain technical criteria. The conservation law of a physical quantity is usually expressed as a continuity equation.
The formal proof of the theorem utilizes the condition of invariance to derive an expression for a current associated with a conserved physical quantity. 
In modern (since ca. 1980) terminology, the conserved quantity is called the "Noether charge", while the flow carrying that charge is called the "Noether current". The Noether current is defined up to a solenoidal (divergenceless) vector field.
In the context of gravitation, Felix Klein's statement of Noether's theorem for action "I" stipulates for the invariants: 
Historical context.
A conservation law states that some quantity "X" in the mathematical description of a system's evolution remains constant throughout its motion — it is an invariant. Mathematically, the rate of change of "X" (its derivative with respect to time) vanishes,
Such quantities are said to be conserved; they are often called constants of motion (although motion "per se" need not be involved, just evolution in time). For example, if the energy of a system is conserved, its energy is invariant at all times, which imposes a constraint on the system's motion and may help in solving for it. Aside from insights that such constants of motion give into the nature of a system, they are a useful calculational tool; for example, an approximate solution can be corrected by finding the nearest state that satisfies the suitable conservation laws.
The earliest constants of motion discovered were momentum and energy, which were proposed in the 17th century by René Descartes and Gottfried Leibniz on the basis of collision experiments, and refined by subsequent researchers. Isaac Newton was the first to enunciate the conservation of momentum in its modern form, and showed that it was a consequence of Newton's third law. According to general relativity, the conservation laws of linear momentum, energy and angular momentum are only exactly true globally when expressed in terms of the sum of the stress–energy tensor (non-gravitational stress–energy) and the Landau–Lifshitz stress–energy–momentum pseudotensor (gravitational stress–energy). The local conservation of non-gravitational linear momentum and energy in a free-falling reference frame is expressed by the vanishing of the covariant divergence of the stress–energy tensor. Another important conserved quantity, discovered in studies of the celestial mechanics of astronomical bodies, is the Laplace–Runge–Lenz vector.
In the late 18th and early 19th centuries, physicists developed more systematic methods for discovering invariants. A major advance came in 1788 with the development of Lagrangian mechanics, which is related to the principle of least action. In this approach, the state of the system can be described by any type of generalized coordinates q; the laws of motion need not be expressed in a Cartesian coordinate system, as was customary in Newtonian mechanics. The action is defined as the time integral "I" of a function known as the Lagrangian "L"
where the dot over q signifies the rate of change of the coordinates q,
Hamilton's principle states that the physical path q("t")—the one actually taken by the system—is a path for which infinitesimal variations in that path cause no change in "I", at least up to first order. This principle results in the Euler–Lagrange equations,
Thus, if one of the coordinates, say "qk", does not appear in the Lagrangian, the right-hand side of the equation is zero, and the left-hand side requires that
where the momentum
is conserved throughout the motion (on the physical path).
Thus, the absence of the ignorable coordinate "qk" from the Lagrangian implies that the Lagrangian is unaffected by changes or transformations of "qk"; the Lagrangian is invariant, and is said to exhibit a symmetry under such transformations. This is the seed idea generalized in Noether's theorem.
Several alternative methods for finding conserved quantities were developed in the 19th century, especially by William Rowan Hamilton. For example, he developed a theory of canonical transformations which allowed changing coordinates so that some coordinates disappeared from the Lagrangian, as above, resulting in conserved canonical momenta. Another approach, and perhaps the most efficient for finding conserved quantities, is the Hamilton–Jacobi equation.
Mathematical expression.
Simple form using perturbations.
The essence of Noether's theorem is generalizing the ignorable coordinates outlined.
Imagine that the action "I" defined above is invariant under small perturbations (warpings) of the time variable "t" and the generalized coordinates q; in a notation commonly used in physics,
where the perturbations "δt" and "δ"q are both small, but variable. For generality, assume there are (say) "N" such symmetry transformations of the action, i.e. transformations leaving the action unchanged; labelled by an index "r" = 1, 2, 3, …, "N".
Then the resultant perturbation can be written as a linear sum of the individual types of perturbations,
where ε"r" are infinitesimal parameter coefficients corresponding to each: 
For translations, Q"r" is a constant with units of length; for rotations, it is an expression linear in the components of q, and the parameters make up an angle.
Using these definitions, Noether showed that the "N" quantities
(which have the dimensions of [energy]·[time] + [momentum]·[length] = [action]) are conserved (constants of motion).
Examples.
For illustration, consider a Lagrangian that does not depend on time, i.e., that is invariant (symmetric) under changes "t" → "t" + δ"t", without any change in the coordinates q. In this case, "N" = 1, "T" = 1 and Q = 0; the corresponding conserved quantity is the total energy "H"
Consider a Lagrangian which does not depend on an ("ignorable", as above) coordinate "q""k"; so it is invariant (symmetric) under changes "q""k" → "q""k" + "δq""k". In that case, "N" = 1, "T" = 0, and "Q""k" = 1; the conserved quantity is the corresponding momentum "p""k"
In special and general relativity, these apparently separate conservation laws are aspects of a single conservation law, that of the stress–energy tensor, that is derived in the next section.
The conservation of the angular momentum L = r × p is analogous to its linear momentum counterpart. It is assumed that the symmetry of the Lagrangian is rotational, i.e., that the Lagrangian does not depend on the absolute orientation of the physical system in space. For concreteness, assume that the Lagrangian does not change under small rotations of an angle δθ about an axis n; such a rotation transforms the Cartesian coordinates by the equation
Since time is not being transformed, "T"=0. Taking "δθ" as the "ε" parameter and the Cartesian coordinates r as the generalized coordinates q, the corresponding Q variables are given by
Then Noether's theorem states that the following quantity is conserved,
In other words, the component of the angular momentum L along the n axis is conserved.
If n is arbitrary, i.e., if the system is insensitive to any rotation, then every component of L is conserved; in short, angular momentum is conserved.
Field theory version.
Although useful in its own right, the version of Noether's theorem just given is a special case of the general version derived in 1915. To give the flavor of the general theorem, a version of the Noether theorem for continuous fields in four-dimensional space–time is now given. Since field theory problems are more common in modern physics than mechanics problems, this field theory version is the most commonly used version (or most often implemented) of Noether's theorem.
Let there be a set of differentiable fields "φ" defined over all space and time; for example, the temperature "T"(x, "t") would be representative of such a field, being a number defined at every place and time. The principle of least action can be applied to such fields, but the action is now an integral over space and time
Let the action be invariant under certain transformations of the space–time coordinates "x"μ and the fields "φ"
where the transformations can be indexed by "r" = 1, 2, 3, …, "N"
For such systems, Noether's theorem states that there are "N" conserved current densities
In such cases, the conservation law is expressed in a four-dimensional way
which expresses the idea that the amount of a conserved quantity within a sphere cannot change unless some of it flows out of the sphere. For example, electric charge is conserved; the amount of charge within a sphere cannot change unless some of the charge leaves the sphere.
For illustration, consider a physical system of fields that behaves the same under translations in time and space, as considered above; in other words, formula_24 is constant in its third argument. In that case, "N" = 4, one for each dimension of space and time. Since only the positions in space–time are being warped, not the fields, the Ψ are all zero and the "X"μν equal the Kronecker delta δμν, where we have used μ instead of "r" for the index. In that case, Noether's theorem corresponds to the conservation law for the stress–energy tensor "T"μν
The conservation of electric charge, by contrast, can be derived by considering zero "X"μν=0 and "Ψ" linear in the fields "φ" themselves. In quantum mechanics, the probability amplitude ψ(x) of finding a particle at a point x is a complex field "φ", because it ascribes a complex number to every point in space and time. The probability amplitude itself is physically unmeasurable; only the probability "p" = |ψ|2 can be inferred from a set of measurements. Therefore, the system is invariant under transformations of the ψ field and its complex conjugate field ψ* that leave |ψ|2 unchanged, such as
a complex rotation. In the limit when the phase "θ" becomes infinitesimally small, "δθ", it may be taken as the parameter "ε", while the "Ψ" are equal to "iψ" and −"iψ"*, respectively. A specific example is the Klein–Gordon equation, the relativistically correct version of the Schrödinger equation for spinless particles, which has the Lagrangian density
In this case, Noether's theorem states that the conserved (∂⋅"j" = 0) current equals
which, when multiplied by the charge on that species of particle, equals the electric current density due to that type of particle. This "gauge invariance" was first noted by Hermann Weyl, and is one of the prototype gauge symmetries of physics.
Derivations.
One independent variable.
Consider the simplest case, a system with one independent variable, time. Suppose the dependent variables q are such that the action integral
is invariant under brief infinitesimal variations in the dependent variables. In other words, they satisfy the Euler–Lagrange equations
And suppose that the integral is invariant under a continuous symmetry. Mathematically such a symmetry is represented as a flow, φ, which acts on the variables as follows
where ε is a real variable indicating the amount of flow, and "T" is a real constant (which could be zero) indicating how much the flow shifts time.
The action integral flows to
which may be regarded as a function of ε. Calculating the derivative at ε = 0 and using the symmetry, we get
Notice that the Euler–Lagrange equations imply
Substituting this into the previous equation, one gets
Again using the Euler–Lagrange equations we get
Substituting this into the previous equation, one gets
From which one can see that
is a constant of the motion, i.e., it is a conserved quantity. Since φ[q, 0] = q, we get formula_41 and so the conserved quantity simplifies to
To avoid excessive complication of the formulas, this derivation assumed that the flow does not change as time passes. The same result can be obtained in the more general case.
Field-theoretic derivation.
Noether's theorem may also be derived for tensor fields "φ""A" where the index "A" ranges over the various components of the various tensor fields. These field quantities are functions defined over a four-dimensional space whose points are labeled by coordinates "x"μ where the index "μ" ranges over time ("μ" = 0) and three spatial dimensions ("μ" = 1, 2, 3). These four coordinates are the independent variables; and the values of the fields at each event are the dependent variables. Under an infinitesimal transformation, the variation in the coordinates is written
whereas the transformation of the field variables is expressed as
By this definition, the field variations δφ"A" result from two factors: intrinsic changes in the field themselves and changes in coordinates, since the transformed field "α""A" depends on the transformed coordinates ξμ. To isolate the intrinsic changes, the field variation at a single point "x"μ may be defined
If the coordinates are changed, the boundary of the region of space–time over which the Lagrangian is being integrated also changes; the original boundary and its transformed version are denoted as Ω and Ω’, respectively.
Noether's theorem begins with the assumption that a specific transformation of the coordinates and field variables does not change the action, which is defined as the integral of the Lagrangian density over the given region of spacetime. Expressed mathematically, this assumption may be written as
where the comma subscript indicates a partial derivative with respect to the coordinate(s) that follows the comma, e.g.
Since ξ is a dummy variable of integration, and since the change in the boundary Ω is infinitesimal by assumption, the two integrals may be combined using the four-dimensional version of the divergence theorem into the following form
The difference in Lagrangians can be written to first-order in the infinitesimal variations as
However, because the variations are defined at the same point as described above, the variation and the derivative can be done in reverse order; they commute
Using the Euler–Lagrange field equations
the difference in Lagrangians can be written neatly as
Thus, the change in the action can be written as
Since this holds for any region Ω, the integrand must be zero
For any combination of the various symmetry transformations, the perturbation can be written
where formula_57 is the Lie derivative of φ"A" in the "X""μ" direction. When "φ""A" is a scalar or formula_58,
These equations imply that the field variation taken at one point equals
Differentiating the above divergence with respect to ε at ε=0 and changing the sign yields the conservation law
where the conserved current equals
Manifold/fiber bundle derivation.
Suppose we have an "n"-dimensional oriented Riemannian manifold, "M" and a target manifold "T". Let formula_63 be the configuration space of smooth functions from "M" to "T". (More generally, we can have smooth sections of a fiber bundle over "M".)
Examples of this "M" in physics include:
Now suppose there is a functional
called the action. (Note that it takes values into R, rather than C; this is for physical reasons, and doesn't really matter for this proof.)
To get to the usual version of Noether's theorem, we need additional restrictions on the action. We assume formula_66 is the integral over "M" of a function
called the Lagrangian density, depending on φ, its derivative and the position. In other words, for φ in formula_63
Suppose we are given boundary conditions, i.e., a specification of the value of φ at the boundary if "M" is compact, or some limit on φ as "x" approaches ∞. Then the subspace of formula_63 consisting of functions φ such that all functional derivatives of formula_71 at φ are zero, that is:
and that φ satisfies the given boundary conditions, is the subspace of on shell solutions. (See principle of stationary action)
Now, suppose we have an infinitesimal transformation on formula_63, generated by a functional derivation, "Q" such that
for all compact submanifolds "N" or in other words,
for all "x", where we set
If this holds on shell and off shell, we say "Q" generates an off-shell symmetry. If this only holds on shell, we say "Q" generates an on-shell symmetry. Then, we say "Q" is a generator of a one parameter symmetry Lie group.
Now, for any "N", because of the Euler–Lagrange theorem, on shell (and only on-shell), we have
Since this is true for any "N", we have
But this is the continuity equation for the current formula_78 defined by:
which is called the Noether current associated with the symmetry. The continuity equation tells us that if we integrate this current over a space-like slice, we get a conserved quantity called the Noether charge (provided, of course, if "M" is noncompact, the currents fall off sufficiently fast at infinity).
Comments.
Noether's theorem is an on shell theorem: it relies on use of the equations of motion—the classical path. It reflects the relation between the boundary conditions and the variational principle. Assuming no boundary terms in the action, Noether's theorem implies that
The quantum analogs of Noether's theorem involving expectation values, e.g. ⟨∫"d"4"x" ∂·"J"⟩ = 0, probing off shell quantities as well are the Ward–Takahashi identities.
Generalization to Lie algebras.
Suppose say we have two symmetry derivations "Q"1 and "Q"2. Then, ["Q"1, "Q"2] is also a symmetry derivation. Let's see this explicitly. Let's say
and
Then,
where "f"12 = "Q"1["f"2"μ"] − "Q"2["f"1"μ"]. So,
This shows we can extend Noether's theorem to larger Lie algebras in a natural way.
Generalization of the proof.
This applies to "any" local symmetry derivation "Q" satisfying "QS" ≈ 0, and also to more general local functional differentiable actions, including ones where the Lagrangian depends on higher derivatives of the fields. Let "ε" be any arbitrary smooth function of the spacetime (or time) manifold such that the closure of its support is disjoint from the boundary. "ε" is a test function. Then, because of the variational principle (which does "not" apply to the boundary, by the way), the derivation distribution q generated by "q"["ε"][Φ("x")] = "ε"("x")"Q"[Φ("x")] satisfies "q"["ε"]["S"] ≈ 0 for every "ε", or more compactly, "q"("x")["S"] ≈ 0 for all "x" not on the boundary (but remember that "q"("x") is a shorthand for a derivation "distribution", not a derivation parametrized by "x" in general). This is the generalization of Noether's theorem.
To see how the generalization is related to the version given above, assume that the action is the spacetime integral of a Lagrangian that only depends on φ and its first derivatives. Also, assume
Then,
for all "ε".
More generally, if the Lagrangian depends on higher derivatives, then
Examples.
Example 1: Conservation of energy.
Looking at the specific case of a Newtonian particle of mass "m", coordinate "x", moving under the influence of a potential "V", coordinatized by time "t". The action, "S", is:
The first term in the brackets is the kinetic energy of the particle, whilst the second is its potential energy. Consider the generator of time translations "Q" = ∂/∂"t". In other words, formula_89. Note that "x" has an explicit dependence on time, whilst "V" does not; consequently:
so we can set
Then,
The right hand side is the energy, and Noether's theorem states that formula_93 (i.e. the principle of conservation of energy is a consequence of invariance under time translations).
More generally, if the Lagrangian does not depend explicitly on time, the quantity
(called the Hamiltonian) is conserved.
Example 2: Conservation of center of momentum.
Still considering 1-dimensional time, let
i.e. "N" Newtonian particles where the potential only depends pairwise upon the relative displacement.
For formula_96, let's consider the generator of Galilean transformations (i.e. a change in the frame of reference). In other words,
Note that
This has the form of formula_99 so we can set
Then,
where formula_104 is the total momentum, "M" is the total mass and formula_105 is the center of mass. Noether's theorem states:
Example 3: Conformal transformation.
Both examples 1 and 2 are over a 1-dimensional manifold (time). An example involving spacetime is a conformal transformation of a massless real scalar field with a quartic potential in (3 + 1)-Minkowski spacetime.
For "Q", consider the generator of a spacetime rescaling. In other words,
The second term on the right hand side is due to the "conformal weight" of φ. Note that
This has the form of
(where we have performed a change of dummy indices) so set
Then,
Noether's theorem states that formula_113 (as one may explicitly check by substituting the Euler–Lagrange equations into the left hand side).
Note that if one tries to find the Ward–Takahashi analog of this equation, one runs into a problem because of anomalies.
Applications.
Application of Noether's theorem allows physicists to gain powerful insights into any general theory in physics, by just analyzing the various transformations that would make the form of the laws involved invariant. For example:
In quantum field theory, the analog to Noether's theorem, the Ward–Takahashi identity, yields further conservation laws, such as the conservation of electric charge from the invariance with respect to a change in the phase factor of the complex field of the charged particle and the associated gauge of the electric potential and vector potential.
The Noether charge is also used in calculating the entropy of stationary black holes.

</doc>
<doc id="4791400" url="https://en.wikipedia.org/wiki?curid=4791400" title="Non-abelian group">
Non-abelian group

In mathematics, a nonabelian group, sometimes called a noncommutative group, is a group ( "G", ∗ ) in which there exists at least one pair of elements "a" and "b" of "G", such that "a" ∗ "b" ≠ "b" ∗ "a". This class of groups contrasts with the abelian groups. (In an abelian group, all pairs of group elements commute).
Nonabelian groups are pervasive in mathematics and physics. One of the simplest examples of a nonabelian group is the dihedral group of order 6. It is the smallest finite nonabelian group. A common example from physics is the rotation group SO(3) in three dimensions (rotating something 90 degrees away from you and then 90 degrees to the left is not the same as doing them the other way round).
Both discrete groups and continuous groups may be nonabelian. Most of the interesting Lie groups are nonabelian, and these play an important role in gauge theory.

</doc>
<doc id="58610" url="https://en.wikipedia.org/wiki?curid=58610" title="Non-Euclidean geometry">
Non-Euclidean geometry

In mathematics, non-Euclidean geometry consists of two geometries based on axioms closely related to those specifying Euclidean geometry. As Euclidean geometry lies at the intersection of metric geometry and affine geometry, non-Euclidean geometry arises when either the metric requirement is relaxed, or the parallel postulate is replaced with an alternative one. In the latter case one obtains hyperbolic geometry and elliptic geometry, the traditional non-Euclidean geometries. When the metric requirement is relaxed, then there are affine planes associated with the planar algebras which give rise to kinematic geometries that have also been called non-Euclidean geometry.
The essential difference between the metric geometries is the nature of parallel lines. Euclid's fifth postulate, the parallel postulate, is equivalent to Playfair's postulate, which states that, within a two-dimensional plane, for any given line "ℓ" and a point "A", which is not on "ℓ", there is exactly one line through "A" that does not intersect "ℓ". In hyperbolic geometry, by contrast, there are infinitely many lines through "A" not intersecting "ℓ", while in elliptic geometry, any line through "A" intersects "ℓ".
Another way to describe the differences between these geometries is to consider two straight lines indefinitely extended in a two-dimensional plane that are both perpendicular to a third line:
History.
Early history.
While Euclidean geometry, named after the Greek mathematician Euclid, includes some of the oldest known mathematics, non-Euclidean geometries were not widely accepted as legitimate until the 19th century.
The debate that eventually led to the discovery of the non-Euclidean geometries began almost as soon as Euclid's work "Elements" was written. In the "Elements", Euclid began with a limited number of assumptions (23 definitions, five common notions, and five postulates) and sought to prove all the other results (propositions) in the work. The most notorious of the postulates is often referred to as "Euclid's Fifth Postulate," or simply the "parallel postulate", which in Euclid's original formulation is:
If a straight line falls on two straight lines in such a manner that the interior angles on the same side are together less than two right angles, then the straight lines, if produced indefinitely, meet on that side on which are the angles less than the two right angles.
Other mathematicians have devised simpler forms of this property. Regardless of the form of the postulate, however, it consistently appears to be more complicated than Euclid's other postulates:
1. To draw a straight line from any point to any point.
2. To produce [extend] a finite straight line continuously in a straight line.
3. To describe a circle with any centre and distance [radius].
4. That all right angles are equal to one another.
For at least a thousand years, geometers were troubled by the disparate complexity of the fifth postulate, and believed it could be proved as a theorem from the other four. Many attempted to find a proof by contradiction, including Ibn al-Haytham (Alhazen, 11th century), Omar Khayyám (12th century), Nasīr al-Dīn al-Tūsī (13th century), and Giovanni Girolamo Saccheri (18th century).
The theorems of Ibn al-Haytham, Khayyam and al-Tusi on quadrilaterals, including the Lambert quadrilateral and Saccheri quadrilateral, were "the first few theorems of the hyperbolic and the elliptic geometries." These theorems along with their alternative postulates, such as Playfair's axiom, played an important role in the later development of non-Euclidean geometry. These early attempts at challenging the fifth postulate had a considerable influence on its development among later European geometers, including Witelo, Levi ben Gerson, Alfonso, John Wallis and Saccheri. All of these early attempts made at trying to formulate non-Euclidean geometry however provided flawed proofs of the parallel postulate, containing assumptions that were essentially equivalent to the parallel postulate. These early attempts did, however, provide some early properties of the hyperbolic and elliptic geometries.
Khayyam, for example, tried to derive it from an equivalent postulate he formulated from "the principles of the Philosopher" (Aristotle): "Two convergent straight lines intersect and it is impossible for two convergent straight lines to diverge in the direction in which they converge." Khayyam then considered the three cases right, obtuse, and acute that the summit angles of a Saccheri quadrilateral can take and after proving a number of theorems about them, he correctly refuted the obtuse and acute cases based on his postulate and hence derived the classic postulate of Euclid which he didn't realize was equivalent to his own postulate. Another example is al-Tusi's son, Sadr al-Din (sometimes known as "Pseudo-Tusi"), who wrote a book on the subject in 1298, based on al-Tusi's later thoughts, which presented another hypothesis equivalent to the parallel postulate. "He essentially revised both the Euclidean system of axioms and postulates and the proofs of many propositions from the "Elements"." His work was published in Rome in 1594 and was studied by European geometers, including Saccheri who criticised this work as well as that of Wallis.
Giordano Vitale, in his book "Euclide restituo" (1680, 1686), used the Saccheri quadrilateral to prove that if three points are equidistant on the base AB and the summit CD, then AB and CD are everywhere equidistant.
In a work titled "Euclides ab Omni Naevo Vindicatus" ("Euclid Freed from All Flaws"), published in 1733, Saccheri quickly discarded elliptic geometry as a possibility (some others of Euclid's axioms must be modified for elliptic geometry to work) and set to work proving a great number of results in hyperbolic geometry.
He finally reached a point where he believed that his results demonstrated the impossibility of hyperbolic geometry. His claim seems to have been based on Euclidean presuppositions, because no "logical" contradiction was present. In this attempt to prove Euclidean geometry he instead unintentionally discovered a new viable geometry, but did not realize it.
In 1766 Johann Lambert wrote, but did not publish, "Theorie der Parallellinien" in which he attempted, as Saccheri did, to prove the fifth postulate. He worked with a figure that today we call a "Lambert quadrilateral", a quadrilateral with three right angles (can be considered half of a Saccheri quadrilateral). He quickly eliminated the possibility that the fourth angle is obtuse, as had Saccheri and Khayyam, and then proceeded to prove many theorems under the assumption of an acute angle. Unlike Saccheri, he never felt that he had reached a contradiction with this assumption. He had proved the non-Euclidean result that the sum of the angles in a triangle increases as the area of the triangle decreases, and this led him to speculate on the possibility of a model of the acute case on a sphere of imaginary radius. He did not carry this idea any further.
At this time it was widely believed that the universe worked according to the principles of Euclidean geometry.
Discovery of non-Euclidean geometry.
The beginning of the 19th century would finally witness decisive steps in the creation of non-Euclidean geometry.
Circa 1813, Carl Friedrich Gauss and independently around 1818, the German professor of law Ferdinand Karl Schweikart had the germinal ideas of non-Euclidean geometry worked out, but neither published any results. Then, around 1830, the Hungarian mathematician János Bolyai and the Russian mathematician Nikolai Ivanovich Lobachevsky separately published treatises on hyperbolic geometry. Consequently, hyperbolic geometry is called Bolyai-Lobachevskian geometry, as both mathematicians, independent of each other, are the basic authors of non-Euclidean geometry. Gauss mentioned to Bolyai's father, when shown the younger Bolyai's work, that he had developed such a geometry several years before, though he did not publish. While Lobachevsky created a non-Euclidean geometry by negating the parallel postulate, Bolyai worked out a geometry where both the Euclidean and the hyperbolic geometry are possible depending on a parameter "k". Bolyai ends his work by mentioning that it is not possible to decide through mathematical reasoning alone if the geometry of the physical universe is Euclidean or non-Euclidean; this is a task for the physical sciences.
Bernhard Riemann, in a famous lecture in 1854, founded the field of Riemannian geometry, discussing in particular the ideas now called manifolds, Riemannian metric, and curvature.
He constructed an infinite family of geometries which are not Euclidean by giving a formula for a family of Riemannian metrics on the unit ball in Euclidean space. The simplest of these is called elliptic geometry and it is considered to be a non-Euclidean geometry due to its lack of parallel lines.
By formulating the geometry in terms of a curvature tensor, Riemann allowed non-Euclidean geometry to be applied to higher dimensions.
Terminology.
It was Gauss who coined the term "non-Euclidean geometry". He was referring to his own work which today we call "hyperbolic geometry". Several modern authors still consider "non-Euclidean geometry" and "hyperbolic geometry" to be synonyms.
Arthur Cayley noted that distance between points inside a conic could be defined in terms of logarithm and the projective cross-ratio function. The method has become called the Cayley-Klein metric because Felix Klein exploited it to describe the non-euclidean geometries in articles in 1871 and 73 and later in book form. The Cayley-Klein metrics provided working models of hyperbolic and elliptic metric geometries, as well as Euclidean geometry.
Klein is responsible for the terms "hyperbolic" and "elliptic" (in his system he called Euclidean geometry "parabolic", a term which generally fell out of use). His influence has led to the current usage of the term "non-Euclidean geometry" to mean either "hyperbolic" or "elliptic" geometry.
There are some mathematicians who would extend the list of geometries that should be called "non-Euclidean" in various ways.
Axiomatic basis of non-Euclidean geometry.
Euclidean geometry can be axiomatically described in several ways. Unfortunately, Euclid's original system of five postulates (axioms) is not one of these as his proofs relied on several unstated assumptions which should also have been taken as axioms. Hilbert's system consisting of 20 axioms most closely follows the approach of Euclid and provides the justification for all of Euclid's proofs. Other systems, using different sets of undefined terms obtain the same geometry by different paths. In all approaches, however, there is an axiom which is logically equivalent to Euclid's fifth postulate, the parallel postulate. Hilbert uses the Playfair axiom form, while Birkhoff, for instance, uses the axiom which says that "there exists a pair of similar but not congruent triangles." In any of these systems, removal of the one axiom which is equivalent to the parallel postulate, in whatever form it takes, and leaving all the other axioms intact, produces absolute geometry. As the first 28 propositions of Euclid (in "The Elements") do not require the use of the parallel postulate or anything equivalent to it, they are all true statements in absolute geometry.
To obtain a non-Euclidean geometry, the parallel postulate (or its equivalent) "must" be replaced by its negation. Negating the Playfair's axiom form, since it is a compound statement (... there exists one and only one ...), can be done in two ways:
Models of non-Euclidean geometry.
Two dimensional Euclidean geometry is modelled by our notion of a "flat plane."
Elliptic geometry.
The simplest model for elliptic geometry is a sphere, where lines are "great circles" (such as the equator or the meridians on a globe), and points opposite each other (called antipodal points) are identified (considered to be the same). This is also one of the standard models of the real projective plane. The difference is that as a model of elliptic geometry a metric is introduced permitting the measurement of lengths and angles, while as a model of the projective plane there is no such metric.
In the elliptic model, for any given line "ℓ" and a point "A", which is not on "ℓ", all lines through "A" will intersect "ℓ".
Hyperbolic geometry.
Even after the work of Lobachevsky, Gauss, and Bolyai, the question remained: "Does such a model exist for hyperbolic geometry?". The model for hyperbolic geometry was answered by Eugenio Beltrami, in 1868, who first showed that a surface called the pseudosphere has the appropriate curvature to model a portion of hyperbolic space and in a second paper in the same year, defined the Klein model which models the entirety of hyperbolic space, and used this to show that Euclidean geometry and hyperbolic geometry were equiconsistent so that hyperbolic geometry was logically consistent if and only if Euclidean geometry was. (The reverse implication follows from the horosphere model of Euclidean geometry.)
In the hyperbolic model, within a two-dimensional plane, for any given line "ℓ" and a point "A", which is not on "ℓ", there are infinitely many lines through "A" that do not intersect "ℓ".
In these models the concepts of non-Euclidean geometries are being represented by Euclidean objects in a Euclidean setting. This introduces a perceptual distortion wherein the straight lines of the non-Euclidean geometry are being represented by Euclidean curves which visually bend. This "bending" is not a property of the non-Euclidean lines, only an artifice of the way they are being represented.
Three-dimensional non-Euclidean geometry.
In three dimensions, there are eight models of geometries. There are Euclidean, elliptic, and hyperbolic geometries, as in the two-dimensional case; mixed geometries that are partially Euclidean and partially hyperbolic or spherical; twisted versions of the mixed geometries; and one unusual geometry that is completely anisotropic (i.e. every direction behaves differently).
Uncommon properties.
Euclidean and non-Euclidean geometries naturally have many similar properties, namely those which do not depend upon the nature of parallelism. This commonality is the subject of absolute geometry (also called "neutral geometry"). However, the properties which distinguish one geometry from the others are the ones which have historically received the most attention.
Besides the behavior of lines with respect to a common perpendicular, mentioned in the introduction, we also have the following:
Importance.
Before the models of a non-Euclidean plane were presented by Beltrami, Klein, and Poincaré, Euclidean geometry stood unchallenged as the mathematical model of space. Furthermore, since the substance of the subject in synthetic geometry was a chief exhibit of rationality, the Euclidean point of view represented absolute authority.
The discovery of the non-Euclidean geometries had a ripple effect which went far beyond the boundaries of mathematics and science. The philosopher Immanuel Kant's treatment of human knowledge had a special role for geometry. It was his prime example of synthetic a priori knowledge; not derived from the senses nor deduced through logic — our knowledge of space was a truth that we were born with. Unfortunately for Kant, his concept of this unalterably true geometry was Euclidean. Theology was also affected by the change from absolute truth to relative truth in the way that mathematics is related to the world around it, that was a result of this paradigm shift.
Non-Euclidean geometry is an example of a scientific revolution in the history of science, mathematicians and scientists changed the way they viewed their subjects., Some geometers called Lobachevsky the "Copernicus of Geometry" due to the revolutionary character of his work.
The existence of non-Euclidean geometries impacted the intellectual life of Victorian England in many ways and in particular was one of the leading factors that caused a re-examination of the teaching of geometry based on Euclid's Elements. This curriculum issue was hotly debated at the time and was even the subject of a book, "Euclid and his Modern Rivals", written by Charles Lutwidge Dodgson (1832–1898) better known as Lewis Carroll, the author of Alice in Wonderland.
Planar algebras.
In analytic geometry a plane is described with Cartesian coordinates : }. The points are sometimes identified with complex numbers where }.
The Euclidean plane corresponds to the case since the modulus of is given by
and this quantity is the square of the Euclidean distance between and the origin.
For instance, } is the unit circle.
For planar algebra, non-Euclidean geometry arises in the other cases.
When , then is a split-complex number and conventionally replaces epsilon. Then
and } is the unit hyperbola.
When , then is a dual number.
This approach to non-Euclidean geometry explains the non-Euclidean angles: the parameters of slope in the dual number plane and hyperbolic angle in the split-complex plane correspond to angle in Euclidean geometry. Indeed, they each arise in polar decomposition of a complex number .
Kinematic geometries.
Hyperbolic geometry found an application in kinematics with the cosmology introduced by Hermann Minkowski in 1908. Minkowski introduced terms like worldline and proper time into mathematical physics. He realized that the submanifold, of events one moment of proper time into the future, could be considered a hyperbolic space of three dimensions.
Already in the 1890s Alexander Macfarlane was charting this submanifold through his Algebra of Physics and hyperbolic quaternions, though Macfarlane did not use cosmological language as Minkowski did in 1908. The relevant structure is now called the hyperboloid model of hyperbolic geometry.
The non-Euclidean planar algebras support kinematic geometries in the plane. For instance, the split-complex number "z" = e"a"j can represent a spacetime event one moment into the future of a frame of reference of rapidity a. Furthermore, multiplication by "z" amounts to a Lorentz boost mapping the frame with rapidity zero to that with rapidity "a".
Kinematic study makes use of the dual numbers formula_3 to represent the classical description of motion in absolute time and space:
The equations formula_4 are equivalent to a shear mapping in linear algebra:
With dual numbers the mapping is formula_6
Another view of special relativity as a non-Euclidean geometry was advanced by E. B. Wilson and Gilbert Lewis in "Proceedings of the American Academy of Arts and Sciences" in 1912. They revamped the analytic geometry implicit in the split-complex number algebra into synthetic geometry of premises and deductions.
Fiction.
Non-Euclidean geometry often makes appearances in works of science fiction and fantasy.
Professor James Moriarty, a character in stories written by Sir Arthur Conan Doyle, is a criminal mastermind with a PhD in non-Euclidean geometries.
In 1895 H. G. Wells published the short story "The Remarkable Case of Davidson’s Eyes". To appreciate this story one should know how antipodal points on a sphere are identified in a model of the elliptic plane. In the story, in the midst of a thunderstorm, Sidney Davidson sees "Waves and a remarkably neat schooner" while working in an electrical laboratory at Harlow Technical College. At the story’s close Davidson proves to have witnessed H.M.S. "Fulmar" off Antipodes Island.
Non-Euclidean geometry is sometimes connected with the influence of the 20th century horror fiction writer H. P. Lovecraft. In his works, many unnatural things follow their own unique laws of geometry: In Lovecraft's Cthulhu Mythos, the sunken city of R'lyeh is characterized by its non-Euclidean geometry. It is heavily implied this is achieved as a side effect of not following the natural laws of this universe rather than simply using an alternate geometric model, as the sheer innate wrongness of it is said to be capable of driving those who look upon it insane.
The main character in Robert Pirsig's "Zen and the Art of Motorcycle Maintenance" mentioned Riemannian Geometry on multiple occasions.
In "The Brothers Karamazov", Dostoevsky discusses non-Euclidean geometry through his main character Ivan.
Christopher Priest's novel "Inverted World" describes the struggle of living on a planet with the form of a rotating pseudosphere.
Robert Heinlein's "The Number of the Beast" utilizes non-Euclidean geometry to explain instantaneous transport through space and time and between parallel and fictional universes.
Alexander Bruce's "Antichamber" uses non-Euclidean geometry to create a minimal, Escher-like world, where geometry and space follow unfamiliar rules.
In the Renegade Legion science fiction setting for FASA's wargame, role-playing-game and fiction, faster-than-light travel and communications is possible through the use of Hsieh Ho's Polydimensional Non-Euclidean Geometry, published sometime in the middle of the 22nd century.
In Ian Stewart's "Flatterland" the protagonist Victoria Line visit all kinds of non-Euclidean worlds.
In Jean-Pierre Petit's "Here's looking at Euclid (and not looking at Euclid)" Archibald Higgins stumbles upon spherical geometry

</doc>
<doc id="2434557" url="https://en.wikipedia.org/wiki?curid=2434557" title="Non-inertial reference frame">
Non-inertial reference frame

A non-inertial reference frame is a frame of reference that is undergoing acceleration with respect to an inertial frame. An accelerometer at rest in a non-inertial frame will in general detect a non-zero acceleration. In a curved spacetime all frames are non-inertial. The laws of motion in non-inertial frames do not take the simple form they do in inertial frames, and the laws vary from frame to frame depending on the acceleration. To explain the motion of bodies entirely within the viewpoint of non-inertial reference frames, fictitious forces (also called inertial forces, pseudo-forces and d'Alembert forces) must be introduced to account for the observed motion, such as the Coriolis force or the centrifugal force, as derived from the acceleration of the non-inertial frame.
As stated by Goodman and Warner, "One might say that F "m"a holds in any coordinate system provided the term 'force' is redefined to include the so-called 'reversed effective forces' or 'inertia forces'."
Avoiding fictitious forces in calculations.
In flat spacetime, the use of non-inertial frames can be avoided if desired. Measurements with respect to non-inertial reference frames can always be transformed to an inertial frame, incorporating directly the acceleration of the non-inertial frame as that acceleration as seen from the inertial frame. This approach avoids use of fictitious forces (it is based on an inertial frame, where fictitious forces are absent, by definition) but it may be less convenient from an intuitive, observational, and even a calculational viewpoint. As pointed out by Ryder for the case of rotating frames as used in meteorology:
Detection of a non-inertial frame: need for fictitious forces.
That a given frame is non-inertial can be detected by its need for fictitious forces to explain observed motions. For example, the rotation of the Earth can be observed using a Foucault pendulum. The rotation of the Earth seemingly causes the pendulum to change its plane of oscillation because the surroundings of the pendulum move with the Earth. As seen from an Earth-bound (non-inertial) frame of reference, the explanation of this apparent change in orientation requires the introduction of the fictitious Coriolis force.
Another famous example is that of the tension in the string between two spheres rotating about each other. In that case, prediction of the measured tension in the string based upon the motion of the spheres as observed from a rotating reference frame requires the rotating observers to introduce a fictitious centrifugal force.
In this connection, it may be noted that a change in coordinate system, for example, from Cartesian to polar, if implemented without any change in relative motion, does not cause the appearance of fictitious forces, despite the fact that the form of the laws of motion varies from one type of curvilinear coordinate system to another.
Fictitious forces in curvilinear coordinates.
A different use of the term "fictitious force" often is used in curvilinear coordinates, particularly polar coordinates. To avoid confusion, this distracting ambiguity in terminologies is pointed out here. These so-called "forces" are non-zero in all frames of reference, inertial or non-inertial, and do "not" transform as vectors under rotations and translations of the coordinates (as all Newtonian forces do, fictitious or otherwise).
This incompatible use of the term "fictitious force" is unrelated to non-inertial frames. These so-called "forces" are defined by determining the acceleration of a particle within the curvilinear coordinate system, and then separating the simple double-time derivatives of coordinates from the remaining terms. These remaining terms then are called "fictitious forces". More careful usage calls these terms "generalized fictitious forces" to indicate their connection to the generalized coordinates of Lagrangian mechanics. The application of Lagrangian methods to polar coordinates can be found here.
Relativistic point of view.
Frames and flat spacetime.
If a region of spacetime is declared to be Euclidean, and effectively free from obvious gravitational fields, then if an accelerated coordinate system is overlaid onto the same region, it can be said that a "uniform fictitious field" exists in the accelerated frame (we reserve the word gravitational for the case in which a mass is involved). An object accelerated to be stationary in the accelerated frame will "feel" the presence of the field, and they will also be able to see environmental matter with inertial states of motion (stars, galaxies, etc.) to be apparently falling "downwards" in the field along curved trajectories as if the field is real.
In frame-based descriptions, this supposed field can be made to appear or disappear by switching between "accelerated" and "inertial" coordinate systems.
More advanced descriptions.
As the situation is modeled in finer detail, using the general principle of relativity, the concept of a "frame-dependent" gravitational field becomes less realistic. In these Machian models, the accelerated body can agree that the apparent gravitational field is associated with the motion of the background matter, but can also claim that the motion of the material as if there is a gravitational field, causes the gravitational field - the accelerating background matter "drags light". Similarly, a background observer can argue that the forced acceleration of the mass causes an apparent gravitational field in the region between it and the environmental material (the accelerated mass also "drags light").
This "mutual" effect, and the ability of an accelerated mass to warp lightbeam geometry and lightbeam-based coordinate systems, is referred to as frame-dragging.
Frame-dragging removes the usual distinction between accelerated frames (which show gravitational effects) and inertial frames (where the geometry is supposedly free from gravitational fields). When a forcibly-accelerated body physically "drags" a coordinate system, the problem becomes an exercise in warped spacetime for all observers.

</doc>
<doc id="295917" url="https://en.wikipedia.org/wiki?curid=295917" title="Noncommutative geometry">
Noncommutative geometry

Noncommutative geometry (NCG) is a branch of mathematics concerned with a geometric approach to noncommutative algebras, and with the construction of "spaces" that are locally presented by noncommutative algebras of functions (possibly in some generalized sense). A noncommutative algebra is an associative algebra in which the multiplication is not commutative, that is, for which formula_1 does not always equal formula_2; or more generally an algebraic structure in which one of the principal binary operations is not commutative; one also allows additional structures, e.g. topology or norm, to be possibly carried by the noncommutative algebra of functions.
Motivation.
The main motivation is to extend the commutative duality between spaces and functions to the noncommutative setting. In mathematics, "spaces", which are geometric in nature, can be related to numerical functions on them. In general, such functions will form a commutative ring. For instance, one may take the ring "C"("X") of continuous complex-valued functions on a topological space "X". In many cases ("e.g.", if "X" is a compact Hausdorff space), we can recover "X" from "C"("X"), and therefore it makes some sense to say that "X" has "commutative topology".
More specifically, in topology, compact Hausdorff topological spaces can be reconstructed from the Banach algebra of functions on the space (Gel'fand-Neimark). In commutative algebraic geometry, algebraic schemes are locally prime spectra of commutative unital rings (A. Grothendieck), and schemes can be reconstructed from the categories of quasicoherent sheaves of modules on them (P. Gabriel-A. Rosenberg). For Grothendieck topologies, the cohomological properties of a site are invariant of the corresponding category of sheaves of sets viewed abstractly as a topos (A. Grothendieck). In all these cases, a space is reconstructed from the algebra of functions or its categorified version—some category of sheaves on that space.
Functions on a topological space can be multiplied and added pointwise hence they form a commutative algebra; in fact these operations are local in the topology of the base space, hence the functions form a sheaf of commutative rings over the base space.
The dream of noncommutative geometry is to generalize this duality to the duality between
and interact between the algebraic and geometric description of those via this duality.
Regarding that the commutative rings correspond to usual affine schemes, and commutative C*-algebras to usual topological spaces, the extension to noncommutative rings and algebras requires non-trivial generalization of topological spaces, as "non-commutative spaces". For this reason, some talk about non-commutative topology, though the term also has other meanings.
Applications in mathematical physics.
Some applications in particle physics are described on the entries Noncommutative standard model and Noncommutative quantum field theory. Sudden rise in interest in noncommutative geometry in physics, follows after the speculations of its role in M-theory made in 1997.
Motivation from ergodic theory.
Some of the theory developed by Alain Connes to handle noncommutative geometry at a technical level has roots in older attempts, in particular in ergodic theory. The proposal of George Mackey to create a "virtual subgroup" theory, with respect to which ergodic group actions would become homogeneous spaces of an extended kind, has by now been subsumed.
Noncommutative C*-algebras, von Neumann algebras.
(The formal duals of) non-commutative C*-algebras are often now called non-commutative spaces. This is by analogy with the Gelfand representation, which shows that commutative C*-algebras are dual to locally compact Hausdorff spaces. In general, one can associate to any C*-algebra "S" a topological space "Ŝ"; see spectrum of a C*-algebra.
For the duality between σ-finite measure spaces and commutative von Neumann algebras, noncommutative von Neumann algebras are called "non-commutative measure spaces".
Noncommutative differentiable manifolds.
A smooth Riemannian manifold "M" is a topological space with a lot of extra structure. From its algebra of continuous functions "C(M)" we only recover "M" topologically. The algebraic invariant that recovers the Riemannian structure is a spectral triple. It is constructed from a smooth vector bundle "E" over "M", e.g. the exterior algebra bundle. The Hilbert space "L2(M,E)" of square integrable sections of "E" carries a representation of "C(M)" by multiplication operators, and we consider an unbounded operator "D" in "L2(M,E)" with compact resolvent (e.g. the signature operator), such that the commutators "[D,f]" are bounded whenever "f" is smooth. A recent deep theorem states that "M" as a Riemannian manifold can be recovered from this data.
This suggests that one might define a noncommutative Riemannian manifold as a spectral triple "(A,H,D)", consisting of a representation of a "C*"-algebra "A" on a Hilbert space "H", together with an unbounded operator "D" on "H", with compact resolvent, such that "[D,a]" is bounded for all "a" in some dense subalgebra of "A". Research in spectral triples is very active, and many examples of noncommutative manifolds have been constructed.
Noncommutative affine and projective schemes.
In analogy to the duality between affine schemes and commutative rings, we define a category of noncommutative affine schemes as the dual of the category of associative unital rings. There are certain analogues of Zariski topology in that context so that one can glue such affine schemes to more general objects.
There are also generalizations of the Cone and of the Proj of a commutative graded ring, mimicking a Serre's theorem on Proj. Namely the category of quasicoherent sheaves of O-modules on a Proj of a commutative graded algebra is equivalent to the category of graded modules over the ring localized on Serre's subcategory of graded modules of finite length; there is also analogous theorem for coherent sheaves when the algebra is Noetherian. This theorem is extended as a definition of noncommutative projective geometry by Michael Artin and J. J. Zhang, who add also some general ring-theoretic conditions (e.g. Artin-Schelter regularity).
Many properties of projective schemes extend to this context. For example, there exist an analog of the celebrated Serre duality for noncommutative projective schemes of Artin and Zhang.
A. L. Rosenberg has created a rather general relative concept of noncommutative quasicompact scheme (over a base category), abstracting the Grothendieck's study of morphisms of schemes and covers in terms of categories of quasicoherent sheaves and flat localization functors. There is also another interesting approach via localization theory, due to Fred Van Oystaeyen, Luc Willaert and Alain Verschoren, where the main concept is that of a schematic algebra.
Invariants for noncommutative spaces.
Some of the motivating questions of the theory are concerned with extending known topological invariants to formal duals of noncommutative (operator) algebras and other replacements and candidates for noncommutative spaces. One of the main starting points of the Alain Connes' direction in noncommutative geometry is his discovery of a new homology theory associated to noncommutative associative algebras and noncommutative operator algebras, namely the cyclic homology and its relations to the algebraic K-theory (primarily via Connes-Chern character map).
The theory of characteristic classes of smooth manifolds has been extended to spectral triples, employing the tools of operator K-theory and cyclic cohomology. Several generalizations of now classical index theorems allow for effective extraction of numerical invariants from spectral triples. The fundamental characteristic class in cyclic cohomology, the JLO cocycle, generalizes the classical Chern character.

</doc>
<doc id="990534" url="https://en.wikipedia.org/wiki?curid=990534" title="Norm (mathematics)">
Norm (mathematics)

In linear algebra, functional analysis, and related areas of mathematics, a norm is a function that assigns a strictly positive "length" or "size" to each vector in a vector space—save for the zero vector, which is assigned a length of zero. A seminorm, on the other hand, is allowed to assign zero length to some non-zero vectors (in addition to the zero vector).
A norm must also satisfy certain properties pertaining to scalability and additivity which are given in the formal definition below.
A simple example is the 2-dimensional Euclidean space R2 equipped with the Euclidean norm. Elements in this vector space (e.g., ) are usually drawn as arrows in a 2-dimensional cartesian coordinate system starting at the origin . The Euclidean norm assigns to each vector the length of its arrow. Because of this, the Euclidean norm is often known as the magnitude.
A vector space on which a norm is defined is called a normed vector space. Similarly, a vector space with a seminorm is called a seminormed vector space. It is often possible to supply a norm for a given vector space in more than one way.
Definition.
Given a vector space "V" over a subfield "F" of the complex numbers, a norm on "V" is a function with the following properties:
For all "a" ∈ "F" and all u, v ∈ "V",
By the first axiom, absolute homogeneity, we have and , so that by the triangle inequality
A seminorm on "V" is a function with the properties 1. and 2. above.
Every vector space "V" with seminorm "p" induces a normed space "V"/"W", called the quotient space, where "W" is the subspace of "V" consisting of all vectors v in "V" with . The induced norm on "V"/"W" is clearly well-defined and is given by:
Two norms (or seminorms) "p" and "q" on a vector space "V" are equivalent if there exist two real constants "c" and "C", with such that
A topological vector space is called normable (seminormable) if the topology of the space can be induced by a norm (seminorm).
Notation.
If a norm is given on a vector space "V" then the norm of a vector is usually denoted by enclosing it within double vertical lines: . Such notation is also sometimes used if "p" is only a seminorm.
For the length of a vector in Euclidean space (which is an example of a norm, as explained below), the notation with single vertical lines is also widespread.
In Unicode, the codepoint of the "double vertical line" character ‖ is U+2016. The double vertical line should not be confused with the "parallel to" symbol, Unicode U+2225 ( ∥ ). This is usually not a problem because the former is used in parenthesis-like fashion, whereas the latter is used as an infix operator. The double vertical line used here should also not be confused with the symbol used to denote lateral clicks, Unicode U+01C1 ( ǁ ). The single vertical line | is called "vertical line" in Unicode and its codepoint is U+007C.
Examples.
Absolute-value norm.
The absolute value
is a norm on the one-dimensional vector spaces formed by the real or complex numbers.
Euclidean norm.
On an "n"-dimensional Euclidean space R"n", the intuitive notion of length of the vector is captured by the formula
This gives the ordinary distance from the origin to the point x, a consequence of the Pythagorean theorem.
The Euclidean norm is by far the most commonly used norm on R"n", but there are other norms on this vector space as will be shown below. However all these norms are equivalent in the sense that they all define the same topology.
On an "n"-dimensional complex space C"n" the most common norm is
In both cases we can also express the norm as the square root of the inner product of the vector and itself:
where x is represented as a column vector (["x"1; "x"2; ...; "x""n"]), and x∗ denotes its conjugate transpose.
This formula is valid for any inner product space, including Euclidean and complex spaces. For Euclidean spaces, the inner product is equivalent to the dot product. Hence, in this specific case the formula can be also written with the following notation:
The Euclidean norm is also called the Euclidean length, L"2 distance, ℓ2 distance, L"2 norm, or ℓ2 norm; see "L""p" space.
The set of vectors in R"n"+1 whose Euclidean norm is a given positive constant forms an "n"-sphere.
Euclidean norm of a complex number.
The Euclidean norm of a complex number is the absolute value (also called the modulus) of it, if the complex plane is identified with the Euclidean plane R2. This identification of the complex number as a vector in the Euclidean plane, makes the quantity formula_6 (as first suggested by Euler) the Euclidean norm associated with the complex number.
Taxicab norm or Manhattan norm.
The name relates to the distance a taxi has to drive in a rectangular street grid to get from the origin to the point "x".
The set of vectors whose 1-norm is a given constant forms the surface of a cross polytope of dimension equivalent to that of the norm minus 1. The Taxicab norm is also called the formula_8"1 norm. The distance derived from this norm is called the Manhattan distance or formula_8"1 distance.
The 1-norm is simply the sum of the absolute values of the columns.
In contrast,
is not a norm because it may yield negative results.
"p"-norm.
Let be a real number.
For we get the taxicab norm, for we get the Euclidean norm, and as "p" approaches formula_12 the "p"-norm approaches the infinity norm or . The "p"-norm is related to the generalized mean or power mean.
This definition is still of some interest for , but the resulting function does not define a norm, because it violates the triangle inequality. What is true for this case of , even in the measurable analog, is that the corresponding "Lp" class is a vector space, and it is also true that the function
(without "p"th root) defines a distance that makes "L""p"("X") into a complete metric topological vector space. These spaces are of great interest in functional analysis, probability theory, and harmonic analysis.
However, outside trivial cases, this topological vector space is not locally convex and has no continuous nonzero linear forms. Thus the topological dual space contains only the zero functional.
The derivative of the "p"-norm is given by
The derivative with respect to x, therefore, is
where formula_16 denotes Hadamard product and formula_17 is used for absolute value of each component of the vector.
For the special case of , this becomes
or
Maximum norm (special case of: infinity norm, uniform norm, or supremum norm).
The set of vectors whose infinity norm is a given constant, "c", forms the surface of a hypercube with edge length 2"c".
Zero norm.
In probability and functional analysis, the zero norm induces a complete metric topology for the space of measurable functions and for the F-space of sequences with F–norm formula_21, which is discussed by Stefan Rolewicz in "Metric Linear Spaces". Here we mean by "F-norm" some real-valued function formula_22 on an F-space with distance "d", such that formula_23.
Hamming distance of a vector from zero.
In metric geometry, the discrete metric takes the value one for distinct points and zero otherwise. When applied coordinate-wise to the elements of a vector space, the discrete distance defines the "Hamming distance", which is important in coding and information theory. In the field of real or complex numbers, the distance of the discrete metric from zero is not homogeneous in the non-zero point; indeed, the distance from zero remains one as its non-zero argument approaches zero. However, the discrete distance of a number from zero does satisfy the other properties of a norm, namely the triangle inequality and positive definiteness. When applied component-wise to vectors, the discrete distance from zero behaves like a non-homogeneous "norm", which counts the number of non-zero components in its vector argument; again, this non-homogeneous "norm" is discontinuous.
In signal processing and statistics, David Donoho referred to the "zero" "norm" with quotation marks. Following Donoho's notation, the zero "norm" of x is simply the number of non-zero coordinates of x, or the Hamming distance of the vector from zero. When this "norm" is localized to a bounded set, it is the limit of "p"-norms as "p" approaches 0. Of course, the zero "norm" is not truly a norm, because it is not positive homogeneous. Indeed, it is not even an F-norm in the sense described above, since it is discontinuous, jointly and severally, with respect to the scalar argument in scalar–vector multiplication and with respect to its vector argument. Abusing terminology, some engineers omit Donoho's quotation marks and inappropriately call the number-of-nonzeros function the "L0" norm, echoing the notation for the Lebesgue space of measurable functions.
Other norms.
Other norms on R"n" can be constructed by combining the above; for example
is a norm on R4.
For any norm and any injective linear transformation "A" we can define a new norm of "x", equal to
In 2D, with "A" a rotation by 45° and a suitable scaling, this changes the taxicab norm into the maximum norm. In 2D, each "A" applied to the taxicab norm, up to inversion and interchanging of axes, gives a different unit ball: a parallelogram of a particular shape, size and orientation. In 3D this is similar but different for the 1-norm (octahedrons) and the maximum norm (prisms with parallelogram base).
All the above formulas also yield norms on C"n" without modification.
Infinite-dimensional case.
The generalization of the above norms to an infinite number of components leads to the "L""p" spaces, with norms
(for complex-valued sequences "x" resp. functions "f" defined on formula_27), which can be further generalized (see Haar measure).
Any inner product induces in a natural way the norm formula_28
Other examples of infinite dimensional normed vector spaces can be found in the Banach space article.
Properties.
The concept of unit circle (the set of all vectors of norm 1) is different in different norms: for the 1-norm the unit circle in R2 is a square, for the 2-norm (Euclidean norm) it is the well-known unit circle, while for the infinity norm it is a different square. For any "p"-norm it is a superellipse (with congruent axes). See the accompanying illustration. Due to the definition of the norm, the unit circle is always convex and centrally symmetric (therefore, for example, the unit ball may be a rectangle but cannot be a triangle).
In terms of the vector space, the seminorm defines a topology on the space, and this is a Hausdorff topology precisely when the seminorm can distinguish between distinct vectors, which is again equivalent to the seminorm being a norm. The topology thus defined (by either a norm or a seminorm) can be understood either in terms of sequences or open sets. A sequence of vectors formula_29 is said to converge in norm to formula_30 if formula_31 as formula_32. Equivalently, the topology consists of all sets that can be represented as a union of open balls.
Two norms ‖•‖"α" and ‖•‖"β" on a vector space "V" are called "equivalent" if there exist positive real numbers "C" and "D" such that for all "x" in "V"
For instance, on formula_34, if "p" > "r" > 0, then
In particular,
If the vector space is a finite-dimensional real or complex one, all norms are equivalent. On the other hand, in the case of infinite-dimensional vector spaces, not all norms are equivalent.
Equivalent norms define the same notions of continuity and convergence and for many purposes do not need to be distinguished. To be more precise the uniform structure defined by equivalent norms on the vector space is uniformly isomorphic.
Every (semi)-norm is a sublinear function, which implies that every norm is a convex function. As a result, finding a global optimum of a norm-based objective function is often tractable.
Given a finite family of seminorms "p""i" on a vector space the sum
is again a seminorm.
For any norm "p" on a vector space "V", we have that for all u and :
"Proof:" Applying the triangular inequality to both formula_40 and formula_41:
Thus, "p"(u ± v) ≥ |"p"(u) − "p"(v)|.
If formula_46 and formula_47 are normed spaces and formula_48 is a continuous linear map, then the norm of formula_49 and the norm of the transpose of formula_49 are equal.
For the lp norms, we have Hölder's inequality
A special case of this is the Cauchy–Schwarz inequality:
Classification of seminorms: absolutely convex absorbing sets.
All seminorms on a vector space "V" can be classified in terms of absolutely convex absorbing sets in "V". To each such set, "A", corresponds a seminorm "pA" called the gauge of "A", defined as
with the property that
Conversely:
Any locally convex topological vector space has a local basis consisting of absolutely convex sets. A common method to construct such a basis is to use a family ("p") of seminorms "p" that separates points: the collection of all finite intersections of sets {"p" < 1/"n"} turns the space into a locally convex topological vector space so that every p is continuous.
Such a method is used to design weak and weak* topologies.
norm case:
Generalizations.
There are several generalizations of norms and semi-norms. If "p" is absolute homogeneity but in place of subadditivity we require that
then "p" satisfies the triangle inequality but is called a quasi-seminorm and the smallest value of "b" for which this holds is called the multiplier of "p"; if in addition "p" separates points then it is called a quasi-norm.
On the other hand, if "p" satisfies the triangle inequality but in place of absolute homogeneity we require that
then "p" is called a "k"-seminorm.
We have the following relationship between quasi-seminorms and "k"-seminorms:

</doc>
<doc id="21462" url="https://en.wikipedia.org/wiki?curid=21462" title="Normal distribution">
Normal distribution

\, e^{-\frac{(x - \mu)^2}{2 \sigma^2}}</math>
 | cdf = formula_1
 | quantile = formula_2
 | mean = 
 | median = 
 | mode = 
 | variance = formula_3
 | skewness = 0
 | kurtosis = 0 
 | entropy = formula_4
 | mgf = formula_5
 | char = formula_6
 | fisher = formula_7
 | conjugate prior = Normal distribution
In probability theory, the normal (or Gaussian) distribution is a very common continuous probability distribution. Normal distributions are important in statistics and are often used in the natural and social sciences to represent real-valued random variables whose distributions are not known.
The normal distribution is remarkably useful because of the central limit theorem. In its most general form, under some conditions (which include finite variance), it states that averages of random variables independently drawn from independent distributions converge in distribution to the normal, that is, become normally distributed when the number of random variables is sufficiently large. Physical quantities that are expected to be the sum of many independent processes (such as measurement errors) often have distributions that are nearly normal. Moreover, many results and methods (such as propagation of uncertainty and least squares parameter fitting) can be derived analytically in explicit form when the relevant variables are normally distributed.
The normal distribution is sometimes informally called the bell curve. However, many other distributions are bell-shaped (such as Cauchy's, Student's, and logistic). The terms Gaussian function and Gaussian bell curve are also ambiguous because they sometimes refer to multiples of the normal distribution that cannot be directly interpreted in terms of probabilities.
The probability density of the normal distribution is:
Here, formula_9 is the "mean" or "expectation" of the distribution (and also its median and mode). The parameter formula_10 is its standard deviation with its variance then formula_11. A random variable with a Gaussian distribution is said to be normally distributed and is called a normal deviate.
If formula_12 and formula_13, the distribution is called the standard normal distribution or the unit normal distribution denoted by formula_14 and a random variable with that distribution is a standard normal deviate.
The normal distribution is the only absolutely continuous distribution whose cumulants beyond the first two (i.e., other than the mean and variance) are zero. It is also the continuous distribution with the maximum entropy for a specified mean and variance.
The normal distribution is a subclass of the elliptical distributions. The normal distribution is symmetric about its mean, and is non-zero over the entire real line. As such it may not be a suitable model for variables that are inherently positive or strongly skewed, such as the weight of a person or the price of a share. Such variables may be better described by other distributions, such as the log-normal distribution or the Pareto distribution.
The value of the normal distribution is practically zero when the value "x" lies more than a few standard deviations away from the mean. Therefore, it may not be an appropriate model when one expects a significant fraction of outliers—values that lie many standard deviations away from the mean—and least squares and other statistical inference methods that are optimal for normally distributed variables often become highly unreliable when applied to such data. In those cases, a more heavy-tailed distribution should be assumed and the appropriate robust statistical inference methods applied.
The Gaussian distribution belongs to the family of stable distributions which are the attractors of sums of independent, identically distributed distributions whether or not the mean or variance is finite. Except for the Gaussian which is a limiting case, all stable distributions have heavy tails and infinite variance.
Definition.
Standard normal distribution.
The simplest case of a normal distribution is known as the "standard normal distribution". This is a special case where μ=0 and σ=1, and it is described by this probability density function:
The factor formula_16 in this expression ensures that the total area under the curve formula_17 is equal to one. The in the exponent ensures that the distribution has unit variance (and therefore also unit standard deviation). This function is symmetric around "x"=0, where it attains its maximum value formula_18; and has inflection points at +1 and −1.
Authors may differ also on which normal distribution should be called the "standard" one. Gauss defined the standard normal as having variance formula_19, that is
Stigler goes even further, defining the standard normal with variance formula_21 :
General normal distribution.
Every normal distribution is a version of the standard normal distribution whose domain has been stretched by a factor "σ" (the standard deviation) and then translated by "μ" (the mean value):
The probability density must be scaled by formula_24 so that the integral is still 1.
If "Z" is a standard normal deviate, then "X" = "Zσ" + "μ" will have a normal distribution with expected value "μ" and standard deviation "σ". Conversely, if "X" is a general normal deviate, then "Z" = ("X" − "μ")/"σ" will have a standard normal distribution.
Every normal distribution is the exponential of a quadratic function:
where "a" is negative and "c" is formula_26. In this form, the mean value "μ" is −"b"/(2"a"), and the variance "σ"2 is −1/(2"a"). For the standard normal distribution, "a" is −1/2, "b" is zero, and "c" is formula_27.
Notation.
The standard Gaussian distribution (with zero mean and unit variance) is often denoted with the Greek letter "ϕ" (phi). The alternative form of the Greek phi letter, "φ", is also used quite often.
The normal distribution is also often denoted by "N"("μ", "σ"2). Thus when a random variable "X" is distributed normally with mean "μ" and variance "σ"2, we write
Alternative parameterizations.
Some authors advocate using the precision formula_29 as the parameter defining the width of the distribution, instead of the deviation "σ" or the variance "σ"2. The precision is normally defined as the reciprocal of the variance, 1/"σ"2. The formula for the distribution then becomes
This choice is claimed to have advantages in numerical computations when "σ" is very close to zero and simplify formulas in some contexts, such as in the Bayesian inference of variables with multivariate normal distribution.
Also the reciprocal of the standard deviation formula_31 might be defined as the "precision" and the expression of the normal distribution becomes
According to Stigler, this formulation is advantageous because of a much simpler and easier-to-remember formula, the fact that the pdf has unit height at zero, and simple approximate formulas for the quantiles of the distribution.
Properties.
Symmetries and derivatives.
The normal distribution "f"("x"), with any mean "μ" and any positive deviation "σ", has the following properties:
Furthermore, the density "ϕ" of the standard normal distribution (with and ) also has the following properties:
Moments.
The plain and absolute moments of a variable "X" are the expected values of "Xp" and |"X"|"p",respectively. If the expected value "μ" of "X" is zero, these parameters are called "central moments". Usually we are interested only in moments with integer order "p".
If "X" has a normal distribution, these moments exist and are finite for any "p" whose real part is greater than −1. For any non-negative integer "p", the plain central moments are
Here "n"!! denotes the double factorial, that is, the product of every number from "n" to 1 that has the same parity as "n".
The central absolute moments coincide with plain moments for all even orders, but are nonzero for odd orders. For any non-negative integer "p",
The last formula is valid also for any non-integer .
When the mean "μ" is not zero, the plain and absolute moments can be expressed in terms of confluent hypergeometric functions 1"F"1 and "U".
These expressions remain valid even if "p" is not integer. See also generalized Hermite polynomials.
Fourier transform and characteristic function.
The Fourier transform of a normal distribution "f" with mean "μ" and deviation "σ" is
where i is the imaginary unit. If the mean "μ" is zero, the first factor is 1, and the Fourier transform is also a normal distribution on the frequency domain, with mean 0 and standard deviation 1/"σ". In particular, the standard normal distribution "ϕ" (with "μ"=0 and "σ"=1) is an eigenfunction of the Fourier transform.
In probability theory, the Fourier transform of the probability distribution of a real-valued random variable "X" is called the characteristic function of that variable, and can be defined as the expected value of "e""i tX", as a function of the real variable "t" (the frequency parameter of the Fourier transform). This definition can be analytically extended to a complex-value parameter "t".
Moment and cumulant generating functions.
The moment generating function of a real random variable "X" is the expected value of "etX", as a function of the real parameter "t". For a normal distribution with mean "μ" and deviation "σ", the moment generating function exists and is equal to
The cumulant generating function is the logarithm of the moment generating function, namely
Since this is a quadratic polynomial in "t", only the first two cumulants are nonzero, namely the mean "μ" and the variance "σ"2.
Cumulative distribution function.
The cumulative distribution function (CDF) of the standard normal distribution, usually denoted with the capital Greek letter formula_45 (phi), is the integral
In statistics one often uses the related error function, or erf("x"), defined as the probability of a random variable with normal distribution of mean 0 and variance 1/2 falling in the range formula_47; that is
These integrals cannot be expressed in terms of elementary functions, and are often said to be special functions. However, many numerical approximations are known; see below.
The two functions are closely related, namely
For a generic normal distribution "f" with mean "μ" and deviation "σ", the cumulative distribution function is
The complement of the standard normal CDF, formula_51, is often called the Q-function, especially in engineering texts. It gives the probability that the value of a standard normal random variable "X" will exceed "x". Other definitions of the "Q"-function, all of which are simple transformations of formula_45, are also used occasionally.
The graph of the standard normal CDF formula_45 has 2-fold rotational symmetry around the point (0,1/2); that is, formula_54. Its antiderivative (indefinite integral) formula_55 is formula_56.
where formula_58 denotes the double factorial.
As an example, the following Pascal function approximates the CDF:
Standard deviation and tolerance intervals.
About 68% of values drawn from a normal distribution are within one standard deviation "σ" away from the mean; about 95% of the values lie within two standard deviations; and about 99.7% are within three standard deviations. This fact is known as the 68-95-99.7 (empirical) rule, or the "3-sigma rule".
More precisely, the probability that a normal deviate lies in the range and is given by
To 12 decimal places, the values for "n" = 1, 2, …, 6 are:
Quantile function.
The quantile function of a distribution is the inverse of the cumulative distribution function. The quantile function of the standard normal distribution is called the probit function, and can be expressed in terms of the inverse error function:
For a normal random variable with mean "μ" and variance "σ"2, the quantile function is
The quantile formula_62 of the standard normal distribution is commonly denoted as "zp". These values are used in hypothesis testing, construction of confidence intervals and Q-Q plots. A normal random variable "X" will exceed "μ" + "σzp" with probability 1−"p"; and will lie outside the interval "μ" ± "σzp" with probability 2(1−"p"). In particular, the quantile "z"0.975 is 1.96; therefore a normal random variable will lie outside the interval "μ" ± 1.96"σ" in only 5% of cases.
The following table gives the multiple "n" of "σ" such that "X" will lie in the range with a specified probability "p". These values are useful to determine tolerance interval for sample averages and other statistical estimators with normal (or asymptotically normal) distributions:
Zero-variance limit.
In the limit when "σ" tends to zero, the probability density "f"("x") eventually tends to zero at any , but grows without limit if , while its integral remains equal to 1. Therefore, the normal distribution cannot be defined as an ordinary function when .
However, one can define the normal distribution with zero variance as a generalized function; specifically, as Dirac's "delta function" "δ" translated by the mean "μ", that is "f"("x") = "δ"("x"−"μ").
Its CDF is then the Heaviside step function translated by the mean "μ", namely
Central limit theorem.
The central limit theorem states that under certain (fairly common) conditions, the sum of many random variables will have an approximately normal distribution. More specifically, where "X"1, …, "Xn" are independent and identically distributed random variables with the same arbitrary distribution, zero mean, and variance "σ"2; and "Z" is their
mean scaled by formula_64
Then, as "n" increases, the probability distribution of "Z" will
tend to the normal distribution with zero mean and variance "σ"2.
The theorem can be extended to variables "Xi" that are not independent and/or not identically distributed if certain constraints are placed on the degree of dependence and the moments
of the distributions.
Many test statistics, scores, and estimators encountered in practice contain sums of certain random variables in them, and even more estimators can be represented as sums of random variables through the use of influence functions. The central limit theorem implies that those statistical parameters will have asymptotically normal distributions.
The central limit theorem also implies that certain distributions can be approximated by the normal distribution, for example:
Whether these approximations are sufficiently accurate depends on the purpose for which they are needed, and the rate of convergence to the normal distribution. It is typically the case that such approximations are less accurate in the tails of the distribution.
A general upper bound for the approximation error in the central limit theorem is given by the Berry–Esseen theorem, improvements of the approximation are given by the Edgeworth expansions.
Maximum entropy.
Of all probability distributions over the reals with a specified mean "μ" and variance "σ"2, the normal distribution is the one with maximum entropy. If "X" is a continuous random variable with probability density "f"("x"), then the entropy of "X" is defined as
where "f"("x") log "f"("x") is understood to be zero whenever "f"("x") = 0. This functional can be maximized, subject to the constraints that the distribution is properly normalized and has a specified variance, by using variational calculus. A function with two Lagrangian multipliers is defined:
where "f(x)" is, for now, regarded as some function with mean μ and standard deviation formula_10. At maximum entropy, a small variation δ"f"("x") about "f(x)" will produce a variation δ"L" about "L" which is equal to zero:
Since this must hold for any small δ"f"("x"), the term in brackets must be zero, and solving for "f(x)" yields:
Using the constraint equations to solve for λ0 and λ yields the normal distribution:
Operations on normal deviates.
The family of normal distributions is closed under linear transformations: if "X" is normally distributed with mean "μ" and standard deviation "σ", then the variable , for any real numbers "a" and "b", is also normally distributed, with
mean "aμ" + "b" and standard deviation "|a|σ".
Also if "X"1 and "X"2 are two independent normal random variables, with means "μ"1, "μ"2 and standard deviations "σ"1, "σ"2, then their sum will also be normally distributed,[proof] with mean "μ"1 + "μ"2 and variance formula_72.
In particular, if "X" and "Y" are independent normal deviates with zero mean and variance "σ"2, then and are also independent and normally distributed, with zero mean and variance 2"σ"2. This is a special case of the polarization identity.
Also, if "X"1, "X"2 are two independent normal deviates with mean "μ" and deviation "σ", and "a", "b" are arbitrary real numbers, then the variable
is also normally distributed with mean "μ" and deviation "σ". It follows that the normal distribution is stable (with exponent "α" = 2).
More generally, any linear combination of independent normal deviates is a normal deviate.
Infinite divisibility and Cramér's theorem.
For any positive integer "n", any normal distribution with mean "μ" and variance "σ"2 is the distribution of the sum of "n" independent normal deviates, each with mean "μ/n" and variance "σ"2"/n". This property is called infinite divisibility.
Conversely, if "X"1 and "X"2 are independent random variables and their sum has a normal distribution, then both "X"1 and "X"2 must be normal deviates.
This result is known as Cramér's decomposition theorem, and is equivalent to saying that the convolution of two distributions is normal if and only if both are normal. Cramér's theorem implies that a linear combination of independent non-Gaussian variables will never have an exactly normal distribution, although it may approach it arbitrarily closely.
Bernstein's theorem.
Bernstein's theorem states that if "X" and "Y" are independent and and are also independent, then both "X" and "Y" must necessarily have normal distributions.
More generally, if "X"1, …, "Xn" are independent random variables, then two distinct linear combinations ∑"akXk" and ∑"bkXk" will be independent if and only if all "Xk"'s are normal and , where denotes the variance of "Xk".
Other properties.
 e^{-\frac{1}{4}\frac{(\mu_1-\mu_2)^2}{\sigma_1^2+\sigma_2^2}}\ .
 </math>
Related distributions.
Operations on a single random variable.
If "X" is distributed normally with mean "μ" and variance "σ"2, then
Combination of two independent random variables.
If "X"1 and "X"2 are two independent standard normal random variables with mean 0 and variance 1, then
Operations on the density function.
The split normal distribution is most directly defined in terms of joining scaled sections of the density functions of different normal distributions and rescaling the density to integrate to one. The truncated normal distribution results from rescaling a section of a single density function.
Extensions.
The notion of normal distribution, being one of the most important distributions in probability theory, has been extended far beyond the standard framework of the univariate (that is one-dimensional) case (Case 1). All these extensions are also called "normal" or "Gaussian" laws, so a certain ambiguity in names exists.
A random variable "x" has a two piece normal distribution if it has a distribution
where "μ" is the mean and "σ"1 and "σ"2 are the standard deviations of the distribution to the left and right of the mean respectively.
The mean, variance and third central moment of this distribution have been determined
where "E"("x"), "V"("x") and "T"("x") are the mean, variance and third central moment respectively.
One of the main practical uses of the Gaussian law is to model the empirical distributions of many different random variables encountered in practice. In such case a possible extension would be a richer family of distributions, having more than two parameters and therefore being able to fit the empirical distribution more accurately. The examples of such extensions are:
Normality tests.
Normality tests assess the likelihood that the given data set {"x"1, …, "xn"} comes from a normal distribution. Typically the null hypothesis "H"0 is that the observations are distributed normally with unspecified mean "μ" and variance "σ"2, versus the alternative "Ha" that the distribution is arbitrary. Many tests (over 40) have been devised for this problem, the more prominent of them are outlined below:
Estimation of parameters.
It is often the case that we don't know the parameters of the normal distribution, but instead want to estimate them. That is, having a sample ("x"1, …, "xn") from a normal population we would like to learn the approximate values of parameters "μ" and "σ"2. The standard approach to this problem is the maximum likelihood method, which requires maximization of the "log-likelihood function":
Taking derivatives with respect to "μ" and "σ"2 and solving the resulting system of first order conditions yields the "maximum likelihood estimates":
Sample mean.
Estimator formula_90 is called the "sample mean", since it is the arithmetic mean of all observations. The statistic formula_91 is complete and sufficient for "μ", and therefore by the Lehmann–Scheffé theorem, formula_90 is the uniformly minimum variance unbiased (UMVU) estimator. In finite samples it is distributed normally:
The variance of this estimator is equal to the "μμ"-element of the inverse Fisher information matrix formula_94. This implies that the estimator is finite-sample efficient. Of practical importance is the fact that the standard error of formula_90 is proportional to formula_96, that is, if one wishes to decrease the standard error by a factor of 10, one must increase the number of points in the sample by a factor of 100. This fact is widely used in determining sample sizes for opinion polls and the number of trials in Monte Carlo simulations.
From the standpoint of the asymptotic theory, formula_90 is consistent, that is, it converges in probability to "μ" as "n" → ∞. The estimator is also asymptotically normal, which is a simple corollary of the fact that it is normal in finite samples:
Sample variance.
The estimator formula_99 is called the "sample variance", since it is the variance of the sample ("x"1, …, "xn"). In practice, another estimator is often used instead of the formula_99. This other estimator is denoted "s"2, and is also called the "sample variance", which represents a certain ambiguity in terminology; its square root "s" is called the "sample standard deviation". The estimator "s"2 differs from formula_99 by having instead of "n" in the denominator (the so-called Bessel's correction):
The difference between "s"2 and formula_99 becomes negligibly small for large "n"'s. In finite samples however, the motivation behind the use of "s"2 is that it is an unbiased estimator of the underlying parameter "σ"2, whereas formula_99 is biased. Also, by the Lehmann–Scheffé theorem the estimator "s"2 is uniformly minimum variance unbiased (UMVU), which makes it the "best" estimator among all unbiased ones. However it can be shown that the biased estimator formula_99 is "better" than the "s"2 in terms of the mean squared error (MSE) criterion. In finite samples both "s"2 and formula_99 have scaled chi-squared distribution with degrees of freedom:
The first of these expressions shows that the variance of "s"2 is equal to , which is slightly greater than the "σσ"-element of the inverse Fisher information matrix formula_94. Thus, "s"2 is not an efficient estimator for "σ"2, and moreover, since "s"2 is UMVU, we can conclude that the finite-sample efficient estimator for "σ"2 does not exist.
Applying the asymptotic theory, both estimators "s"2 and formula_99 are consistent, that is they converge in probability to "σ"2 as the sample size . The two estimators are also both asymptotically normal:
In particular, both estimators are asymptotically efficient for "σ"2.
Confidence intervals.
By Cochran's theorem, for normal distributions the sample mean formula_90 and the sample variance "s"2 are independent, which means there can be no gain in considering their joint distribution. There is also a converse theorem: if in a sample the sample mean and sample variance are independent, then the sample must have come from the normal distribution. The independence between formula_90 and "s" can be employed to construct the so-called "t-statistic":
This quantity "t" has the Student's t-distribution with degrees of freedom, and it is an ancillary statistic (independent of the value of the parameters). Inverting the distribution of this "t"-statistics will allow us to construct the confidence interval for "μ"; similarly, inverting the "χ"2 distribution of the statistic "s"2 will give us the confidence interval for "σ"2:
where "tk,p" and are the "p"th quantiles of the "t"- and "χ"2-distributions respectively. These confidence intervals are of the "confidence level" , meaning that the true values "μ" and "σ"2 fall outside of these intervals with probability (or significance level) "α". In practice people usually take , resulting in the 95% confidence intervals. The approximate formulas in the display above were derived from the asymptotic distributions of formula_90 and "s"2. The approximate formulas become valid for large values of "n", and are more convenient for the manual calculation since the standard normal quantiles "zα/2" do not depend on "n". In particular, the most popular value of , results in .
Bayesian analysis of the normal distribution.
Bayesian analysis of normally distributed data is complicated by the many different possibilities that may be considered:
The formulas for the non-linear-regression cases are summarized in the conjugate prior article.
Sum of two quadratics.
Scalar form.
The following auxiliary formula is useful for simplifying the posterior update equations, which otherwise become fairly tedious.
This equation rewrites the sum of two quadratics in "x" by expanding the squares, grouping the terms in "x", and completing the square. Note the following about the complex constant factors attached to some of the terms:
Vector form.
A similar formula can be written for the sum of two vector quadratics: If x, y, z are vectors of length "k", and A and B are symmetric, invertible matrices of size formula_120, then
where
Note that the form x′ A x is called a quadratic form and is a scalar:
In other words, it sums up all possible combinations of products of pairs of elements from x, with a separate coefficient for each. In addition, since formula_124, only the sum formula_125 matters for any off-diagonal elements of A, and there is no loss of generality in assuming that A is symmetric. Furthermore, if A is symmetric, then the form formula_126 .
Sum of differences from the mean.
Another useful formula is as follows:
where formula_128
With known variance.
For a set of i.i.d. normally distributed data points X of size "n" where each individual point "x" follows formula_129 with known variance σ2, the conjugate prior distribution is also normally distributed.
This can be shown more easily by rewriting the variance as the precision, i.e. using τ = 1/σ2. Then if formula_130 and formula_131 we proceed as follows.
First, the likelihood function is (using the formula above for the sum of differences from the mean):
Then, we proceed as follows:
In the above derivation, we used the formula above for the sum of two quadratics and eliminated all constant factors not involving "μ". The result is the kernel of a normal distribution, with mean formula_134 and precision formula_135, i.e.
This can be written as a set of Bayesian update equations for the posterior parameters in terms of the prior parameters:
That is, to combine "n" data points with total precision of "n"τ (or equivalently, total variance of "n"/σ2) and mean of values formula_138, derive a new total precision simply by adding the total precision of the data to the prior total precision, and form a new mean through a "precision-weighted average", i.e. a weighted average of the data mean and the prior mean, each weighted by the associated total precision. This makes logical sense if the precision is thought of as indicating the certainty of the observations: In the distribution of the posterior mean, each of the input components is weighted by its certainty, and the certainty of this distribution is the sum of the individual certainties. (For the intuition of this, compare the expression "the whole is (or is not) greater than the sum of its parts". In addition, consider that the knowledge of the posterior comes from a combination of the knowledge of the prior and likelihood, so it makes sense that we are more certain of it than of either of its components.)
The above formula reveals why it is more convenient to do Bayesian analysis of conjugate priors for the normal distribution in terms of the precision. The posterior precision is simply the sum of the prior and likelihood precisions, and the posterior mean is computed through a precision-weighted average, as described above. The same formulas can be written in terms of variance by reciprocating all the precisions, yielding the more ugly formulas
With known mean.
For a set of i.i.d. normally distributed data points X of size "n" where each individual point "x" follows formula_129 with known mean μ, the conjugate prior of the variance has an inverse gamma distribution or a scaled inverse chi-squared distribution. The two are equivalent except for having different parameterizations. Although the inverse gamma is more commonly used, we use the scaled inverse chi-squared for the sake of convenience. The prior for σ2 is as follows:
The likelihood function from above, written in terms of the variance, is:
where
Then:
The above is also a scaled inverse chi-squared distribution where
or equivalently
Reparameterizing in terms of an inverse gamma distribution, the result is:
With unknown mean and unknown variance.
For a set of i.i.d. normally distributed data points X of size "n" where each individual point "x" follows formula_129 with unknown mean μ and unknown variance σ2, a combined (multivariate) conjugate prior is placed over the mean and variance, consisting of a normal-inverse-gamma distribution.
Logically, this originates as follows:
The priors are normally defined as follows:
The update equations can be derived, and look as follows:
The respective numbers of pseudo-observations add the number of actual observations to them. The new mean hyperparameter is once again a weighted average, this time weighted by the relative numbers of observations. Finally, the update for formula_151 is similar to the case with known mean, but in this case the sum of squared deviations is taken with respect to the observed data mean rather than the true mean, and as a result a new "interaction term" needs to be added to take care of the additional error source stemming from the deviation between prior and data mean.
The prior distributions are
Therefore, the joint prior is
The likelihood function from the section above with known variance is:
Writing it in terms of variance rather than precision, we get:
where formula_156
Therefore, the posterior is (dropping the hyperparameters as conditioning factors):
In other words, the posterior distribution has the form of a product of a normal distribution over "p"("μ" | "σ"2) times an inverse gamma distribution over "p"(σ2), with parameters that are the same as the update equations above.
Occurrence.
The occurrence of normal distribution in practical problems can be loosely classified into four categories:
Exact normality.
Certain quantities in physics are distributed normally, as was first demonstrated by James Clerk Maxwell. Examples of such quantities are:
Approximate normality.
"Approximately" normal distributions occur in many situations, as explained by the central limit theorem. When the outcome is produced by many small effects acting "additively and independently", its distribution will be close to normal. The normal approximation will not be valid if the effects act multiplicatively (instead of additively), or if there is a single external influence that has a considerably larger magnitude than the rest of the effects.
Assumed normality.
There are statistical methods to empirically test that assumption, see the above Normality tests section.
Produced normality.
In regression analysis, lack of normality in residuals simply indicates that the model postulated is inadequate in accounting for the tendency in the data and needs to be augmented; in other words, normality in residuals can always be achieved given a properly constructed model.
Generating values from normal distribution.
In computer simulations, especially in applications of the Monte-Carlo method, it is often desirable to generate values that are normally distributed. The algorithms listed below all generate the standard normal deviates, since a can be generated as , where "Z" is standard normal. All these algorithms rely on the availability of a random number generator "U" capable of producing uniform random variates.
Numerical approximations for the normal CDF.
The standard normal CDF is widely used in scientific and statistical computing.
The values Φ("x") may be approximated very accurately by a variety of methods, such as numerical integration, Taylor series, asymptotic series and continued fractions. Different approximations are used depending on the desired level of accuracy.
Some more approximations can be found at: Error function#Approximation with elementary functions.
History.
Development.
Some authors attribute the credit for the discovery of the normal distribution to de Moivre, who in 1738 published in the second edition of his "The Doctrine of Chances" the study of the coefficients in the binomial expansion of . De Moivre proved that the middle term in this expansion has the approximate magnitude of formula_160, and that "If "m" or ½"n" be a Quantity infinitely great, then the Logarithm of the Ratio, which a Term distant from the middle by the Interval "ℓ", has to the middle Term, is formula_161." Although this theorem can be interpreted as the first obscure expression for the normal probability law, Stigler points out that de Moivre himself did not interpret his results as anything more than the approximate rule for the binomial coefficients, and in particular de Moivre lacked the concept of the probability density function.
In 1809 Gauss published his monograph "Theoria motus corporum coelestium in sectionibus conicis solem ambientium" where among other things he introduces several important statistical concepts, such as the method of least squares, the method of maximum likelihood, and the "normal distribution". Gauss used "M", , to denote the measurements of some unknown quantity "V", and sought the "most probable" estimator: the one that maximizes the probability of obtaining the observed experimental results. In his notation "φΔ" is the probability law of the measurement errors of magnitude "Δ". Not knowing what the function "φ" is, Gauss requires that his method should reduce to the well-known answer: the arithmetic mean of the measured values. Starting from these principles, Gauss demonstrates that the only law that rationalizes the choice of arithmetic mean as an estimator of the location parameter, is the normal law of errors:
formula_162 
where "h" is "the measure of the precision of the observations". Using this normal law as a generic model for errors in the experiments, Gauss formulates what is now known as the non-linear weighted least squares (NWLS) method.
Although Gauss was the first to suggest the normal distribution law, Laplace made significant contributions. It was Laplace who first posed the problem of aggregating several observations in 1774, although his own solution led to the Laplacian distribution. It was Laplace who first calculated the value of the integral in 1782, providing the normalization constant for the normal distribution. Finally, it was Laplace who in 1810 proved and presented to the Academy the fundamental central limit theorem, which emphasized the theoretical importance of the normal distribution.
It is of interest to note that in 1809 an American mathematician Adrain published two derivations of the normal probability law, simultaneously and independently from Gauss. His works remained largely unnoticed by the scientific community, until in 1871 they were "rediscovered" by Abbe.
In the middle of the 19th century Maxwell demonstrated that the normal distribution is not just a convenient mathematical tool, but may also occur in natural phenomena: "The number of particles whose velocity, resolved in a certain direction, lies between "x" and "x" + "dx" is
Naming.
Since its introduction, the normal distribution has been known by many different names: the law of error, the law of facility of errors, Laplace's second law, Gaussian law, etc. Gauss himself apparently coined the term with reference to the "normal equations" involved in its applications, with normal having its technical meaning of orthogonal rather than "usual". However, by the end of the 19th century some authors had started using the name "normal distribution", where the word "normal" was used as an adjective – the term now being seen as a reflection of the fact that this distribution was seen as typical, common – and thus "normal". Peirce (one of those authors) once defined "normal" thus: "...the 'normal' is not the average (or any other kind of mean) of what actually occurs, but of what "would", in the long run, occur under certain circumstances." Around the turn of the 20th century Pearson popularized the term "normal" as a designation for this distribution.
Also, it was Pearson who first wrote the distribution in terms of the standard deviation "σ" as in modern notation. Soon after this, in year 1915, Fisher added the location parameter to the formula for normal distribution, expressing it in the way it is written nowadays:
The term "standard normal", which denotes the normal distribution with zero mean and unit variance came into general use around the 1950s, appearing in the popular textbooks by P.G. Hoel (1947) ""Introduction to mathematical statistics" and A.M. Mood (1950) "Introduction to the theory of statistics"".
When the name is used, the "Gaussian distribution" was named after Carl Friedrich Gauss, who introduced the distribution in 1809 as a way of rationalizing the method of least squares as outlined above. Among English speakers, both "normal distribution" and "Gaussian distribution" are in common use, with different terms preferred by different communities.

</doc>
<doc id="170366" url="https://en.wikipedia.org/wiki?curid=170366" title="Normal matrix">
Normal matrix

In mathematics, a complex square matrix is normal if
where is the conjugate transpose of . That is, a matrix is normal if it commutes with its conjugate transpose.
A real square matrix satisfies , and is therefore normal if .
Normality is a convenient test for diagonalizability: a matrix is normal if and only if it is unitarily similar to a diagonal matrix, and therefore any matrix satisfying the equation is diagonalizable.
The concept of normal matrices can be extended to normal operators on infinite dimensional Hilbert spaces and to normal elements in C*-algebras. As in the matrix case, normality means commutativity is preserved, to the extent possible, in the noncommutative setting. This makes normal operators, and normal elements of C*-algebras, more amenable to analysis.
Special cases.
Among complex matrices, all unitary, Hermitian, and skew-Hermitian matrices are normal. Likewise, among real matrices, all orthogonal, symmetric, and skew-symmetric matrices are normal. However, it is "not" the case that all normal matrices are either unitary or (skew-)Hermitian. For example,
is neither unitary, Hermitian, nor skew-Hermitian, yet it is normal because 
Consequences.
Let be a normal upper triangular matrix. Since , one has i.e. the first row must have the same norm as the first column:
The first entry of row 1 and column 1 are the same, and the rest of column 1 is zero. This implies the first row must be zero for entries 2 through . Continuing this argument for row–column pairs 2 through shows is diagonal.
The concept of normality is important because normal matrices are precisely those to which the spectral theorem applies: 
The diagonal entries of are the eigenvalues of , and the columns of are the eigenvectors of . The matching eigenvalues in come in the same order as the eigenvectors are ordered as columns of .
Another way of stating the spectral theorem is to say that normal matrices are precisely those matrices that can be represented by a diagonal matrix with respect to a properly chosen orthonormal basis of . Phrased differently: a matrix is normal if and only if its eigenspaces span and are pairwise orthogonal with respect to the standard inner product of .
The spectral theorem for normal matrices is a special case of the more general Schur decomposition which holds for all square matrices. Let be a square matrix. Then by Schur decomposition it is unitary similar to an upper-triangular matrix, say, . If is normal, so is . But then must be diagonal, for, as noted above, a normal upper-triangular matrix is diagonal.
The spectral theorem permits the classification of normal matrices in terms of their spectra, for example: 
In general, the sum or product of two normal matrices need not be normal. However, the following holds: 
In this special case, the columns of are eigenvectors of both and and form an orthonormal basis in . This follows by combining the theorems that, over an algebraically closed field, commuting matrices are simultaneously triangularizable and a normal matrix is diagonalizable – the added result is that these can both be done simultaneously.
Equivalent definitions.
It is possible to give a fairly long list of equivalent definitions of a normal matrix. Let be a complex matrix. Then the following are equivalent:
Some but not all of the above generalize to normal operators on infinite-dimensional Hilbert spaces. For example, a bounded operator satisfying (9) is only quasinormal.
Analogy.
It is occasionally useful (but sometimes misleading) to think of the relationships of different kinds of normal matrices as analogous to the relationships between different kinds of complex numbers:
As a special case, the complex numbers may be embedded in the normal real matrices by the mapping 
which preserves addition and multiplication. It is easy to check that this embedding respects all of the above analogies.

</doc>
<doc id="478195" url="https://en.wikipedia.org/wiki?curid=478195" title="Normal mode">
Normal mode

A normal mode of an oscillating system is a pattern of motion in which all parts of the system move sinusoidally with the same frequency and with a fixed phase relation. The free motion described by the normal modes takes place at the fixed frequencies. These fixed frequencies of the normal modes of a system are known as its natural frequencies or resonant frequencies. A physical object, such as a building, bridge or molecule, has a set of normal modes and their natural frequencies that depend on its structure, materials and boundary conditions.
When relating to music, normal modes of vibrating instruments (strings, air pipes, drums, etc.) are called "harmonics" or "overtones".
The most general motion of a system is a superposition of its normal modes. The modes are normal in the sense that they can move independently, that is to say that an excitation of one mode will never cause motion of a different mode. In mathematical terms, normal modes are orthogonal to each other.
The concept of normal modes also finds application in wave theory, optics, quantum mechanics, and molecular dynamics.
Mode numbers.
A mode of vibration is characterized by a modal frequency and a mode shape. It is numbered according to the number of half waves in the vibration. For example, if a vibrating beam with both ends pinned displayed a mode shape of half of a sine wave (one peak on the vibrating beam) it would be vibrating in mode 1. If it had a full sine wave (one peak and one trough) it would be vibrating in mode 2.
In a system with two or more dimensions, such as the pictured disk, each dimension is given a mode number. Using polar coordinates, we have a radial coordinate and an angular coordinate. If you measured from the center outward along the radial coordinate you would encounter a full wave, so the mode number in the radial direction is 2. The other direction is trickier, because only half of the disk is considered due to the antisymmetric (also called skew-symmetry) nature of a disk's vibration in the angular direction. Thus, measuring 180° along the angular direction you would encounter a half wave, so the mode number in the angular direction is 1. So the mode number of the system is 2-1 or 1-2, depending on which coordinate is considered the "first" and which is considered the "second" coordinate (so it is important to always indicate which mode number matches with each coordinate direction).
In linear systems each mode is entirely independent of all other modes. In general all modes have different frequencies (with lower modes having lower frequencies) and different mode shapes.
Nodes.
In a one-dimensional system at a given mode the vibration will have nodes, or places where the displacement is always zero. These nodes correspond to points in the mode shape where the mode shape is zero. Since the vibration of a system is given by the mode shape multiplied by a time function, the displacement of the node points remain zero at all times.
When expanded to a two dimensional system, these nodes become lines where the displacement is always zero. If you watch the animation above you will see two circles (one about half way between the edge and center, and the other on the edge itself) and a straight line bisecting the disk, where the displacement is close to zero. In a real system these lines would equal zero exactly, as shown to the right.
Coupled oscillators.
Consider two equal bodies (not affected by gravity), each of mass "m", attached to three springs, each with spring constant "k". They are attached in the following manner:
where the edge points are fixed and cannot move. We'll use "x"1("t") to denote the horizontal displacement of the left mass, and "x"2("t") to denote the displacement of the right mass.
If we denote acceleration (the second derivative of "x"("t") with respect to time) as formula_1, the equations of motion are:
Since we expect oscillatory motion of a normal mode (where ω is the same for both masses), we try:
Substituting these into the equations of motion gives us:
Since the exponential factor is common to all terms, we omit it and simplify:
And in matrix representation:
For this to be generically true for any amplitude, the matrix on the left must be singular i.e. must not be invertible, such that one cannot multiply both sides of the equation by the inverse, leaving the right matrix equal to zero. It follows that the determinant of the matrix must be equal to 0, so:
Solving for formula_8, we have two positive solutions:
If we substitute ω1 into the matrix and solve for ("A"1, "A"2), we get (1, 1). If we substitute ω2, we get (1, −1). (These vectors are eigenvectors, and the frequencies are eigenvalues.)
The first normal mode is:
Which corresponds to both masses moving in the same direction at the same time.
The second normal mode is:
This corresponds to the masses moving in the opposite directions, while the center of mass remains stationary.
The general solution is a superposition of the normal modes where "c"1, "c"2, φ1, and φ2, are determined by the initial conditions of the problem.
The process demonstrated here can be generalized and formulated using the formalism of Lagrangian mechanics or Hamiltonian mechanics.
Standing waves.
A standing wave is a continuous form of normal mode. In a standing wave, all the space elements (i.e. ("x", "y", "z") coordinates) are oscillating in the same frequency and in phase (reaching the equilibrium point together), but each has a different amplitude.
The general form of a standing wave is:
where "ƒ"("x", "y", "z") represents the dependence of amplitude on location and the cosine\sine are the oscillations in time.
Physically, standing waves are formed by the interference (superposition) of waves and their reflections (although one may also say the opposite; that a moving wave is a superposition of standing waves). The geometric shape of the medium determines what would be the interference pattern, thus determines the "ƒ"("x", "y", "z") form of the standing wave. This space-dependence is called a normal mode.
Usually, for problems with continuous dependence on ("x", "y", "z") there is no single or finite number of normal modes, but there are infinitely many normal modes. If the problem is bounded (i.e. it is defined on a finite section of space) there are countably many (a discrete infinity of ) normal modes (usually numbered "n" = 1, 2, 3, ...). If the problem is not bounded, there is a continuous spectrum of normal modes.
Elastic solids.
See: Einstein solid and Debye model
In any solid at any temperature, the primary particles (e.g. atoms or molecules) are not stationary, but rather vibrate about mean positions. In insulators the capacity of the solid to store thermal energy is due almost entirely to these vibrations. Many physical properties of the solid (e.g. modulus of elasticity) can be predicted given knowledge of the frequencies with which the particles vibrate. The simplest assumption (by Einstein) is that all the particles oscillate about their mean positions with the same natural frequency "ν". This is equivalent to the assumption that all atoms vibrate independently with a frequency "ν". Einstein also assumed that the allowed energy states of these oscillations are harmonics, or integral multiples of "hν". The spectrum of waveforms can be described mathematically using a Fourier series of sinusoidal density fluctuations (or thermal phonons).
Debye subsequently recognized that each oscillator is intimately coupled to its neighboring oscillators at all times. Thus, by replacing Einstein's identical uncoupled oscillators with the same number of coupled oscillators, Debye correlated the elastic vibrations of a one-dimensional solid with the number of mathematically special modes of vibration of a stretched string (see figure). The pure tone of lowest pitch or frequency is referred to as the fundamental and the multiples of that frequency are called its harmonic overtones. He assigned to one of the oscillators the frequency of the fundamental vibration of the whole block of solid. He assigned to the remaining oscillators the frequencies of the harmonics of that fundamental, with the highest of all these frequencies being limited by the motion of the smallest primary unit.
The normal modes of vibration of a crystal are in general superpositions of many overtones, each with an appropriate amplitude and phase. Longer wavelength (low frequency) phonons are exactly those acoustical vibrations which are considered in the theory of sound. Both longitudinal and transverse waves can be propagated through a solid, while, in general, only longitudinal waves are supported by fluids.
In the longitudinal mode, the displacement of particles from their positions of equilibrium coincides with the propagation direction of the wave. Mechanical longitudinal waves have been also referred to as "compression waves". For transverse modes, individual particles move perpendicular to the propagation of the wave.
According to quantum theory, the mean energy of a normal vibrational mode of a crystalline solid with characteristic frequency "υ" is:
The term (1/2)"hυ" represents the "zero-point energy", or the energy which an oscillator will have at absolute zero. "E" ("ν" ) tends to the classic value "kT" at high temperatures
By knowing the thermodynamic formula,
the entropy per normal mode is:
The free energy is:
which, for "kT" » "hν", tends to:
In order to calculate the internal energy and the specific heat, we must know the number of normal vibrational modes a frequency between the values "ν" and "ν" + "dν". Allow this number to be "f" (ν)dν. Since the total number of normal modes is 3"N", the function "f" (ν) is given by:
The integration is performed over all frequencies of the crystal. Then the internal energy "U" will be given by:
Quantum mechanics.
In quantum mechanics, a state formula_21 of a system is described by a wavefunction formula_22 which solves the Schrödinger equation. The square of the absolute value of formula_23, i.e.
is the probability density to measure the particle in place "x" at time "t".
Usually, when involving some sort of potential, the wavefunction is decomposed into a superposition of energy eigenstates, each oscillating with frequency of formula_25. Thus, we may write
The eigenstates have a physical meaning further than an orthonormal basis. When the energy of the system is measured, the wavefunction collapses into one of its eigenstates and so the particle wavefunction is described by the pure eigenstate corresponding to the measured energy.
Earth.
Normal modes are generated in the earth from long wavelength seismic waves from large earthquakes interfering to form standing waves.
For an elastic, isotropic, homogeneous sphere, spheroidal, toroidal and radial (or breathing) modes arise. Spheroidal modes only involve P and SV waves (like Rayleigh waves) and depend on overtone number n and angular order l but have degeneracy of azimuthal order m. Increasing l concentrates fundamental branch closer to surface and at large l this tends to Rayleigh waves. Toroidal modes only involve SH waves (like Love waves) and do not exist in fluid outer core. Radial modes are just a subset of spheroidal modes with l=0. The degeneracy doesn’t exists on Earth as it is broken by rotation, ellipticity and 3D heterogeneous velocity and density structure.
We either assume that each mode can be isolated, the self-coupling approximation, or that many modes close in frequency resonant, the cross-coupling approximation. Self-coupling will change just the phase velocity and not the number of waves around a great circle resulting in a stretching or shrinking of standing wave pattern. Cross-coupling can be caused by rotation of Earth leading to mixing of fundamental spheroidal and toroidal modes, or by aspherical mantle structure or Earth’s ellipticity.

</doc>
<doc id="190167" url="https://en.wikipedia.org/wiki?curid=190167" title="Normal operator">
Normal operator

In mathematics, especially functional analysis, a normal operator on a complex Hilbert space "H" is a continuous linear operator "N" : "H" → "H" that commutes with its hermitian adjoint "N*", that is: "NN*" = "N*N". 
Normal operators are important because the spectral theorem holds for them. The class of normal operators is well-understood. Examples of normal operators are
A normal matrix is the matrix expression of a normal operator on the Hilbert space C"n".
Properties.
Normal operators are characterized by the spectral theorem. A compact normal operator (in particular, a normal operator on a finite-dimensional linear space) is unitarily diagonalizable.
Let "T" be a bounded operator. The following are equivalent.
If "N" is a normal operator, then "N" and "N*" have the same kernel and range. Consequently, the range of "N" is dense if and only if "N" is injective. Put in another way, the kernel of a normal operator is the orthogonal complement of its range. It follows that the kernel of the operator "Nk" coincides with that of "N" for any "k". Every generalized eigenvalue of a normal operator is thus genuine. λ is an eigenvalue of a normal operator "N" if and only if its complex conjugate formula_6 is an eigenvalue of "N*". Eigenvectors of a normal operator corresponding to different eigenvalues are orthogonal, and a normal operator stabilizes the orthogonal complement of each of its eigenspaces. This implies the usual spectral theorem: every normal operator on a finite-dimensional space is diagonalizable by a unitary operator. There is also an infinite-dimensional version of the spectral theorem expressed in terms of projection-valued measures. The residual spectrum of a normal operator is empty.
The product of normal operators that commute is again normal; this is nontrivial, but follows directly from Fuglede's theorem, which states (in a form generalized by Putnam):
The operator norm of a normal operator equals its numerical radius and spectral radius.
A normal operator coincides with its Aluthge transform.
Properties in finite-dimensional case.
If a normal operator "T" on a "finite-dimensional" real or complex Hilbert space (inner product space) "H" stabilizes a subspace "V", then it also stabilizes its orthogonal complement "V"⊥. (This statement is trivial in the case where "T" is self-adjoint.)
"Proof." Let "PV" be the orthogonal projection onto "V". Then the orthogonal projection onto "V"⊥ is 1"H"−"PV". The fact that "T" stabilizes "V" can be expressed as (1"H"−"PV")"TPV" = 0, or "TPV" = "PVTPV". The goal is to show that "X" := "PVT"(1"H"−"PV") = 0. Since ("A", "B") ↦ tr("AB*") is an inner product on the space of endomorphisms of "H", it is enough to show that tr("XX*") = 0. But first we express "XX*" in terms of orthogonal projections:
Now using properties of the trace and of orthogonal projections we have:
The same argument goes through for compact normal operators in infinite dimensional Hilbert spaces, where one make use of the Hilbert-Schmidt inner product, defined by tr("AB*") suitably interpreted. However, for bounded normal operators orthogonal complement to a stable subspace may not be stable. It follows that the Hilbert space cannot be spanned by eigenvectors of such an operator. Consider, for example, the bilateral shift (or two-sided shift) acting on formula_13, which is normal, but has no eigenvalues. 
The invariant subspaces of a shift acting on Hardy space are characterized by Beurling's theorem.
Normal elements of algebras.
The notion of normal operators generalizes to an involutive algebra:
An element "x" of an involutive algebra is said to be normal if "xx*" = "x*x".
Selfadjoint and unitary elements are normal.
The most important case is when such an algebra is a C*-algebra.
Unbounded normal operators.
The definition of normal operators naturally generalizes to some class of unbounded operators. Explicitly, a closed operator "N" is said to be normal if we can write
Here, the existence of the adjoint "N*" requires that the domain of "N" be dense, and the equality includes the assertion that the domain of "N*N" equals that of "NN*", which is not necessarily the case in general.
Equivalently normal operators are precisely those for which
with
The spectral theorem still holds for unbounded (normal) operators. The proofs work by reduction to bounded (normal) operators.
Generalization.
The success of the theory of normal operators led to several attempts for generalization by weakening the commutativity requirement. Classes of operators that include normal operators are (in order of inclusion)

</doc>
<doc id="21538" url="https://en.wikipedia.org/wiki?curid=21538" title="Normed vector space">
Normed vector space

In mathematics, with 2- or 3-dimensional vectors with real-valued entries, the idea of the "length" of a vector is intuitive and can easily be extended to any real vector space R"n". The following properties of "vector length" are crucial.
The generalization of these three properties to more abstract vector spaces leads to the notion of norm. A vector space on which a norm is defined is then called a normed vector space.
Normed vector spaces are central to the study of linear algebra and functional analysis.
Definition.
A normed vector space is a pair ("V", ‖·‖ ) where "V" is a vector space and ‖·‖ a norm on "V".
A seminormed vector space is a pair ("V","p") where "V" is a vector space and "p" a seminorm on "V".
We often omit "p" or ‖·‖ and just write "V" for a space if it is clear from the context what (semi) norm we are using.
In a more general sense, a vector norm can be taken to be any real-valued function that satisfies these three properties. The properties 1. and 2. together imply that
A useful variation of the triangle inequality is
This also shows that a vector norm is a continuous function.
Note that property 2 depends on a choice of norm formula_9 on the field of scalars. When the scalar field is formula_10 (or more generally a subset of formula_11), this is usually taken to be the ordinary absolute value, but other choices are possible. For example, for a vector space over formula_12 one could take formula_9 to be the "p"-adic norm, which gives rise to a different class of normed vector spaces.
Topological structure.
If ("V", ‖·‖) is a normed vector space, the norm ‖·‖ induces a metric (a notion of "distance") and therefore a topology on "V". This metric is defined in the natural way: the distance between two vectors u and v is given by ‖u−v‖. This topology is precisely the weakest topology which makes ‖·‖ continuous and which is compatible with the linear structure of "V" in the following sense:
Similarly, for any semi-normed vector space we can define the distance between two vectors u and v as ‖u−v‖. This turns the seminormed space into a pseudometric space (notice this is weaker than a metric) and allows the definition of notions such as continuity and convergence.
To put it more abstractly every semi-normed vector space is a topological vector space and thus carries a topological structure which is induced by the semi-norm.
Of special interest are complete normed spaces called Banach spaces. Every normed vector space "V" sits as a dense subspace inside a Banach space; this Banach space is essentially uniquely defined by "V" and is called the "completion" of "V".
All norms on a finite-dimensional vector space are equivalent from a topological viewpoint as they induce the same topology (although the resulting metric spaces need not be the same). And since any Euclidean space is complete, we can thus conclude that all finite-dimensional normed vector spaces are Banach spaces. A normed vector space "V" is locally compact if and only if the unit ball "B" = {"x" : ‖"x"‖ ≤ 1} is compact, which is the case if and only if "V" is finite-dimensional; this is a consequence of Riesz's lemma. (In fact, a more general result is true: a topological vector space is locally compact if and only if it is finite-dimensional.
The point here is that we don't assume the topology comes from a norm.)
The topology of a seminormed vector space has many nice properties. Given a neighbourhood system formula_14 around 0 we can construct all other neighbourhood systems as
with
Moreover there exists a neighbourhood basis for 0 consisting of absorbing and convex sets. As this property is very useful in functional analysis, generalizations of normed vector spaces with this property are studied under the name locally convex spaces.
Linear maps and dual spaces.
The most important maps between two normed vector spaces are the continuous linear maps. Together with these maps, normed vector spaces form a category.
The norm is a continuous function on its vector space. All linear maps between finite dimensional vector spaces are also continuous.
An "isometry" between two normed vector spaces is a linear map "f" which preserves the norm (meaning ‖"f"(v)‖ = ‖v‖ for all vectors v). Isometries are always continuous and injective. A surjective isometry between the normed vector spaces "V" and "W" is called an "isometric isomorphism", and "V" and "W" are called "isometrically isomorphic". Isometrically isomorphic normed vector spaces are identical for all practical purposes.
When speaking of normed vector spaces, we augment the notion of dual space to take the norm into account. The dual "V" ' of a normed vector space "V" is the space of all "continuous" linear maps from "V" to the base field (the complexes or the reals) — such linear maps are called "functionals". The norm of a functional φ is defined as the supremum of |φ(v)| where v ranges over all unit vectors (i.e. vectors of norm 1) in "V". This turns "V" ' into a normed vector space. An important theorem about continuous linear functionals on normed vector spaces is the Hahn–Banach theorem.
Normed spaces as quotient spaces of seminormed spaces.
The definition of many normed spaces (in particular, Banach spaces) involves a seminorm defined on a vector space and then the normed space is defined as the quotient space by the subspace of elements of seminorm zero. For instance, with the L"p" spaces, the function defined by
is a seminorm on the vector space of all functions on which the Lebesgue integral on the right hand side is defined and finite. However, the seminorm is equal to zero for any function supported on a set of Lebesgue measure zero. These functions form a subspace which we "quotient out", making them equivalent to the zero function.
Finite product spaces.
Given "n" seminormed spaces "X""i" with seminorms "q""i" we can define the product space as
with vector addition defined as
and scalar multiplication defined as
We define a new function "q"
for example as
which is a seminorm on "X". The function "q" is a norm if and only if all "q""i" are norms.
More generally, for each real "p"≥1 we have the seminorm:
For each p this defines the same topological space.
A straightforward argument involving elementary linear algebra shows that the only finite-dimensional seminormed spaces are those arising as the product space of a normed space and a space with trivial seminorm. Consequently, many of the more interesting examples and applications of seminormed spaces occur for infinite-dimensional vector spaces.

</doc>
<doc id="3043836" url="https://en.wikipedia.org/wiki?curid=3043836" title="Nuclear binding energy">
Nuclear binding energy

Nuclear binding energy is the energy that would be required to disassemble the nucleus of an atom into its component parts. These component parts are neutrons and protons, which are collectively called nucleons. The binding energy of nuclei is due to the attractive forces that hold these nucleons together and this is usually a positive number, since most nuclei would require the expenditure of energy to separate them into individual protons and neutrons. The mass of an atomic nucleus is usually less than the sum of the individual masses of the protons and neutrons (according to Einstein's equation E=mc2) and this 'missing mass' is known as the mass defect, and represents the energy that was released when the nucleus was formed.
The term nuclear binding energy may also refer to the energy balance in processes in which the nucleus splits into fragments composed of more than one nucleon. If new binding energy is available when light nuclei fuse, or when heavy nuclei split, either process can result in release of this binding energy. This energy may be made available as "nuclear energy" and can be used to produce electricity as in (nuclear power) or in a nuclear weapon. When a large nucleus splits into pieces, excess energy is emitted as photons (gamma rays) and as the kinetic energy of a number of different ejected particles (nuclear fission products).
The nuclear binding energies and forces are on the order of a million times greater than the electron binding energies of light atoms like hydrogen.
The mass defect of a nucleus represents the mass of the energy of binding of the nucleus, and is the difference between the mass of a nucleus and the sum of the masses of the nucleons of which it is composed.
Introduction.
Nuclear binding energy is explained by the basic principles involved in nuclear physics.
Nuclear energy.
An absorption or release of nuclear energy occurs in nuclear reactions or radioactive decay; those that absorb energy are called endothermic reactions and those that release energy are exothermic reactions. Energy is consumed or liberated because of differences in the nuclear binding energy between the incoming and outgoing products of the nuclear transmutation.
The best-known classes of exothermic nuclear transmutations are fission and fusion. Nuclear energy may be liberated by atomic fission, when heavy atomic nuclei (like uranium and plutonium) are broken apart into lighter nuclei. The energy from fission is used to generate electric power in hundreds of locations worldwide. Nuclear energy is also released during atomic fusion, when light nuclei like hydrogen are combined to form heavier nuclei such as helium. The Sun and other stars use nuclear fusion to generate thermal energy which is later radiated from the surface, a type of stellar nucleosynthesis. In any exothermic nuclear process, nuclear mass might ultimately be converted to thermal energy, given off as heat.
In order to quantify the energy released or absorbed in any nuclear transmutation, one must know the nuclear binding energies of the nuclear components involved in the transmutation.
The nuclear force.
Electrons and nuclei are kept together by electrostatic attraction (negative attracts positive). Furthermore, electrons are sometimes shared by neighboring atoms or transferred to them (by processes of quantum physics), and this link between atoms is referred to as a chemical bond, and is responsible for the formation of all chemical compounds.
The force of electric attraction does not hold nuclei together, because all protons carry a positive charge and repel each other. Thus, electric forces do not hold nuclei together, because they act in the opposite direction. It has been established that binding neutrons to nuclei clearly requires a non-electrical attraction.
Therefore, another force, called the "nuclear force" (or "residual strong force") holds the nucleons of nuclei together. This force is a residuum of the strong interaction, which binds quarks into nucleons at an even smaller level of distance.
The nuclear force must be stronger than the electric repulsion at short distances, but weaker far away, or else different nuclei might tend to clump together. Therefore it has short-range characteristics. An analogy to the nuclear force is the force between two small magnets: magnets are very difficult to separate when stuck together, but once pulled a short distance apart, the force between them drops almost to zero.
Unlike gravity or electrical forces, the nuclear force is effective only at very short distances. At greater distances, the electrostatic force dominates: the protons repel each other because they are positively charged, and like charges repel. For that reason, the protons forming the nuclei of ordinary hydrogen—for instance, in a balloon filled with hydrogen—do not combine to form helium (a process that also would require some protons to combine with electrons and become neutrons). They cannot get close enough for the nuclear force, which attracts them to each other, to become important. Only under conditions of extreme pressure and temperature (for example, within the core of a star), can such a process take place.
Physics of nuclei.
The nuclei of atoms are found in many different sizes. In hydrogen they contain just one proton, in deuterium or heavy hydrogen a proton and a neutron; in helium, two protons and two neutrons, and in carbon, nitrogen and oxygen - six, seven and eight of each particle, respectively. A helium nucleus weighs less than the sum of the weights of its components. The same phenomenon is found for carbon, nitrogen and oxygen. For example, the carbon nucleus is slightly lighter than three helium nuclei, which can combine to make a carbon nucleus. This illustrates the mass defect.
Mass defect.
The "mass defect" can be explained using Albert Einstein's formula "E" = "m" "c"2, expressing the equivalence of energy and mass. By this formula, adding energy also increases mass (both weight and inertia), whereas removing energy decreases mass.
If a combination of particles contains extra energy—for instance, in a molecule of the explosive TNT—weighing it reveals some extra mass, compared to its end products after an explosion. (The weighing must be done after the products have been stopped and cooled, however, as the extra mass must escape from the system as heat before its loss can be noticed, in theory.) On the other hand, if one must inject energy to separate a system of particles into its components, then the initial weight is less than that of the components after they are separated. In the latter case, the energy injected is "stored" as potential energy, which shows as the increased mass of the components that store it. This is an example of the fact that energy of all types is seen in systems as mass, since mass and energy are equivalent, and each is a "property" of the other.
The latter scenario is the case with nuclei such as helium: to break them up into protons and neutrons, one must inject energy. On the other hand, if a process existed going in the opposite direction, by which hydrogen atoms could be combined to form helium, then energy would be released. The energy can be computed using "E" = Δ"m" "c"2 for each nucleus, where Δ"m" is the difference between the mass of the helium nucleus and the mass of four protons (plus two electrons, absorbed to create the neutrons of helium).
For elements heavier than oxygen, the energy that can be released by assembling them from lighter elements decreases, up to iron. For nuclei heavier than iron, one actually releases energy by breaking them up into 2 fragments. That is how energy is extracted by breaking up uranium nuclei in nuclear power reactors.
The reason the trend reverses after iron is the growing positive charge of the nuclei. The electric force may be weaker than the nuclear force, but its range is greater: in an iron nucleus, each proton repels the other 25 protons, while the nuclear force only binds close neighbors.
As nuclei grow bigger still, this disruptive effect becomes steadily more significant. By the time polonium is reached (84 protons), nuclei can no longer accommodate their large positive charge, but emit their excess protons quite rapidly in the process of alpha radioactivity—the emission of helium nuclei, each containing two protons and two neutrons. (Helium nuclei are an especially stable combination.) Because of this process, nuclei with more than 94 protons are not found naturally on Earth (see periodic table). The isotopes beyond uranium (atomic number 92) with the longest half-lives are plutonium-244 (80 million years) and curium-247 (16 million years).
Solar binding energy.
The nuclear fusion process works as follows: five billion years ago, the new Sun formed when gravity pulled together a vast cloud of gas and dust, from which the Earth and other planets also arose. The gravitational pull released energy and heated the early Sun, much in the way Helmholtz proposed.
Thermal energy appears as the motion of atoms and molecules: the higher the temperature of a collection of particles, the greater is their velocity and the more violent are their collisions. When the temperature at the center of the newly formed Sun became great enough for collisions between nuclei to overcome their electric repulsion, and bring them into the short range of the attractive nuclear force, nuclei began to stick together. When this began to happen, protons combined into deuterium and then helium, with some protons changing in the process to neutrons (plus positrons, positive electrons, which combine with electrons and are destroyed). This released nuclear energy now keeps up the high temperature of the Sun's core, and the heat also keeps the gas pressure high, keeping the Sun at its present size, and stopping gravity from compressing it any more. There is now a stable balance between gravity and pressure.
Different nuclear reactions may predominate at different stages of the Sun's existence, including the proton-proton reaction and the carbon-nitrogen cycle—which involves heavier nuclei, but whose final product is still the combination of protons to form helium.
A branch of physics, the study of controlled nuclear fusion, has tried since the 1950s to derive useful power from nuclear fusion reactions that combine small nuclei into bigger ones, typically to heat boilers, whose steam could turn turbines and produce electricity. Unfortunately, no earthly laboratory can match one feature of the solar powerhouse: the great mass of the Sun, whose weight keeps the hot plasma compressed and confines the nuclear furnace to the Sun's core. Instead, physicists use strong magnetic fields to confine the plasma, and for fuel they use heavy forms of hydrogen, which burn more easily. Magnetic traps can be rather unstable, and any plasma hot enough and dense enough to undergo nuclear fusion tends to slip out of them after a short time. Even with ingenious tricks, the confinement in most cases lasts only a small fraction of a second.
Combining nuclei.
Small nuclei that are larger than hydrogen can combine into bigger ones and release energy, but in combining such nuclei, the amount of energy released is much smaller compared to hydrogen fusion. The reason is that while the overall process releases energy from letting the nuclear attraction do its work, energy must first be injected to force together positively charged protons, which also repel each other with their electric charge.
For elements that weigh more than iron (a nucleus with 26 protons), the fusion process no longer releases energy. In even heavier nuclei energy is consumed, not released, by combining similar sized nuclei. With such large nuclei, overcoming the electric repulsion (which affects all protons in the nucleus) requires more energy than what is released by the nuclear attraction (which is effective mainly between close neighbors). Conversely, energy could actually be released by breaking apart nuclei heavier than iron.
With the nuclei of elements heavier than lead, the electric repulsion is so strong that some of them spontaneously eject positive fragments, usually nuclei of helium that form very stable combinations (alpha particles). This spontaneous break-up is one of the forms of radioactivity exhibited by some nuclei.
Nuclei heavier than lead (except for bismuth, thorium, uranium, and plutonium) spontaneously break up too quickly to appear in nature as primordial elements, though they can be produced artificially or as intermediates in the decay chains of lighter elements. Generally, the heavier the nuclei are, the faster they spontaneously decay.
Iron nuclei are the most stable nuclei (in particular iron-56), and the best sources of energy are therefore nuclei whose weights are as far removed from iron as possible. One can combine the lightest ones—nuclei of hydrogen (protons)—to form nuclei of helium, and that is how the Sun generates its energy. Or else one can break up the heaviest ones—nuclei of uranium or plutonium—into smaller fragments, and that is what nuclear power reactors do.
Nuclear binding energy.
An example that illustrates nuclear binding energy is the nucleus of 12C (Carbon 12), which contains 6 protons and 6 neutrons. The protons are all positively charged and repel each other, but the nuclear force overcomes the repulsion and causes them to stick together. The nuclear force is a close-range force (it is very strongly inversely proportionate to distance), and virtually no effect of this force is observed outside the nucleus. The nuclear force also pulls neutrons together, or neutrons and protons.
The energy of the nucleus is negative with regard to the energy of the particles pulled apart to infinite distance (just like the gravitational energy of planets of the solar system), because energy must be utilized to split a nucleus into its individual protons and neutrons. Mass spectrometers have measured the masses of nuclei, which are always less than the sum of the masses of protons and neutrons that form them, and the difference—by the formula "E" = "m" "c"2—gives the binding energy of the nucleus.
Nuclear fusion.
The binding energy of helium is the energy source of the Sun and of most stars. The sun is composed of 74 percent hydrogen (measured by mass), an element whose nucleus is a single proton. Energy is released in the sun when 4 protons combine into a helium nucleus, a process in which two of them are also converted to neutrons.
The conversion of protons to neutrons is the result of another nuclear force, known as the weak (nuclear) force. The weak force, like the strong force, has a short range, but is much weaker than the strong force. The weak force tries to make the number of neutrons and protons into the most energetically stable configuration. For nuclei containing less than 40 particles, these numbers are usually about equal. Protons and neutrons are closely related and are sometimes collectively known as nucleons. As the number of particles increases toward a maximum of about 209, the number of neutrons to maintain stability begins to outstrip the number of protons, until the ratio of neutrons to protons is about three to two.
The protons of hydrogen combine to helium only if they have enough velocity to overcome each other's mutual repulsion sufficiently to get within range of the strong nuclear attraction. This means that fusion only occurs within a very hot gas. Hydrogen hot enough for combining to helium requires an enormous pressure to keep it confined, but suitable conditions exist in the central regions of the Sun, where such pressure is provided by the enormous weight of the layers above the core, pressed inwards by the Sun's strong gravity. The process of combining protons to form helium is an example of nuclear fusion.
The earth's oceans contain a large amount of hydrogen that could theoretically be used for fusion, and helium byproduct of fusion does not harm the environment, so some consider nuclear fusion a good alternative to supply humanity's energy needs. Experiments to generate electricity from fusion have so far have only partially succeeded. Sufficiently hot hydrogen must be ionized and confined. One technique is to use very strong magnetic fields, because charged particles (like those trapped in the Earth's radiation belt) are guided by magnetic field lines. Fusion experiments also rely on heavy hydrogen, which fuses more easily, and gas densities can be moderate. But even with these techniques far more net energy is consumed by the fusion experiments than is yielded by the process.
The binding energy maximum and ways to approach it by decay.
In the main isotopes of light nuclei, such as carbon, nitrogen and oxygen, the most stable combination of neutrons and of protons are when the numbers are equal (this continues to element 20, calcium). However, in heavier nuclei, the disruptive energy of protons increases, since they are confined to a tiny volume and repel each other. The energy of the strong force holding the nucleus together also increases, but at a slower rate, as if inside the nucleus, only nucleons close to each other are tightly bound, not ones more widely separated.
The net binding energy of a nucleus is that of the nuclear attraction, minus the disruptive energy of the electric force. As nuclei get heavier than helium, their net binding energy per nucleon (deduced from the difference in mass between the nucleus and the sum of masses of component nucleons) grows more and more slowly, reaching its peak at iron. As nucleons are added, the total nuclear binding energy always increases—but the total disruptive energy of electric forces (positive protons repelling other protons) also increases, and past iron, the second increase outweighs the first. Iron-56 (56Fe) is the most efficiently bound nucleus meaning that it has the least average mass per nucleon. However, nickel-62 is the most tightly bound nucleus in terms of energy of binding per nucleon. (Nickel-62's higher energy of binding does not translate to a larger mean mass loss than Fe-56, because Ni-62 has a slightly higher ratio of neutrons/protons than does iron-56, and the presence of the heavier neutrons increases nickel-62's average mass per nucleon).
To reduce the disruptive energy, the weak interaction allows the number of neutrons to exceed that of protons—for instance, the main isotope of iron has 26 protons and 30 neutrons. Isotopes also exist where the number of neutrons differs from the most stable number for that number of nucleons. If the ratio of protons to neutrons is too far from stability, nucleons may spontaneously change from proton to neutron, or neutron to proton.
The two methods for this conversion are mediated by the weak force, and involve types of beta decay. In the simplest beta decay, neutrons are converted to protons by emitting a negative electron and an antineutrino. This is always possible outside a nucleus because neutrons are more massive than protons by an equivalent of about 2.5 electrons. In the opposite process, which only happens within a nucleus, and not to free particles, a proton may become a neutron by ejecting a positron. This is permitted if enough energy is available between parent and daughter nuclides to do this (the required energy difference is equal to 1.022 MeV, which is the mass of 2 electrons). If the mass difference between parent and daughter is less than this, a proton-rich nucleus may still convert protons to neutrons by the process of electron capture, in which a proton simply electron captures one of the atom's K orbital electrons, emits a neutrino, and becomes a neutron.
Among the heaviest nuclei, starting with tellurium nuclei (element 52) containing 106 or more nucleons, electric forces may be so destabilizing that entire chunks of the nucleus may be ejected, usually as alpha particles, which consist of two protons and two neutrons (alpha particles are fast helium nuclei). (Beryllium-8 also decays, very quickly, into two alpha particles.) Alpha particles are extremely stable. This type of decay becomes more and more probable as elements rise in atomic weight past 106.
The curve of binding energy is a graph that plots the binding energy per nucleon against atomic mass. This curve has its main peak at iron and nickel and then slowly decreases again, and also a narrow isolated peak at helium, which as noted is very stable. The heaviest nuclei in nature, uranium 238U, are unstable, but having a lifetime of 4.5 billion years, close to the age of the Earth, they are still relatively abundant; they (and other nuclei heavier than iron) may have formed in a supernova explosion preceding the formation of the solar system. The most common isotope of thorium, 232Th, also undergoes α particle emission, and its half-life (time over which half a number of atoms decays) is even longer, by several times. In each of these, radioactive decay produces daughter isotopes that are also unstable, starting a chain of decays that ends in some stable isotope of lead.
Determining nuclear binding energy.
Calculation can be employed to determine the nuclear binding energy of nuclei. The calculation involves determining the "mass defect", converting it into energy, and expressing the result as energy per mole of atoms, or as energy per nucleon.
Conversion of mass defect into energy.
Mass defect is defined as the difference between the mass of a nucleus, and the sum of the masses of the nucleons of which it is composed. The mass defect is determined by calculating three quantities. These are: the actual mass of the nucleus, the composition of the nucleus (number of protons and of neutrons), and the masses of a proton and of a neutron. This is then followed by converting the mass defect into energy. This quantity is the nuclear binding energy, however it must be expressed as energy per mole of atoms or as energy per nucleon.
Fission and fusion.
Nuclear energy is released by the splitting (fission) or merging (fusion) of the nuclei of atom(s). The conversion of nuclear mass-energy to a form of energy, which can remove some mass when the energy is removed, is consistent with the mass-energy equivalence formula:
Δ"E" = Δ"m" "c"2, 
in which, 
Δ"E" = energy release, 
Δ"m" = mass defect, 
and "c" = the speed of light in a vacuum (a physical constant).
Nuclear energy was first discovered by French physicist Henri Becquerel in 1896, when he found that photographic plates stored in the dark near uranium were blackened like X-ray plates (X-rays had recently been discovered in 1895).
Nuclear chemistry can be used as a form of alchemy to turn lead into gold or change any atom to any other atom (though this may require many intermediate steps). Radionuclide (radioisotope) production often involves irradiation of another isotope (or more precisely a nuclide), with alpha particles, beta particles, or gamma rays. 
Nickel-62 has the highest binding energy per nucleon of any isotope. If an atom of lower average binding energy is changed into two atoms of higher average binding energy, energy is given off. Also, if two atoms of lower average binding energy fuse into an atom of higher average binding energy, energy is given off. The chart shows that fusion of hydrogen, the combination to form heavier atoms, releases energy, as does fission of uranium, the breaking up of a larger nucleus into smaller parts. Stability varies between isotopes: the isotope U-235 is much less stable than the more common U-238.
Nuclear energy is released by three "exoenergetic" (or exothermic) processes:
Binding energy for atoms.
The binding energy of an atom (including its electrons) is not the same as the binding energy of the atom's nucleus. The measured mass deficits of isotopes are always listed as mass deficits of the neutral atoms of that isotope, and mostly in MeV. As a consequence, the listed mass deficits are not a measure for the stability or binding energy of isolated nuclei, but for the whole atoms. This has very practical reasons, because it is very hard to totally ionize heavy elements, i.e. strip them of all of their electrons.
This practice is useful for other reasons, too: Stripping all the electrons from a heavy unstable nucleus (thus producing a bare nucleus) changes the lifetime of the nucleus, indicating that the nucleus cannot be treated independently (Experiments at the heavy ion accelerator GSI). This is also evident from phenomena like electron capture. Theoretically, in orbital models of heavy atoms, the electron orbits partially inside the nucleus (it doesn't "orbit" in a strict sense, but has a non-vanishing probability of being located inside the nucleus).
A nuclear decay happens to the nucleus, meaning that properties ascribed to the nucleus change in the event. In the field of physics the concept of "mass deficit" as a measure for "binding energy" means "mass deficit of the neutral atom" (not just the nucleus) and is a measure for stability of the whole atom.
Nuclear binding energy curve.
In the periodic table of elements, the series of light elements from hydrogen up to sodium is observed to exhibit generally increasing binding energy per nucleon as the atomic mass increases. This increase is generated by increasing forces per nucleon in the nucleus, as each additional nucleon is attracted by other nearby nucleons, and thus more tightly bound to the whole.
The region of increasing binding energy is followed by a region of relative stability (saturation) in the sequence from magnesium through xenon. In this region, the nucleus has become large enough that nuclear forces no longer completely extend efficiently across its width. Attractive nuclear forces in this region, as atomic mass increases, are nearly balanced by repellent electromagnetic forces between protons, as the atomic number increases.
Finally, in elements heavier than xenon, there is a decrease in binding energy per nucleon as atomic number increases. In this region of nuclear size, electromagnetic repulsive forces are beginning to overcome the strong nuclear force attraction.
At the peak of binding energy, nickel-62 is the most tightly bound nucleus (per nucleon), followed by iron-58 and iron-56. This is the approximate basic reason why iron and nickel are very common metals in planetary cores, since they are produced profusely as end products in supernovae and in the final stages of silicon burning in stars. However, it is not binding energy per defined nucleon (as defined above), which controls which exact nuclei are made, because within stars, neutrons are free to convert to protons to release even more energy, per generic nucleon, if the result is a stable nucleus with a larger fraction of protons. In fact, it has been argued that photodisintegration of 62Ni to form 56Fe may be energetically possible in an extremely hot star core, due to this beta decay conversion of neutrons to protons. The conclusion is that at the pressure and temperature conditions in the cores of large stars, energy is released by converting all matter into 56Fe nuclei (ionized atoms). (However, at high temperatures not all matter will be in the lowest energy state.) This energetic maximum should also hold for ambient conditions, say "T" = 298 K and "p" = 1 atm, for neutral condensed matter consisting of 56Fe atoms—however, in these conditions nuclei of atoms are inhibited from fusing into the most stable and low energy state of matter.
It is generally believed that iron-56 is more common than nickel isotopes in the universe for mechanistic reasons, because its unstable progenitor nickel-56 is copiously made by staged build-up of 14 helium nuclei inside supernovas, where it has no time to decay to iron before being released into the interstellar medium in a matter of a few minutes, as the supernova explodes. However, nickel-56 then decays to cobalt-56 within a few weeks, then this radioisotope finally decays to iron-56 with a half life of about 77.3 days. The radioactive decay-powered light curve of such a process has been observed to happen in type II supernovae, such as SN 1987A. In a star, there are no good ways to create nickel-62 by alpha-addition processes, or else there would presumably be more of this highly stable nuclide in the universe.
Measuring the binding energy.
The fact that the maximum binding energy is found in medium-sized nuclei is a consequence of the trade-off in the effects of two opposing forces that have different range characteristics. The attractive nuclear force (strong nuclear force), which binds protons and neutrons equally to each other, has a limited range due to a rapid exponential decrease in this force with distance. However, the repelling electromagnetic force, which acts between protons to force nuclei apart, falls off with distance much more slowly (as the inverse square of distance). For nuclei larger than about four nucleons in diameter, the additional repelling force of additional protons more than offsets any binding energy that results between further added nucleons as a result of additional strong force interactions. Such nuclei become increasingly less tightly bound as their size increases, though most of them are still stable. Finally, nuclei containing more than 209 nucleons (larger than about 6 nucleons in diameter) are all too large to be stable, and are subject to spontaneous decay to smaller nuclei.
Nuclear fusion produces energy by combining the very lightest elements into more tightly bound elements (such as hydrogen into helium), and nuclear fission produces energy by splitting the heaviest elements (such as uranium and plutonium) into more tightly bound elements (such as barium and krypton). Both processes produce energy, because middle-sized nuclei are the most tightly bound of all.
As seen above in the example of deuterium, nuclear binding energies are large enough that they may be easily measured as fractional mass deficits, according to the equivalence of mass and energy. The atomic binding energy is simply the amount of energy (and mass) released, when a collection of free nucleons are joined together to form a nucleus.
Nuclear binding energy can be computed from the difference in mass of a nucleus, and the sum of the masses of the number of free neutrons and protons that make up the nucleus. Once this mass difference, called the mass defect or mass deficiency, is known, Einstein's mass-energy equivalence formula "E" = "mc"² can be used to compute the binding energy of any nucleus. Early nuclear physicists used to refer to computing this value as a "packing fraction" calculation.
For example, the atomic mass unit (1 u) is defined as 1/12 of the mass of a 12C atom—but the atomic mass of a 1H atom (which is a proton plus electron) is 1.007825 "u", so each nucleon in 12C has lost, on average, about 0.8% of its mass in the form of binding energy.
Semiempirical formula for nuclear binding energy.
For a nucleus with "A" nucleons, including "Z" protons and "N" neutrons, a semi-empirical formula for the binding energy (BE) per nucleon is:
where the coefficients are given by: formula_2; formula_3; formula_4; formula_5; formula_6.
The first term formula_7 is called the saturation contribution and ensures that the binding energy per nucleon is the same for all nuclei to a first approximation. The term formula_8 is a surface tension effect and is proportional to the number of nucleons that are situated on the nuclear surface; it is largest for light nuclei. The term formula_9 is the Coulomb electrostatic repulsion; this becomes more important as formula_10 increases. The symmetry correction term formula_11 takes into account the fact that in the absence of other effects the most stable arrangement has equal numbers of protons and neutrons; this is because the n-p interaction in a nucleus is stronger than either the n-n or p-p interaction. The pairing term formula_12 is purely empirical; it is + for even-even nuclei and - for odd-odd nuclei.
Example values deduced from experimentally measured atom nuclide masses.
The following table lists some binding energies and mass defect values. Notice also that we use 1 u = (931.494028 ± 0.000023) MeV. To calculate the binding energy we use the formula "Z" ("m"p + "m"e) + "N" "m"n - "m"nuclide where "Z" denotes the number of protons in the nuclides and "N" their number of neutrons. We take
"m"p = 938.2723 MeV, "m"e = 0.5110 MeV and "m"n = 939.5656 MeV. The letter "A" denotes the sum of "Z" and "N" (number of nucleons in the nuclide). If we assume the reference nucleon has the mass of a neutron (so that all "total" binding energies calculated are maximal) we could define the total binding energy as the difference from the mass of the nucleus, and the mass of a collection of "A" free neutrons. In other words, it would be ("Z" + "N") "m"n - "m"nuclide. The ""total" binding energy per nucleon" would be this value divided by "A".
56Fe has the lowest nucleon-specific mass of the four nuclides listed in this table, but this does not imply it is the strongest bound atom per hadron, unless the choice of beginning hadrons is completely free. Iron releases the largest energy if any 56 nucleons are allowed to build a nuclide—changing one to another if necessary, The highest binding energy per hadron, with the hadrons starting as the same number of protons "Z" and total nucleons "A" as in the bound nucleus, is 62Ni. Thus, the true absolute value of the total binding energy of a nucleus depends on what we are allowed to construct the nucleus out of. If all nuclei of mass number "A" were to be allowed to be constructed of "A" neutrons, then 56Fe would release the most energy per nucleon, since it has a larger fraction of protons than 62Ni. However, if nucleons are required to be constructed of only the same number of protons and neutrons that they contain, then nickel-62 is the most tightly bound nucleus, per nucleon.
In the table above it can be seen that the decay of a neutron, as well as the transformation of tritium into helium-3, releases energy; hence, it manifests a stronger bound new state when measured against the mass of an equal number of neutrons (and also a lighter state per number of total hadrons). Such reactions are not driven by changes in binding energies as calculated from previously fixed "N" and "Z" numbers of neutrons and protons, but rather in decreases in the total mass of the nuclide/per nucleon, with the reaction. (Note that the Binding Energy given above for hydrogen-1 is the atomic binding energy, not the nuclear binding energy which would be zero.)

</doc>
<doc id="22054" url="https://en.wikipedia.org/wiki?curid=22054" title="Nuclear fission">
Nuclear fission

In nuclear physics and nuclear chemistry, nuclear fission is either a nuclear reaction or a radioactive decay process in which the nucleus of an atom splits into smaller parts (lighter nuclei). The fission process often produces free neutrons and photons (in the form of gamma rays), and releases a very large amount of energy even by the energetic standards of radioactive decay.
Nuclear fission of heavy elements was discovered on December 17, 1938 by German Otto Hahn and his assistant Fritz Strassmann, and explained theoretically in January 1939 by Lise Meitner and her nephew Otto Robert Frisch. Frisch named the process by analogy with biological fission of living cells. It is an exothermic reaction which can release large amounts of energy both as electromagnetic radiation and as kinetic energy of the fragments (heating the bulk material where fission takes place). In order for fission to produce energy, the total binding energy of the resulting elements must be less negative (higher energy) than that of the starting element.
Fission is a form of nuclear transmutation because the resulting fragments are not the same element as the original atom. The two nuclei produced are most often of comparable but slightly different sizes, typically with a mass ratio of products of about 3 to 2, for common fissile isotopes. Most fissions are binary fissions (producing two charged fragments), but occasionally (2 to 4 times per 1000 events), "three" positively charged fragments are produced, in a ternary fission. The smallest of these fragments in ternary processes ranges in size from a proton to an argon nucleus.
Apart from fission induced by a neutron, harnessed and exploited by humans, a natural form of spontaneous radioactive decay (not requiring a neutron) is also referred to as fission, and occurs especially in very high-mass-number isotopes. Spontaneous fission was discovered in 1940 by Flyorov, Petrzhak and Kurchatov in Moscow, when they decided to confirm that, without bombardment by neutrons, the fission rate of uranium was indeed negligible, as predicted by Niels Bohr; it wasn't.
The unpredictable composition of the products (which vary in a broad probabilistic and somewhat chaotic manner) distinguishes fission from purely quantum-tunnelling processes such as proton emission, alpha decay and cluster decay, which give the same products each time. Nuclear fission produces energy for nuclear power and drives the explosion of nuclear weapons. Both uses are possible because certain substances called nuclear fuels undergo fission when struck by fission neutrons, and in turn emit neutrons when they break apart. This makes possible a self-sustaining nuclear chain reaction that releases energy at a controlled rate in a nuclear reactor or at a very rapid uncontrolled rate in a nuclear weapon.
The amount of free energy contained in nuclear fuel is millions of times the amount of free energy contained in a similar mass of chemical fuel such as gasoline, making nuclear fission a very dense source of energy. The products of nuclear fission, however, are on average far more radioactive than the heavy elements which are normally fissioned as fuel, and remain so for significant amounts of time, giving rise to a nuclear waste problem. Concerns over nuclear waste accumulation and over the destructive potential of nuclear weapons may counterbalance the desirable qualities of fission as an energy source, and give rise to ongoing political debate over nuclear power.
Physical overview.
Mechanism.
Nuclear fission can occur without neutron bombardment as a type of radioactive decay. This type of fission (called spontaneous fission) is rare except in a few heavy isotopes. In engineered nuclear devices, essentially all nuclear fission occurs as a "nuclear reaction" — a bombardment-driven process that results from the collision of two subatomic particles. In nuclear reactions, a subatomic particle collides with an atomic nucleus and causes changes to it. Nuclear reactions are thus driven by the mechanics of bombardment, not by the relatively constant exponential decay and half-life characteristic of spontaneous radioactive processes.
Many types of nuclear reactions are currently known. Nuclear fission differs importantly from other types of nuclear reactions, in that it can be amplified and sometimes controlled via a nuclear chain reaction (one type of general chain reaction). In such a reaction, free neutrons released by each fission event can trigger yet more events, which in turn release more neutrons and cause more fissions.
The chemical element isotopes that can sustain a fission chain reaction are called nuclear fuels, and are said to be "fissile". The most common nuclear fuels are 235U (the isotope of uranium with an atomic mass of 235 and of use in nuclear reactors) and 239Pu (the isotope of plutonium with an atomic mass of 239). These fuels break apart into a bimodal range of chemical elements with atomic masses centering near 95 and 135 u (fission products). Most nuclear fuels undergo spontaneous fission only very slowly, decaying instead mainly via an alpha/beta decay chain over periods of millennia to eons. In a nuclear reactor or nuclear weapon, the overwhelming majority of fission events are induced by bombardment with another particle, a neutron, which is itself produced by prior fission events.
Nuclear fissions in fissile fuels are the result of the nuclear excitation energy produced when a fissile nucleus captures a neutron. This energy, resulting from the neutron capture, is a result of the attractive nuclear force acting between the neutron and nucleus. It is enough to deform the nucleus into a double-lobed "drop," to the point that nuclear fragments exceed the distances at which the nuclear force can hold two groups of charged nucleons together, and when this happens, the two fragments complete their separation and then are driven further apart by their mutually repulsive charges, in a process which becomes irreversible with greater and greater distance. A similar process occurs in fissionable isotopes (such as uranium-238), but in order to fission, these isotopes require additional energy provided by fast neutrons (such as those produced by nuclear fusion in thermonuclear weapons).
The liquid drop model of the atomic nucleus predicts equal-sized fission products as an outcome of nuclear deformation. The more sophisticated nuclear shell model is needed to mechanistically explain the route to the more energetically favorable outcome, in which one fission product is slightly smaller than the other. A theory of the fission based on shell model has been formulated by Maria Goeppert Mayer.
The most common fission process is binary fission, and it produces the fission products noted above, at 95±15 and 135±15 u. However, the binary process happens merely because it is the most probable. In anywhere from 2 to 4 fissions per 1000 in a nuclear reactor, a process called ternary fission produces three positively charged fragments (plus neutrons) and the smallest of these may range from so small a charge and mass as a proton (Z=1), to as large a fragment as argon (Z=18). The most common small fragments, however, are composed of 90% helium-4 nuclei with more energy than alpha particles from alpha decay (so-called "long range alphas" at ~ 16 MeV), plus helium-6 nuclei, and tritons (the nuclei of tritium). The ternary process is less common, but still ends up producing significant helium-4 and tritium gas buildup in the fuel rods of modern nuclear reactors.
Energetics.
Input.
The fission of a heavy nucleus requires a total input energy of about 7 to 8 million electron volts (MeV) to initially overcome the nuclear force which holds the nucleus into a spherical or nearly spherical shape, and from there, deform it into a two-lobed ("peanut") shape in which the lobes are able to continue to separate from each other, pushed by their mutual positive charge, in the most common process of binary fission (two positively charged fission products + neutrons). Once the nuclear lobes have been pushed to a critical distance, beyond which the short range strong force can no longer hold them together, the process of their separation proceeds from the energy of the (longer range) electromagnetic repulsion between the fragments. The result is two fission fragments moving away from each other, at high energy.
About 6 MeV of the fission-input energy is supplied by the simple binding of an extra neutron to the heavy nucleus via the strong force; however, in many fissionable isotopes, this amount of energy is not enough for fission. Uranium-238, for example, has a near-zero fission cross section for neutrons of less than one MeV energy. If no additional energy is supplied by any other mechanism, the nucleus will not fission, but will merely absorb the neutron, as happens when U-238 absorbs slow and even some fraction of fast neutrons, to become U-239. The remaining energy to initiate fission can be supplied by two other mechanisms: one of these is more kinetic energy of the incoming neutron, which is increasingly able to fission a fissionable heavy nucleus as it exceeds a kinetic energy of one MeV or more (so-called fast neutrons). Such high energy neutrons are able to fission U-238 directly (see thermonuclear weapon for application, where the fast neutrons are supplied by nuclear fusion). However, this process cannot happen to a great extent in a nuclear reactor, as too small a fraction of the fission neutrons produced by any type of fission have enough energy to efficiently fission U-238 (fission neutrons have a mode energy of 2 MeV, but a median of only 0.75 MeV, meaning half of them have less than this insufficient energy).
Among the heavy actinide elements, however, those isotopes that have an odd number of neutrons (such as U-235 with 143 neutrons) bind an extra neutron with an additional 1 to 2 MeV of energy over an isotope of the same element with an even number of neutrons (such as U-238 with 146 neutrons). This extra binding energy is made available as a result of the mechanism of neutron pairing effects. This extra energy results from the Pauli exclusion principle allowing an extra neutron to occupy the same nuclear orbital as the last neutron in the nucleus, so that the two form a pair. In such isotopes, therefore, no neutron kinetic energy is needed, for all the necessary energy is supplied by absorption of any neutron, either of the slow or fast variety (the former are used in moderated nuclear reactors, and the latter are used in fast neutron reactors, and in weapons). As noted above, the subgroup of fissionable elements that may be fissioned efficiently with their own fission neutrons (thus potentially causing a nuclear chain reaction in relatively small amounts of the pure material) are termed "fissile." Examples of fissile isotopes are U-235 and plutonium-239.
Output.
Typical fission events release about two hundred million eV (200 MeV) of energy for each fission event. The exact isotope which is fissioned, and whether or not it is fissionable or fissile, has only a small impact on the amount of energy released. This can be easily seen by examining the curve of binding energy (image below), and noting that the average binding energy of the actinide nuclides beginning with uranium is around 7.6 MeV per nucleon. Looking further left on the curve of binding energy, where the fission products cluster, it is easily observed that the binding energy of the fission products tends to center around 8.5 MeV per nucleon. Thus, in any fission event of an isotope in the actinide's range of mass, roughly 0.9 MeV is released per nucleon of the starting element. The fission of U235 by a slow neutron yields nearly identical energy to the fission of U238 by a fast neutron. This energy release profile holds true for thorium and the various minor actinides as well.
By contrast, most chemical oxidation reactions (such as burning coal or TNT) release at most a few eV per event. So, nuclear fuel contains at least ten million times more usable energy per unit mass than does chemical fuel. The energy of nuclear fission is released as kinetic energy of the fission products and fragments, and as electromagnetic radiation in the form of gamma rays; in a nuclear reactor, the energy is converted to heat as the particles and gamma rays collide with the atoms that make up the reactor and its working fluid, usually water or occasionally heavy water or molten salts.
When a uranium nucleus fissions into two daughter nuclei fragments, about 0.1 percent of the mass of the uranium nucleus appears as the fission energy of ~200 MeV. For uranium-235 (total mean fission energy 202.5 MeV), typically ~169 MeV appears as the kinetic energy of the daughter nuclei, which fly apart at about 3% of the speed of light, due to Coulomb repulsion. Also, an average of 2.5 neutrons are emitted, with a mean kinetic energy per neutron of ~2 MeV (total of 4.8 MeV). The fission reaction also releases ~7 MeV in prompt gamma ray photons. The latter figure means that a nuclear fission explosion or criticality accident emits about 3.5% of its energy as gamma rays, less than 2.5% of its energy as fast neutrons (total of both types of radiation ~ 6%), and the rest as kinetic energy of fission fragments (this appears almost immediately when the fragments impact surrounding matter, as simple heat). In an atomic bomb, this heat may serve to raise the temperature of the bomb core to 100 million kelvin and cause secondary emission of soft X-rays, which convert some of this energy to ionizing radiation. However, in nuclear reactors, the fission fragment kinetic energy remains as low-temperature heat, which itself causes little or no ionization.
So-called neutron bombs (enhanced radiation weapons) have been constructed which release a larger fraction of their energy as ionizing radiation (specifically, neutrons), but these are all thermonuclear devices which rely on the nuclear fusion stage to produce the extra radiation. The energy dynamics of pure fission bombs always remain at about 6% yield of the total in radiation, as a prompt result of fission.
The total "prompt fission" energy amounts to about 181 MeV, or ~ 89% of the total energy which is eventually released by fission over time. The remaining ~ 11% is released in beta decays which have various half-lives, but begin as a process in the fission products immediately; and in delayed gamma emissions associated with these beta decays. For example, in uranium-235 this delayed energy is divided into about 6.5 MeV in betas, 8.8 MeV in antineutrinos (released at the same time as the betas), and finally, an additional 6.3 MeV in delayed gamma emission from the excited beta-decay products (for a mean total of ~10 gamma ray emissions per fission, in all). Thus, about 6.5% of the total energy of fission is released some time after the event, as non-prompt or delayed ionizing radiation, and the delayed ionizing energy is about evenly divided between gamma and beta ray energy.
In a reactor that has been operating for some time, the radioactive fission products will have built up to steady state concentrations such that their rate of decay is equal to their rate of formation, so that their fractional total contribution to reactor heat (via beta decay) is the same as these radioisotopic fractional contributions to the energy of fission. Under these conditions, the 6.5% of fission which appears as delayed ionizing radiation (delayed gammas and betas from radioactive fission products) contributes to the steady-state reactor heat production under power. It is this output fraction which remains when the reactor is suddenly shut down (undergoes scram). For this reason, the reactor decay heat output begins at 6.5% of the full reactor steady state fission power, once the reactor is shut down. However, within hours, due to decay of these isotopes, the decay power output is far less. See decay heat for detail.
The remainder of the delayed energy (8.8 MeV/202.5 MeV = 4.3% of total fission energy) is emitted as antineutrinos, which as a practical matter, are not considered "ionizing radiation." The reason is that energy released as antineutrinos is not captured by the reactor material as heat, and escapes directly through all materials (including the Earth) at nearly the speed of light, and into interplanetary space (the amount absorbed is minuscule). Neutrino radiation is ordinarily not classed as ionizing radiation, because it is almost entirely not absorbed and therefore does not produce effects (although the very rare neutrino event is ionizing). Almost all of the rest of the radiation (6.5% delayed beta and gamma radiation) is eventually converted to heat in a reactor core or its shielding.
Some processes involving neutrons are notable for absorbing or finally yielding energy — for example neutron kinetic energy does not yield heat immediately if the neutron is captured by a uranium-238 atom to breed plutonium-239, but this energy is emitted if the plutonium-239 is later fissioned. On the other hand, so-called delayed neutrons emitted as radioactive decay products with half-lives up to several minutes, from fission-daughters, are very important to reactor control, because they give a characteristic "reaction" time for the total nuclear reaction to double in size, if the reaction is run in a "delayed-critical" zone which deliberately relies on these neutrons for a supercritical chain-reaction (one in which each fission cycle yields more neutrons than it absorbs). Without their existence, the nuclear chain-reaction would be prompt critical and increase in size faster than it could be controlled by human intervention. In this case, the first experimental atomic reactors would have run away to a dangerous and messy "prompt critical reaction" before their operators could have manually shut them down (for this reason, designer Enrico Fermi included radiation-counter-triggered control rods, suspended by electromagnets, which could automatically drop into the center of Chicago Pile-1). If these delayed neutrons are captured without producing fissions, they produce heat as well.
Product nuclei and binding energy.
In fission there is a preference to yield fragments with even proton numbers, which is called the odd-even effect on the fragments charge distribution. However, no odd-even effect is observed on fragment mass number distribution. This result is attributed to nucleon pair breaking.
In nuclear fission events the nuclei may break into any combination of lighter nuclei, but the most common event is not fission to equal mass nuclei of about mass 120; the most common event (depending on isotope and process) is a slightly unequal fission in which one daughter nucleus has a mass of about 90 to 100 u and the other the remaining 130 to 140 u. Unequal fissions are energetically more favorable because this allows one product to be closer to the energetic minimum near mass 60 u (only a quarter of the average fissionable mass), while the other nucleus with mass 135 u is still not far out of the range of the most tightly bound nuclei (another statement of this, is that the atomic binding energy curve is slightly steeper to the left of mass 120 u than to the right of it).
Origin of the active energy and the curve of binding energy.
Nuclear fission of heavy elements produces energy because the specific binding energy (binding energy per mass) of intermediate-mass nuclei with atomic numbers and atomic masses close to 62Ni and 56Fe is greater than the nucleon-specific binding energy of very heavy nuclei, so that energy is released when heavy nuclei are broken apart. The total rest masses of the fission products (Mp) from a single reaction is less than the mass of the original fuel nucleus (M). The excess mass Δm = M – Mp is the invariant mass of the energy that is released as photons (gamma rays) and kinetic energy of the fission fragments, according to the mass-energy equivalence formula "E" = "mc"2.
The variation in specific binding energy with atomic number is due to the interplay of the two fundamental forces acting on the component nucleons (protons and neutrons) that make up the nucleus. Nuclei are bound by an attractive nuclear force between nucleons, which overcomes the electrostatic repulsion between protons. However, the nuclear force acts only over relatively short ranges (a few nucleon diameters), since it follows an exponentially decaying Yukawa potential which makes it insignificant at longer distances. The electrostatic repulsion is of longer range, since it decays by an inverse-square rule, so that nuclei larger than about 12 nucleons in diameter reach a point that the total electrostatic repulsion overcomes the nuclear force and causes them to be spontaneously unstable. For the same reason, larger nuclei (more than about eight nucleons in diameter) are less tightly bound per unit mass than are smaller nuclei; breaking a large nucleus into two or more intermediate-sized nuclei releases energy. The origin of this energy is the nuclear force, which intermediate-sized nuclei allows to act more efficiently, because each nucleon has more neighbors which are within the short range attraction of this force. Thus less energy is needed in the smaller nuclei and the difference to the state before is set free.
Also because of the short range of the strong binding force, large stable nuclei must contain proportionally more neutrons than do the lightest elements, which are most stable with a 1 to 1 ratio of protons and neutrons. Nuclei which have more than 20 protons cannot be stable unless they have more than an equal number of neutrons. Extra neutrons stabilize heavy elements because they add to strong-force binding (which acts between all nucleons) without adding to proton–proton repulsion. Fission products have, on average, about the same ratio of neutrons and protons as their parent nucleus, and are therefore usually unstable to beta decay (which changes neutrons to protons) because they have proportionally too many neutrons compared to stable isotopes of similar mass.
This tendency for fission product nuclei to beta-decay is the fundamental cause of the problem of radioactive high level waste from nuclear reactors. Fission products tend to be beta emitters, emitting fast-moving electrons to conserve electric charge, as excess neutrons convert to protons in the fission-product atoms. See Fission products (by element) for a description of fission products sorted by element.
Chain reactions.
Several heavy elements, such as uranium, thorium, and plutonium, undergo both spontaneous fission, a form of radioactive decay and "induced fission", a form of nuclear reaction. Elemental isotopes that undergo induced fission when struck by a free neutron are called fissionable; isotopes that undergo fission when struck by a thermal, slow moving neutron are also called fissile. A few particularly fissile and readily obtainable isotopes (notably 233U, 235U and 239Pu) are called nuclear fuels because they can sustain a chain reaction and can be obtained in large enough quantities to be useful.
All fissionable and fissile isotopes undergo a small amount of spontaneous fission which releases a few free neutrons into any sample of nuclear fuel. Such neutrons would escape rapidly from the fuel and become a free neutron, with a mean lifetime of about 15 minutes before decaying to protons and beta particles. However, neutrons almost invariably impact and are absorbed by other nuclei in the vicinity long before this happens (newly created fission neutrons move at about 7% of the speed of light, and even moderated neutrons move at about 8 times the speed of sound). Some neutrons will impact fuel nuclei and induce further fissions, releasing yet more neutrons. If enough nuclear fuel is assembled in one place, or if the escaping neutrons are sufficiently contained, then these freshly emitted neutrons outnumber the neutrons that escape from the assembly, and a "sustained nuclear chain reaction" will take place.
An assembly that supports a sustained nuclear chain reaction is called a critical assembly or, if the assembly is almost entirely made of a nuclear fuel, a critical mass. The word "critical" refers to a cusp in the behavior of the differential equation that governs the number of free neutrons present in the fuel: if less than a critical mass is present, then the amount of neutrons is determined by radioactive decay, but if a critical mass or more is present, then the amount of neutrons is controlled instead by the physics of the chain reaction. The actual mass of a "critical mass" of nuclear fuel depends strongly on the geometry and surrounding materials.
Not all fissionable isotopes can sustain a chain reaction. For example, 238U, the most abundant form of uranium, is fissionable but not fissile: it undergoes induced fission when impacted by an energetic neutron with over 1 MeV of kinetic energy. However, too few of the neutrons produced by 238U fission are energetic enough to induce further fissions in 238U, so no chain reaction is possible with this isotope. Instead, bombarding 238U with slow neutrons causes it to absorb them (becoming 239U) and decay by beta emission to 239Np which then decays again by the same process to 239Pu; that process is used to manufacture 239Pu in breeder reactors. In-situ plutonium production also contributes to the neutron chain reaction in other types of reactors after sufficient plutonium-239 has been produced, since plutonium-239 is also a fissile element which serves as fuel. It is estimated that up to half of the power produced by a standard "non-breeder" reactor is produced by the fission of plutonium-239 produced in place, over the total life-cycle of a fuel load.
Fissionable, non-fissile isotopes can be used as fission energy source even without a chain reaction. Bombarding 238U with fast neutrons induces fissions, releasing energy as long as the external neutron source is present. This is an important effect in all reactors where fast neutrons from the fissile isotope can cause the fission of nearby 238U nuclei, which means that some small part of the 238U is "burned-up" in all nuclear fuels, especially in fast breeder reactors that operate with higher-energy neutrons. That same fast-fission effect is used to augment the energy released by modern thermonuclear weapons, by jacketing the weapon with 238U to react with neutrons released by nuclear fusion at the center of the device. But the explosive effects of nuclear fission chain reactions can be reduced by using substances like moderators which slow down the speed of secondary neutrons.
Fission reactors.
Critical fission reactors are the most common type of nuclear reactor. In a critical fission reactor, neutrons produced by fission of fuel atoms are used to induce yet more fissions, to sustain a controllable amount of energy release. Devices that produce engineered but non-self-sustaining fission reactions are subcritical fission reactors. Such devices use radioactive decay or particle accelerators to trigger fissions.
Critical fission reactors are built for three primary purposes, which typically involve different engineering trade-offs to take advantage of either the heat or the neutrons produced by the fission chain reaction:
While, in principle, all fission reactors can act in all three capacities, in practice the tasks lead to conflicting engineering goals and most reactors have been built with only one of the above tasks in mind. (There are several early counter-examples, such as the Hanford N reactor, now decommissioned). Power reactors generally convert the kinetic energy of fission products into heat, which is used to heat a working fluid and drive a heat engine that generates mechanical or electrical power. The working fluid is usually water with a steam turbine, but some designs use other materials such as gaseous helium. Research reactors produce neutrons that are used in various ways, with the heat of fission being treated as an unavoidable waste product. Breeder reactors are a specialized form of research reactor, with the caveat that the sample being irradiated is usually the fuel itself, a mixture of 238U and 235U.
For a more detailed description of the physics and operating principles of critical fission reactors, see nuclear reactor physics. For a description of their social, political, and environmental aspects, see nuclear power.
Fission bombs.
One class of nuclear weapon, a "fission bomb" (not to be confused with the "fusion bomb"), otherwise known as an "atomic bomb" or "atom bomb", is a fission reactor designed to liberate as much energy as possible as rapidly as possible, before the released energy causes the reactor to explode (and the chain reaction to stop). Development of nuclear weapons was the motivation behind early research into nuclear fission: the Manhattan Project of the U.S. military during World War II carried out most of the early scientific work on fission chain reactions, culminating in the Trinity test bomb and the Little Boy and Fat Man bombs that were exploded over the cities Hiroshima, and Nagasaki, Japan in August 1945.
Even the first fission bombs were thousands of times more explosive than a comparable mass of chemical explosive. For example, Little Boy weighed a total of about four tons (of which 60 kg was nuclear fuel) and was long; it also yielded an explosion equivalent to about 15 kilotons of TNT, destroying a large part of the city of Hiroshima. Modern nuclear weapons (which include a thermonuclear "fusion" as well as one or more fission stages) are hundreds of times more energetic for their weight than the first pure fission atomic bombs (see nuclear weapon yield), so that a modern single missile warhead bomb weighing less than 1/8 as much as Little Boy (see for example W88) has a yield of 475,000 tons of TNT, and could bring destruction to about 10 times the city area.
While the fundamental physics of the fission chain reaction in a nuclear weapon is similar to the physics of a controlled nuclear reactor, the two types of device must be engineered quite differently (see nuclear reactor physics). A nuclear bomb is designed to release all its energy at once, while a reactor is designed to generate a steady supply of useful power. While overheating of a reactor can lead to, and has led to, meltdown and steam explosions, the much lower uranium enrichment makes it impossible for a nuclear reactor to explode with the same destructive power as a nuclear weapon. It is also difficult to extract useful power from a nuclear bomb, although at least one rocket propulsion system, Project Orion, was intended to work by exploding fission bombs behind a massively padded and shielded spacecraft.
The strategic importance of nuclear weapons is a major reason why the technology of nuclear fission is politically sensitive. Viable fission bomb designs are, arguably, within the capabilities of many, being relatively simple from an engineering viewpoint. However, the difficulty of obtaining fissile nuclear material to realize the designs is the key to the relative unavailability of nuclear weapons to all but modern industrialized governments with special programs to produce fissile materials (see uranium enrichment and nuclear fuel cycle).
History.
Discovery of nuclear fission.
The discovery of nuclear fission occurred in 1938 in the buildings of Kaiser Wilhelm Society for Chemistry, today part of the Free University of Berlin, following nearly five decades of work on the science of radioactivity and the elaboration of new nuclear physics that described the components of atoms. In 1911, Ernest Rutherford proposed a model of the atom in which a very small, dense and positively charged nucleus of protons (the neutron had not yet been discovered) was surrounded by orbiting, negatively charged electrons (the Rutherford model). Niels Bohr improved upon this in 1913 by reconciling the quantum behavior of electrons (the Bohr model). Work by Henri Becquerel, Marie Curie, Pierre Curie, and Rutherford further elaborated that the nucleus, though tightly bound, could undergo different forms of radioactive decay, and thereby transmute into other elements. (For example, by alpha decay: the emission of an alpha particle—two protons and two neutrons bound together into a particle identical to a helium nucleus.)
Some work in nuclear transmutation had been done. In 1917, Rutherford was able to accomplish transmutation of nitrogen into oxygen, using alpha particles directed at nitrogen 14N + α → 17O + p.  This was the first observation of a nuclear reaction, that is, a reaction in which particles from one decay are used to transform another atomic nucleus. Eventually, in 1932, a fully artificial nuclear reaction and nuclear transmutation was achieved by Rutherford's colleagues Ernest Walton and John Cockcroft, who used artificially accelerated protons against lithium-7, to split this nucleus into two alpha particles. The feat was popularly known as "splitting the atom", although it was not the modern nuclear fission reaction later discovered in heavy elements, which is discussed below. Meanwhile, the possibility of "combining" nuclei—nuclear fusion—had been studied in connection with understanding the processes which power stars. The first artificial fusion reaction had been achieved by Mark Oliphant in 1932, using two accelerated deuterium nuclei (each consisting of a single proton bound to a single neutron) to create a helium nucleus.
After English physicist James Chadwick discovered the neutron in 1932, Enrico Fermi and his colleagues in Rome studied the results of bombarding uranium with neutrons in 1934. Fermi concluded that his experiments had created new elements with 93 and 94 protons, which the group dubbed ausonium and hesperium. However, not all were convinced by Fermi's analysis of his results. The German chemist Ida Noddack notably suggested in print in 1934 that instead of creating a new, heavier element 93, that "it is conceivable that the nucleus breaks up into several large fragments." However, Noddack's conclusion was not pursued at the time.
After the Fermi publication, Otto Hahn, Lise Meitner, and Fritz Strassmann began performing similar experiments in Berlin. Meitner, an Austrian Jew, lost her citizenship with the "Anschluss", the occupation and annexation of Austria into Nazi Germany in March 1938, but she fled in July 1938 to Sweden and started a correspondence by mail with Hahn in Berlin. By coincidence, her nephew Otto Robert Frisch, also a refugee, was also in Sweden when Meitner received a letter from Hahn dated 19 December describing his chemical proof that some of the product of the bombardment of uranium with neutrons was barium. Hahn suggested a "bursting" of the nucleus, but he was unsure of what the physical basis for the results were. Barium had an atomic mass 40% less than uranium, and no previously known methods of radioactive decay could account for such a large difference in the mass of the nucleus. Frisch was skeptical, but Meitner trusted Hahn's ability as a chemist. Marie Curie had been separating barium from radium for many years, and the techniques were well-known. According to Frisch:
Was it a mistake? No, said Lise Meitner; Hahn was too good a chemist for that. But how could barium be formed from uranium? No larger fragments than protons or helium nuclei (alpha particles) had ever been chipped away from nuclei, and to chip off a large number not nearly enough energy was available. Nor was it possible that the uranium nucleus could have been cleaved right across. A nucleus was not like a brittle solid that can be cleaved or broken; George Gamow had suggested early on, and Bohr had given good arguments that a nucleus was much more like a liquid drop. Perhaps a drop could divide itself into two smaller drops in a more gradual manner, by first becoming elongated, then constricted, and finally being torn rather than broken in two? We knew that there were strong forces that would resist such a process, just as the surface tension of an ordinary liquid drop tends to resist its division into two smaller ones. But nuclei differed from ordinary drops in one important way: they were electrically charged, and that was known to counteract the surface tension.
The charge of a uranium nucleus, we found, was indeed large enough to overcome the effect of the surface tension almost completely; so the uranium nucleus might indeed resemble a very wobbly unstable drop, ready to divide itself at the slightest provocation, such as the impact of a single neutron. But there was another problem. After separation, the two drops would be driven apart by their mutual electric repulsion and would acquire high speed and hence a very large energy, about 200 MeV in all; where could that energy come from? ...Lise Meitner... worked out that the two nuclei formed by the division of a uranium nucleus together would be lighter than the original uranium nucleus by about one-fifth the mass of a proton. Now whenever mass disappears energy is created, according to Einstein's formula "E" = "mc"2, and one-fifth of a proton mass was just equivalent to 200 MeV. So here was the source for that energy; it all fitted!
In short, Meitner and Frisch had correctly interpreted Hahn's results to mean that the nucleus of uranium had split roughly in half. Frisch suggested the process be named "nuclear fission," by analogy to the process of living cell division into two cells, which was then called binary fission. Just as the term nuclear "chain reaction" would later be borrowed from chemistry, so the term "fission" was borrowed from biology.
On 22 December 1938, Hahn and Strassmann sent a manuscript to "Naturwissenschaften" reporting that they had discovered the element barium after bombarding uranium with neutrons. Simultaneously, they communicated these results to Meitner in Sweden. She and Frisch correctly interpreted the results as evidence of nuclear fission. Frisch confirmed this experimentally on 13 January 1939. For proving that the barium resulting from his bombardment of uranium with neutrons was the product of nuclear fission, Hahn was awarded the Nobel Prize for Chemistry in 1944 (the sole recipient) "for his discovery of the fission of heavy nuclei". (The award was actually given to Hahn in 1945, as "the Nobel Committee for Chemistry decided that none of the year's nominations met the criteria as outlined in the will of Alfred Nobel." In such cases, the Nobel Foundation's statutes permit that year's prize be reserved until the following year.)
News spread quickly of the new discovery, which was correctly seen as an entirely novel physical effect with great scientific—and potentially practical—possibilities. Meitner’s and Frisch’s interpretation of the discovery of Hahn and Strassmann crossed the Atlantic Ocean with Niels Bohr, who was to lecture at Princeton University. I.I. Rabi and Willis Lamb, two Columbia University physicists working at Princeton, heard the news and carried it back to Columbia. Rabi said he told Enrico Fermi; Fermi gave credit to Lamb. Bohr soon thereafter went from Princeton to Columbia to see Fermi. Not finding Fermi in his office, Bohr went down to the cyclotron area and found Herbert L. Anderson. Bohr grabbed him by the shoulder and said: “Young man, let me explain to you about something new and exciting in physics.” It was clear to a number of scientists at Columbia that they should try to detect the energy released in the nuclear fission of uranium from neutron bombardment. On 25 January 1939, a Columbia University team conducted the first nuclear fission experiment in the United States, which was done in the basement of Pupin Hall; the members of the team were Herbert L. Anderson, Eugene T. Booth, John R. Dunning, Enrico Fermi, G. Norris Glasoe, and Francis G. Slack. The experiment involved placing uranium oxide inside of an ionization chamber and irradiating it with neutrons, and measuring the energy thus released. The results confirmed that fission was occurring and hinted strongly that it was the isotope uranium 235 in particular that was fissioning. The next day, the Fifth Washington Conference on Theoretical Physics began in Washington, D.C. under the joint auspices of the George Washington University and the Carnegie Institution of Washington. There, the news on nuclear fission was spread even further, which fostered many more experimental demonstrations.
During this period the Hungarian physicist Leó Szilárd, who was residing in the United States at the time, realized that the neutron-driven fission of heavy atoms could be used to create a nuclear chain reaction. Such a reaction using neutrons was an idea he had first formulated in 1933, upon reading Rutherford's disparaging remarks about generating power from his team's 1932 experiment using protons to split lithium. However, Szilárd had not been able to achieve a neutron-driven chain reaction with neutron-rich light atoms. In theory, if in a neutron-driven chain reaction the number of secondary neutrons produced was greater than one, then each such reaction could trigger multiple additional reactions, producing an exponentially increasing number of reactions. It was thus a possibility that the fission of uranium could yield vast amounts of energy for civilian or military purposes (i.e., electric power generation or atomic bombs).
Szilard now urged Fermi (in New York) and Frédéric Joliot-Curie (in Paris) to refrain from publishing on the possibility of a chain reaction, lest the Nazi government become aware of the possibilities on the eve of what would later be known as World War II. With some hesitation Fermi agreed to self-censor. But Joliot-Curie did not, and in April 1939 his team in Paris, including Hans von Halban and Lew Kowarski, reported in the journal "Nature" that the number of neutrons emitted with nuclear fission of 235U was then reported at 3.5 per fission. (They later corrected this to 2.6 per fission.) Simultaneous work by Szilard and Walter Zinn confirmed these results. The results suggested the possibility of building nuclear reactors (first called "neutronic reactors" by Szilard and Fermi) and even nuclear bombs. However, much was still unknown about fission and chain reaction systems.
Fission chain reaction realized.
"Chain reactions" at that time were a known phenomenon in "chemistry", but the analogous process in nuclear physics, using neutrons, had been foreseen as early as 1933 by Szilárd, although Szilárd at that time had no idea with what materials the process might be initiated. Szilárd considered that neutrons would be ideal for such a situation, since they lacked an electrostatic charge.
With the news of fission neutrons from uranium fission, Szilárd immediately understood the possibility of a nuclear chain reaction using uranium. In the summer, Fermi and Szilard proposed the idea of a nuclear reactor (pile) to mediate this process. The pile would use natural uranium as fuel. Fermi had shown much earlier that neutrons were far more effectively captured by atoms if they were of low energy (so-called "slow" or "thermal" neutrons), because for quantum reasons it made the atoms look like much larger targets to the neutrons. Thus to slow down the secondary neutrons released by the fissioning uranium nuclei, Fermi and Szilard proposed a graphite "moderator," against which the fast, high-energy secondary neutrons would collide, effectively slowing them down. With enough uranium, and with pure-enough graphite, their "pile" could theoretically sustain a slow-neutron chain reaction. This would result in the production of heat, as well as the creation of radioactive fission products.
In August 1939, Szilard and fellow Hungarian refugees physicists Teller and Wigner thought that the Germans might make use of the fission chain reaction and were spurred to attempt to attract the attention of the United States government to the issue. Towards this, they persuaded German-Jewish refugee Albert Einstein to lend his name to a letter directed to President Franklin Roosevelt. The Einstein–Szilárd letter suggested the possibility of a uranium bomb deliverable by ship, which would destroy "an entire harbor and much of the surrounding countryside." The President received the letter on 11 October 1939 — shortly after World War II began in Europe, but two years before U.S. entry into it. Roosevelt ordered that a scientific committee be authorized for overseeing uranium work and allocated a small sum of money for pile research.
In England, James Chadwick proposed an atomic bomb utilizing natural uranium, based on a paper by Rudolf Peierls with the mass needed for critical state being 30–40 tons. In America, J. Robert Oppenheimer thought that a cube of uranium deuteride 10 cm on a side (about 11 kg of uranium) might "blow itself to hell." In this design it was still thought that a moderator would need to be used for nuclear bomb fission (this turned out not to be the case if the fissile isotope was separated). In December, Werner Heisenberg delivered a report to the German Ministry of War on the possibility of a uranium bomb. Most of these models were still under the assumption that the bombs would be powered by slow neutron reactions—and thus be similar to a reactor undergoing a meltdown.
In Birmingham, England, Frisch teamed up with Peierls, a fellow German-Jewish refugee. They had the idea of using a purified mass of the uranium isotope 235U, which had a cross section just determined, and which was much larger than that of 238U or natural uranium (which is 99.3% the latter isotope). Assuming that the cross section for fast-neutron fission of 235U was the same as for slow neutron fission, they determined that a pure 235U bomb could have a critical mass of only 6 kg instead of tons, and that the resulting explosion would be tremendous. (The amount actually turned out to be 15 kg, although several times this amount was used in the actual uranium (Little Boy) bomb). In February 1940 they delivered the Frisch–Peierls memorandum. Ironically, they were still officially considered "enemy aliens" at the time. Glenn Seaborg, Joseph W. Kennedy, Arthur Wahl and Italian-Jewish refugee Emilio Segrè shortly thereafter discovered 239Pu in the decay products of 239U produced by bombarding 238U with neutrons, and determined it to be a fissile material, like 235U.
The possibility of isolating uranium-235 was technically daunting, because uranium-235 and uranium-238 are chemically identical, and vary in their mass by only the weight of three neutrons. However, if a sufficient quantity of uranium-235 could be isolated, it would allow for a fast neutron fission chain reaction. This would be extremely explosive, a true "atomic bomb." The discovery that plutonium-239 could be produced in a nuclear reactor pointed towards another approach to a fast neutron fission bomb. Both approaches were extremely novel and not yet well understood, and there was considerable scientific skepticism at the idea that they could be developed in a short amount of time.
On June 28, 1941, the Office of Scientific Research and Development was formed in the U.S. to mobilize scientific resources and apply the results of research to national defense. In September, Fermi assembled his first nuclear "pile" or reactor, in an attempt to create a slow neutron-induced chain reaction in uranium, but the experiment failed to achieve criticality, due to lack of proper materials, or not enough of the proper materials which were available.
Producing a fission chain reaction in natural uranium fuel was found to be far from trivial. Early nuclear reactors did not use isotopically enriched uranium, and in consequence they were required to use large quantities of highly purified graphite as neutron moderation materials. Use of ordinary water (as opposed to heavy water) in nuclear reactors requires enriched fuel — the partial separation and relative enrichment of the rare 235U isotope from the far more common 238U isotope. Typically, reactors also require inclusion of extremely chemically pure neutron moderator materials such as deuterium (in heavy water), helium, beryllium, or carbon, the latter usually as graphite. (The high purity for carbon is required because many chemical impurities such as the boron-10 component of natural boron, are very strong neutron absorbers and thus poison the chain reaction and end it prematurely.)
Production of such materials at industrial scale had to be solved for nuclear power generation and weapons production to be accomplished. Up to 1940, the total amount of uranium metal produced in the USA was not more than a few grams, and even this was of doubtful purity; of metallic beryllium not more than a few kilograms; and concentrated deuterium oxide (heavy water) not more than a few kilograms. Finally, carbon had never been produced in quantity with anything like the purity required of a moderator.
The problem of producing large amounts of high purity uranium was solved by Frank Spedding using the thermite or "Ames" process. Ames Laboratory was established in 1942 to produce the large amounts of natural (unenriched) uranium metal that would be necessary for the research to come. The critical nuclear chain-reaction success of the Chicago Pile-1 (December 2, 1942) which used unenriched (natural) uranium, like all of the atomic "piles" which produced the plutonium for the atomic bomb, was also due specifically to Szilard's realization that very pure graphite could be used for the moderator of even natural uranium "piles". In wartime Germany, failure to appreciate the qualities of very pure graphite led to reactor designs dependent on heavy water, which in turn was denied the Germans by Allied attacks in Norway, where heavy water was produced. These difficulties—among many others— prevented the Nazis from building a nuclear reactor capable of criticality during the war, although they never put as much effort as the United States into nuclear research, focusing on other technologies (see German nuclear energy project for more details).
Manhattan Project and beyond.
In the United States, an all-out effort for making atomic weapons was begun in late 1942. This work was taken over by the U.S. Army Corps of Engineers in 1943, and known as the Manhattan Engineer District. The top-secret Manhattan Project, as it was colloquially known, was led by General Leslie R. Groves. Among the project's dozens of sites were: Hanford Site in Washington state, which had the first industrial-scale nuclear reactors; Oak Ridge, Tennessee, which was primarily concerned with uranium enrichment; and Los Alamos, in New Mexico, which was the scientific hub for research on bomb development and design. Other sites, notably the Berkeley Radiation Laboratory and the Metallurgical Laboratory at the University of Chicago, played important contributing roles. Overall scientific direction of the project was managed by the physicist J. Robert Oppenheimer.
In July 1945, the first atomic bomb, dubbed "Trinity", was detonated in the New Mexico desert. It was fueled by plutonium created at Hanford. In August 1945, two more atomic bombs—"Little Boy", a uranium-235 bomb, and "Fat Man", a plutonium bomb—were used against the Japanese cities of Hiroshima and Nagasaki.
In the years after World War II, many countries were involved in the further development of nuclear fission for the purposes of nuclear reactors and nuclear weapons. The UK opened the first commercial nuclear power plant in 1956. In 2013, there are 437 reactors in 31 countries.
Natural fission chain-reactors on Earth.
Criticality in nature is uncommon. At three ore deposits at Oklo in Gabon, sixteen sites (the so-called Oklo Fossil Reactors) have been discovered at which self-sustaining nuclear fission took place approximately 2 billion years ago. Unknown until 1972 (but postulated by Paul Kuroda in 1956), when French physicist Francis Perrin discovered the Oklo Fossil Reactors, it was realized that nature had beaten humans to the punch. Large-scale natural uranium fission chain reactions, moderated by normal water, had occurred far in the past and would not be possible now. This ancient process was able to use normal water as a moderator only because 2 billion years before the present, natural uranium was richer in the shorter-lived fissile isotope 235U (about 3%), than natural uranium available today (which is only 0.7%, and must be enriched to 3% to be usable in light-water reactors).

</doc>
<doc id="1982496" url="https://en.wikipedia.org/wiki?curid=1982496" title="Nuclear force">
Nuclear force

The nuclear force (or nucleon–nucleon interaction or residual strong force) is the force between protons and neutrons, subatomic particles that are collectively called nucleons. The nuclear force is responsible for binding protons and neutrons into atomic nuclei. Neutrons and protons are affected by the nuclear force almost identically. Since protons have charge +1 "e", they experience a strong electric field repulsion (following Coulomb's law) that tends to push them apart, but at short range the attractive nuclear force overcomes the repulsive electromagnetic force. The mass of a nucleus is less than the sum total of the individual masses of the protons and neutrons which form it. The difference in mass between bound and unbound nucleons is known as the mass defect. Energy is released when some large nuclei break apart, and it is this energy that is used in nuclear power and nuclear weapons. 
The nuclear force is powerfully attractive between nucleons at distances of about 1 femtometer (fm, or 1.0 × 10−15 metres) between their centers, but rapidly decreases to insignificance at distances beyond about 2.5 fm. At distances less than 0.7 fm, the nuclear force becomes repulsive. This repulsive component is responsible for the physical size of nuclei, since the nucleons can come no closer than the force allows. By comparison, the size of an atom, measured in angstroms (Å, or 1.0 × 10−10 m), is five orders of magnitude larger. The nuclear force is not simple, however, since it depends on the nucleon spins, has a tensor component, and may depend on the relative momentum of the nucleons.
A quantitative description of the nuclear force relies on partially empirical equations that model the internucleon potential energies, or potentials. (Generally, forces within a system of particles can be more simply modeled by describing the system's potential energy; the negative gradient of a potential is equal to the vector force.) The constants for the equations are phenomenological, that is, determined by fitting the equations to experimental data. The internucleon potentials attempt to describe the properties of nucleon–nucleon interaction. Once determined, any given potential can be used in, e.g., the Schrödinger equation to determine the quantum mechanical properties of the nucleon system.
The discovery of the neutron in 1932 revealed that atomic nuclei were made of protons and neutrons, held together by an attractive force. By 1935 the nuclear force was conceived to be transmitted by particles called mesons. This theoretical development included a description of the Yukawa potential, an early example of a nuclear potential. Mesons, predicted by theory, were discovered experimentally in 1947. By the 1970s, the quark model had been developed, which showed that the mesons and nucleons were composed of quarks and gluons. By this new model, the nuclear force, resulting from the exchange of mesons between neighboring nucleons, is a residual effect of the strong force.
Description.
The nuclear force is only felt between particles composed of quarks, or hadrons. At small separations between nucleons (less than ~ 0.7 fm between their centers, depending upon spin alignment) the force becomes repulsive, which keeps the nucleons at a certain average separation, even if they are of different types. This repulsion arises from the Pauli exclusion force for identical nucleons (such as two neutrons or two protons). A Pauli exclusion force also occurs between quarks of the same type within nucleons, when the nucleons are different (a proton and a neutron, for example). The nuclear force also has a "tensor" component which depends on whether or not the spins (angular momentum vectors) of the nucleons are aligned (point in the same direction) or anti-aligned (i.e., point in opposite directions in space).
At distances larger than 0.7 fm the force becomes attractive between spin-aligned nucleons, becoming maximal at a center–center distance of about 0.9 fm. Beyond this distance the force drops exponentially, until beyond about 2.0 fm separation, the force is negligible. Nucleons have a radius of about 0.8 fm.
At short distances (less than 1.7 fm or so), the nuclear force is stronger than the Coulomb force between protons; it thus overcomes the repulsion of protons inside the nucleus. However, the Coulomb force between protons has a much larger range due to its decay as the inverse square of charge separation, and Coulomb repulsion thus becomes the only significant force between protons when their separation exceeds about 2 to 2.5 fm.
For two particles that are the same (such as two neutrons or two protons) the force is not enough to bind the particles, since the spin vectors of two particles of the same type must point in opposite directions when the particles are near each other and are (save for spin) in the same quantum state. This requirement for fermions stems from the Pauli exclusion principle. For fermion particles of different types (such as a proton and neutron), particles may be close to each other and have aligned spins without violating the Pauli exclusion principle, and the nuclear force may bind them (in this case, into a deuteron), since the nuclear force is much stronger for spin-aligned particles. But if the particles' spins are anti-aligned the nuclear force is too weak to bind them, even if they are of different types.
To disassemble a nucleus into unbound protons and neutrons requires work against the nuclear force. Conversely, energy is released when a nucleus is created from free nucleons or other nuclei: the nuclear binding energy. Because of mass–energy equivalence (i.e. Einstein's famous formula ), releasing this energy causes the mass of the nucleus to be lower than the total mass of the individual nucleons, leading to the so-called "mass defect".
The nuclear force is nearly independent of whether the nucleons are neutrons or protons. This property is called "charge independence". The force depends on whether the spins of the nucleons are parallel or antiparallel, and it has a noncentral or "tensor" component. This part of the force does not conserve orbital angular momentum, which is a constant of motion under central forces.
The symmetry resulting in the strong force, proposed by Werner Heisenberg, is that protons and neutrons are identical in every respect, other than their charge. This is not completely true, because neutrons are a tiny bit heavier, but it is an approximate symmetry. Protons and neutrons are therefore viewed as the same particle, but with different isospin quantum number. The strong force is invariant under SU(2) transformations, just as particles with "regular spin" are. Isospin and "regular" spin are related under this SU(2) symmetry group. There are only strong attractions when the total isospin is 0, as is confirmed by experiment.
The information on nuclear force are obtained by scattering experiments and the study of light nuclei binding energy.
The nuclear force occurs by the exchange of virtual light mesons, such as the virtual pions, as well as two types of virtual mesons with spin (vector mesons), the rho mesons and the omega mesons. The vector mesons account for the spin-dependence of the nuclear force in this "virtual meson" picture.
The nuclear force is separate from what historically was known as the weak nuclear force. The weak interaction is one of the four fundamental interactions, and it refers to such processes as beta decay. The weak force plays no role in the interaction of nucleons, though it is responsible for the decay of neutrons to protons and vice versa.
History.
The nuclear force has been at the heart of nuclear physics ever since the field was born in 1932 with the discovery of the neutron by James Chadwick. The traditional goal of nuclear physics is to understand the properties of atomic nuclei in terms of the 'bare' interaction between pairs of nucleons, or nucleon–nucleon forces (NN forces).
Within months after the discovery of the neutron, Werner Heisenberg and Dmitri Ivanenko had proposed proton–neutron models for the nucleus. Heisenberg approached the description of protons and neutrons in the nucleus through quantum mechanics, an approach that was not at all obvious at the time. Heisenberg's theory for protons and neutrons in the nucleus was a "major step toward understanding the nucleus as a quantum mechanical system." Heisenberg introduced the first theory of nuclear exchange forces that bind the nucleons. He considered protons and neutrons to be different quantum states of the same particle, i.e., nucleons distinguished by the value of their nuclear isospin quantum numbers.
One of the earliest models for the nucleus was the liquid drop model developed in the 1930s. One property of nuclei is that the average binding energy per nucleon is approximately the same for all stable nuclei, which is similar to a liquid drop. The liquid drop model treated the nucleus as a drop of incompressible nuclear fluid, with nucleons behaving like molecules in a liquid. The model was first proposed by George Gamow and then developed by Niels Bohr, Werner Heisenberg and Carl Friedrich von Weizsäcker. This crude model did not explain all the properties of the nucleus, but it did explain the spherical shape of most nuclei. The model also gave good predictions for the nuclear binding energy of nuclei.
In 1934, Hideki Yukawa made the earliest attempt to explain the nature of the nuclear force. According to his theory, massive bosons (mesons) mediate the interaction between two nucleons. Although, in light of quantum chromodynamics (QCD), meson theory is no longer perceived as fundamental, the meson-exchange concept (where hadrons are treated as elementary particles) continues to represent the best working model for a quantitative "NN" potential. The Yukawa potential (also called a screened Coulomb potential) is a potential of the form
where "g" is a magnitude scaling constant, i.e., the amplitude of potential, formula_2 is the Yukawa particle mass, "r" is the radial distance to the particle. The potential is monotone increasing, implying that the force is always attractive. The constants are determined empirically. The Yukawa potential depends only on the distance between particles, "r", hence it models a central force.
Throughout the 1930s a group at Columbia University led by I. I. Rabi developed magnetic resonance techniques to determine the magnetic moments of nuclei. These measurements led to the discovery in 1939 that the deuteron also possessed an electric quadrupole moment. This electrical property of the deuteron had been interfering with the measurements by the Rabi group. The deuteron, composed of a proton and a neutron, is one of the simplest nuclear systems. The discovery meant that the physical shape of the deuteron was not symmetric, which provided valuable insight into the nature of the nuclear force binding nucleons. In particular, the result showed that the nuclear force was not a central force, but had a tensor character. Hans Bethe identified the discovery of the deuteron's quadrupole moment as one of the important events during the formative years of nuclear physics.
Historically, the task of describing the nuclear force phenomenologically was formidable. The first semi-empirical quantitative models came in the mid-1950s, such as the Woods–Saxon potential (1954). There was substantial progress in experiment and theory related to the nuclear force in the 1960s and 1970s. One influential model was the Reid potential (1968). In recent years, experimenters have concentrated on the subtleties of the nuclear force, such as its charge dependence, the precise value of the π"NN" coupling constant, improved phase shift analysis, high-precision "NN" data, high-precision "NN" potentials, "NN" scattering at intermediate and high energies, and attempts to derive the nuclear force from QCD.
The nuclear force as a residual of the strong force.
The nuclear force is a residual effect of the more fundamental strong force, or strong interaction. The strong interaction is the attractive force that binds the elementary particles called quarks together to form the nucleons themselves. This more powerful force is mediated by particles called gluons. Gluons hold quarks together with a force like that of electric charge, but of far greater strength. Quarks, gluons and their dynamics are mostly confined within nucleons, but residual influences extend slightly beyond nucleon boundaries to give rise to the nuclear force.
The nuclear forces arising between nucleons are analogous to the forces in chemistry between neutral atoms or molecules called London forces. Such forces between atoms are much weaker than the attractive electrical forces that hold the atoms themselves together (i.e., that bind electrons to the nucleus), and their range between atoms is shorter, because they arise from small separation of charges inside the neutral atom. Similarly, even though nucleons are made of quarks in combinations which cancel most gluon forces (they are "color neutral"), some combinations of quarks and gluons nevertheless leak away from nucleons, in the form of short-range nuclear force fields that extend from one nucleon to another nearby nucleon. These nuclear forces are very weak compared to direct gluon forces ("color forces" or strong forces) inside nucleons, and the nuclear forces extend only over a few nuclear diameters, falling exponentially with distance. Nevertheless, they are strong enough to bind neutrons and protons over short distances, and overcome the electrical repulsion between protons in the nucleus.
Sometimes, the nuclear force is called the residual strong force, in contrast to the strong interactions which arise from QCD. This phrasing arose during the 1970s when QCD was being established. Before that time, the "strong nuclear force" referred to the inter-nucleon potential. After the verification of the quark model, "strong interaction" has come to mean QCD.
Nucleon–nucleon potentials.
Two-nucleon systems such as the deuteron, the nucleus of a deuterium atom, as well as proton–proton or neutron–proton scattering are ideal for studying the "NN" force. Such systems can be described by attributing a "potential" (such as the Yukawa potential) to the nucleons and using the potentials in a Schrödinger equation. The form of the potential is derived phenomenologically (by measurement), although for the long-range interaction, meson-exchange theories help to construct the potential. The parameters of the potential are determined by fitting to experimental data such as the deuteron binding energy or "NN" elastic scattering cross sections (or, equivalently in this context, so-called "NN" phase shifts).
The most widely used "NN" potentials are the Paris potential, the Argonne AV18 potential
, the CD-Bonn potential and the Nijmegen potentials. 
A more recent approach is to develop effective field theories for a consistent description of nucleon–nucleon and three-nucleon forces. In particular, chiral symmetry breaking can be analyzed in terms of an effective field theory (called chiral perturbation theory) which allows perturbative calculations of the interactions between nucleons with pions as exchange particles.
From nucleons to nuclei.
The ultimate goal of nuclear physics would be to describe all nuclear interactions from the basic interactions between nucleons. This is called the "microscopic" or "ab initio" approach of nuclear physics. There are two major obstacles to overcome before this dream can become reality:
This is an active area of research with ongoing advances in computational techniques leading to better first-principles calculations of the nuclear shell structure. Two- and three-nucleon potentials have been implemented for nuclides up to "A" = 12.
Nuclear potentials.
A successful way of describing nuclear interactions is to construct one potential for the whole nucleus instead of considering all its nucleon components. This is called the "macroscopic" approach. For example, scattering of neutrons from nuclei can be described by considering a plane wave in the potential of the nucleus, which comprises a real part and an imaginary part. This model is often called the optical model since it resembles the case of light scattered by an opaque glass sphere.
Nuclear potentials can be "local" or "global": local potentials are limited to a narrow energy range and/or a narrow nuclear mass range, while global potentials, which have more parameters and are usually less accurate, are functions of the energy and the nuclear mass and can therefore be used in a wider range of applications.

</doc>
<doc id="21544" url="https://en.wikipedia.org/wiki?curid=21544" title="Nuclear fusion">
Nuclear fusion

In nuclear physics, nuclear fusion is a nuclear reaction in which two or more atomic nuclei come very close and then collide at a very high speed and join to form a new nucleus. During this process, matter is not conserved because some of the matter of the fusing nuclei is converted to photons (energy). Fusion is the process that powers active or "main sequence" stars.
The fusion of two nuclei with lower masses than iron-56 (which, along with nickel-62, has the largest binding energy per nucleon) generally releases energy, while the fusion of nuclei heavier than iron "absorbs" energy. The opposite is true for the reverse process, nuclear fission. This means that generally only lighter elements are fusable, such as Hydrogen and Helium, and likewise, that generally only heavier elements are fissionable, such as Uranium and Plutonium. There are extreme astrophysical events that can lead to short periods of fusion with heavier nuclei. This is the process that gives rise to nucleosynthesis, the creation of the heavy elements during events such as supernova.
Following the discovery of quantum tunneling by physicist Friedrich Hund, in 1929 Robert Atkinson and Fritz Houtermans used the measured masses of light elements to predict that large amounts of energy could be released by fusing small nuclei. Building upon the nuclear transmutation experiments by Ernest Rutherford, carried out several years earlier, the laboratory fusion of hydrogen isotopes was first accomplished by Mark Oliphant in 1932. During the remainder of that decade the steps of the main cycle of nuclear fusion in stars were worked out by Hans Bethe. Research into fusion for military purposes began in the early 1940s as part of the Manhattan Project. Fusion was accomplished in 1951 with the Greenhouse Item nuclear test. Nuclear fusion on a large scale in an explosion was first carried out on November 1, 1952, in the Ivy Mike hydrogen bomb test.
Research into developing controlled thermonuclear fusion for civil purposes also began in earnest in the 1950s, and it continues to this day. The present article is about the theory of fusion. For details of the quest for controlled fusion and its history, see the article Fusion power.
Process.
The origin of the energy released in fusion of light elements is due to interplay of two opposing forces, the nuclear force which combines together protons and neutrons, and the Coulomb force which causes protons to repel each other. The protons are positively charged and repel each other but they nonetheless stick together, demonstrating the existence of another force referred to as nuclear attraction. This force, called the strong nuclear force, overcomes electric repulsion in a very close range. The effect of this force is not observed outside the nucleus, hence the force has a strong dependence on distance, making it a short-range force. The same force also pulls the nucleons (neutrons and protons) together. Because the nuclear force is stronger than the Coulomb force for atomic nuclei smaller than iron and nickel, building up these nuclei from lighter nuclei by fusion releases the extra energy from the net attraction of these particles. For larger nuclei, however, no energy is released, since the nuclear force is short-range and cannot continue to act across still larger atomic nuclei. Thus, energy is no longer released when such nuclei are made by fusion; instead, energy is absorbed in such processes.
Fusion reactions of light elements power the stars and produce virtually all elements in a process called nucleosynthesis. The fusion of lighter elements in stars releases energy (and the mass that always accompanies it). For example, in the fusion of two hydrogen nuclei to form helium, 0.7% of the mass is carried away from the system in the form of kinetic energy or other forms of energy (such as electromagnetic radiation).
Research into controlled fusion, with the aim of producing fusion power for the production of electricity, has been conducted for over 60 years. It has been accompanied by extreme scientific and technological difficulties, but has resulted in progress. At present, controlled fusion reactions have been unable to produce break-even (self-sustaining) controlled fusion reactions. Workable designs for a reactor that theoretically will deliver ten times more fusion energy than the amount needed to heat up plasma to required temperatures are in development (see ITER). The ITER facility is expected to finish its construction phase in 2019. It will start commissioning the reactor that same year and initiate plasma experiments in 2020, but is not expected to begin full deuterium-tritium fusion until 2027.
It takes considerable energy to force nuclei to fuse, even those of the lightest element, hydrogen. This is because all nuclei have a positive charge due to their protons, and as like charges repel, nuclei strongly resist being put close together. Accelerated to high speeds, they can overcome this electrostatic repulsion and be forced close enough for the attractive nuclear force to be sufficiently strong to achieve fusion. The fusion of lighter nuclei, which creates a heavier nucleus and often a free neutron or proton, generally releases more energy than it takes to force the nuclei together; this is an exothermic process that can produce self-sustaining reactions. The US National Ignition Facility, which uses laser-driven inertial confinement fusion, is thought to be capable of break-even fusion.
The first large-scale laser target experiments were performed in June 2009 and ignition experiments began in early 2011.
Energy released in most nuclear reactions is much larger than in chemical reactions, because the binding energy that holds a nucleus together is far greater than the energy that holds electrons to a nucleus. For example, the ionization energy gained by adding an electron to a hydrogen nucleus is —less than one-millionth of the released in the deuterium–tritium (D–T) reaction shown in the diagram to the right (one gram of matter would release of energy). Fusion reactions have an energy density many times greater than nuclear fission; the reactions produce far greater energy per unit of mass even though "individual" fission reactions are generally much more energetic than "individual" fusion ones, which are themselves millions of times more energetic than chemical reactions. Only direct conversion of mass into energy, such as that caused by the annihilatory collision of matter and antimatter, is more energetic per unit of mass than nuclear fusion.
Nuclear fusion in stars.
The most important fusion process in nature is the one that powers stars. In the 20th century, it was realized that the energy released from nuclear fusion reactions accounted for the longevity of the Sun and other stars as a source of heat and light. The fusion of nuclei in a star, starting from its initial hydrogen and helium abundance, provides that energy and synthesizes new nuclei as a byproduct of that fusion process. The prime energy producer in the Sun is the fusion of hydrogen to form helium, which occurs at a solar-core temperature of 14 million kelvin. The net result is the fusion of four protons into one alpha particle, with the release of two positrons, two neutrinos (which changes two of the protons into neutrons), and energy. Different reaction chains are involved, depending on the mass of the star. For stars the size of the sun or smaller, the proton-proton chain dominates. In heavier stars, the CNO cycle is more important.
As a star uses up a substantial fraction of its hydrogen, it begins to synthesize heavier elements, as part of stellar nucleosynthesis. However the heaviest elements are synthesized by fusion that occurs as a more massive star undergoes a violent supernova at the end of its life, a process known as supernova nucleosynthesis.
Requirements.
Details and supporting references on the material in this section can be found in textbooks on nuclear physics or nuclear fusion.
A substantial energy barrier of electrostatic forces must be overcome before fusion can occur. At large distances, two naked nuclei repel one another because of the repulsive electrostatic force between their positively charged protons. If two nuclei can be brought close enough together, however, the electrostatic repulsion can be overcome by the attractive nuclear force, which is stronger at close distances.
When a nucleon such as a proton or neutron is added to a nucleus, the nuclear force attracts it to other nucleons, but primarily to its immediate neighbours due to the short range of the force. The nucleons in the interior of a nucleus have more neighboring nucleons than those on the surface. Since smaller nuclei have a larger surface area-to-volume ratio, the binding energy per nucleon due to the nuclear force generally increases with the size of the nucleus but approaches a limiting value corresponding to that of a nucleus with a diameter of about four nucleons. It is important to keep in mind that the above picture is a toy model because nucleons are quantum objects, and so, for example, since two neutrons in a nucleus are identical to each other, distinguishing one from the other, such as which one is in the interior and which is on the surface, is in fact meaningless, and the inclusion of quantum mechanics is necessary for proper calculations.
The electrostatic force, on the other hand, is an inverse-square force, so a proton added to a nucleus will feel an electrostatic repulsion from "all" the other protons in the nucleus. The electrostatic energy per nucleon due to the electrostatic force thus increases without limit as nuclei get larger.
The net result of these opposing forces is that the binding energy per nucleon generally increases with increasing size, up to the elements iron and nickel, and then decreases for heavier nuclei. Eventually, the binding energy becomes negative and very heavy nuclei (all with more than 208 nucleons, corresponding to a diameter of about 6 nucleons) are not stable. The four most tightly bound nuclei, in decreasing order of binding energy per nucleon, are , , , and . Even though the nickel isotope, , is more stable, the iron isotope is an order of magnitude more common. This is due to the fact that there is no easy way for stars to create through the alpha process.
An exception to this general trend is the helium-4 nucleus, whose binding energy is higher than that of lithium, the next heaviest element. This is because protons and neutrons are fermions, which according to the Pauli exclusion principle cannot exist in the same nucleus in exactly the same state. Each proton or neutron energy state in a nucleus can accommodate both a spin up particle and a spin down particle. Helium-4 has an anomalously large binding energy because its nucleus consists of two protons and two neutrons, so all four of its nucleons can be in the ground state. Any additional nucleons would have to go into higher energy states. Indeed, the helium-4 nucleus is so tightly bound that it is commonly treated as a single particle in nuclear physics, namely, the alpha particle.
The situation is similar if two nuclei are brought together. As they approach each other, all the protons in one nucleus repel all the protons in the other. Not until the two nuclei actually come close enough for long enough can the strong nuclear force take over (by way of tunneling). Consequently, even when the final energy state is lower, there is a large energy barrier that must first be overcome. It is called the Coulomb barrier.
The Coulomb barrier is smallest for isotopes of hydrogen, as their nuclei contain only a single positive charge. A diproton is not stable, so neutrons must also be involved, ideally in such a way that a helium nucleus, with its extremely tight binding, is one of the products.
Using deuterium-tritium fuel, the resulting energy barrier is about 0.1 MeV. In comparison, the energy needed to remove an electron from hydrogen is 13.6 eV, about 7500 times less energy. The (intermediate) result of the fusion is an unstable 5He nucleus, which immediately ejects a neutron with 14.1 MeV. The recoil energy of the remaining 4He nucleus is 3.5 MeV, so the total energy liberated is 17.6 MeV. This is many times more than what was needed to overcome the energy barrier.
The reaction cross section σ is a measure of the probability of a fusion reaction as a function of the relative velocity of the two reactant nuclei. If the reactants have a distribution of velocities, e.g. a thermal distribution, then it is useful to perform an average over the distributions of the product of cross section and velocity. This average is called the 'reactivity', denoted <σv>. The reaction rate (fusions per volume per time) is <σv> times the product of the reactant number densities:
If a species of nuclei is reacting with itself, such as the DD reaction, then the product formula_2 must be replaced by formula_3.
formula_4 increases from virtually zero at room temperatures up to meaningful magnitudes at temperatures of 10–100 keV. At these temperatures, well above typical ionization energies (13.6 eV in the hydrogen case), the fusion reactants exist in a plasma state.
The significance of formula_4 as a function of temperature in a device with a particular energy confinement time is found by considering the Lawson criterion. This is an extremely challenging barrier to overcome on Earth, which explains why fusion research has taken many years to reach the current high state of technical prowess.
Methods for achieving fusion.
Thermonuclear fusion.
If the matter is sufficiently heated (hence being plasma), the fusion reaction may occur due to collisions with extreme thermal kinetic energies of the particles. In the form of thermonuclear weapons, thermonuclear fusion is the only fusion technique so far to yield undeniably large amounts of useful fusion energy. Usable amounts of thermonuclear fusion energy released in a controlled manner have yet to be achieved. In nature, this is what produces energy in stars through stellar nucleosynthesis.
Inertial confinement fusion.
Inertial confinement fusion (ICF) is a type of fusion energy research that attempts to initiate nuclear fusion reactions by heating and compressing a fuel target, typically in the form of a pellet that most often contains a mixture of deuterium and tritium.
Inertial electrostatic confinement.
Inertial electrostatic confinement is a set of devices that use an electric field to heat ions to fusion conditions. The most well known is the fusor. Starting in 1999, a number of amateurs have been able to do amateur fusion using these homemade devices. Other IEC devices include: the Polywell, MIX POPS and Marble concepts.
Beam-beam or beam-target fusion.
If the energy to initiate the reaction comes from accelerating one of the nuclei, the process is called "beam-target" fusion; if both nuclei are accelerated, it is "beam-beam" fusion.
Accelerator-based light-ion fusion is a technique using particle accelerators to achieve particle kinetic energies sufficient to induce light-ion fusion reactions. Accelerating light ions is relatively easy, and can be done in an efficient manner—all it takes is a vacuum tube, a pair of electrodes, and a high-voltage transformer; fusion can be observed with as little as 10 kV between electrodes. The key problem with accelerator-based fusion (and with cold targets in general) is that fusion cross sections are many orders of magnitude lower than Coulomb interaction cross sections. Therefore, the vast majority of ions end up expending their energy on bremsstrahlung and ionization of atoms of the target. Devices referred to as sealed-tube neutron generators are particularly relevant to this discussion. These small devices are miniature particle accelerators filled with deuterium and tritium gas in an arrangement that allows ions of these nuclei to be accelerated against hydride targets, also containing deuterium and tritium, where fusion takes place. Hundreds of neutron generators are produced annually for use in the petroleum industry where they are used in measurement equipment for locating and mapping oil reserves.
Muon-catalyzed fusion.
Muon-catalyzed fusion is a well-established and reproducible fusion process that occurs at ordinary temperatures. It was studied in detail by Steven Jones in the early 1980s. Net energy production from this reaction cannot occur because of the high energy required to create muons, their short 2.2 µs half-life, and the high chance that a muon will bind to the new alpha particle and thus stop catalyzing fusion.
Other principles.
Some other confinement principles have been investigated.
Antimatter-initialized fusion uses small amounts of antimatter to trigger a tiny fusion explosion. This has been studied primarily in the context of making nuclear pulse propulsion, and pure fusion bombs feasible. This is not near becoming a practical power source, due to the cost of manufacturing antimatter alone.
Pyroelectric fusion was reported in April 2005 by a team at UCLA. The scientists used a pyroelectric crystal heated from −34 to 7 °C (−29 to 45 °F), combined with a tungsten needle to produce an electric field of about 25 gigavolts per meter to ionize and accelerate deuterium nuclei into an erbium deuteride target. At the estimated energy levels, the D-D fusion reaction may occur, producing helium-3 and a 2.45 MeV neutron. Although it makes a useful neutron generator, the apparatus is not intended for power generation since it requires far more energy than it produces.
Hybrid nuclear fusion-fission (hybrid nuclear power) is a proposed means of generating power by use of a combination of nuclear fusion and fission processes. The concept dates to the 1950s, and was briefly advocated by Hans Bethe during the 1970s, but largely remained unexplored until a revival of interest in 2009, due to the delays in the realization of pure fusion.
Project PACER, carried out at Los Alamos National Laboratory (LANL) in the mid-1970s, explored the possibility of a fusion power system that would involve exploding small hydrogen bombs (fusion bombs) inside an underground cavity. As an energy source, the system is the only fusion power system that could be demonstrated to work using existing technology. However it would also require a large, continuous supply of nuclear bombs, making the economics of such a system rather questionable.
Important reactions.
Astrophysical reaction chains.
At the temperatures and densities in stellar cores the rates of fusion reactions are notoriously slow. For example, at solar core temperature ("T" ≈ 15 MK) and density (160 g/cm3), the energy release rate is only 276 μW/cm3—about a quarter of the volumetric rate at which a resting human body generates heat. Thus, reproduction of stellar core conditions in a lab for nuclear fusion power production is completely impractical. Because nuclear reaction rates strongly depend on temperature (exp(−"E"/"kT")), achieving reasonable power levels in terrestrial fusion reactors requires 10–100 times higher temperatures (compared to stellar interiors): "T" ≈ 0.1–1.0 GK.
Criteria and candidates for terrestrial reactions.
In artificial fusion, the primary fuel is not constrained to be protons and higher temperatures can be used, so reactions with larger cross-sections are chosen. Another concern is the production of neutrons, which activate the reactor structure radiologically, but also have the advantages of allowing volumetric extraction of the fusion energy and tritium breeding. Reactions that release no neutrons are referred to as "aneutronic".
To be a useful energy source, a fusion reaction must satisfy several criteria. It must:
Few reactions meet these criteria. The following are those with the largest cross sections:
For reactions with two products, the energy is divided between them in inverse proportion to their masses, as shown. In most reactions with three products, the distribution of energy varies. For reactions that can result in more than one set of products, the branching ratios are given.
Some reaction candidates can be eliminated at once. The D-6Li reaction has no advantage compared to p+- because it is roughly as difficult to burn but produces substantially more neutrons through - side reactions. There is also a p+- reaction, but the cross section is far too low, except possibly when "T"i > 1 MeV, but at such high temperatures an endothermic, direct neutron-producing reaction also becomes very significant. Finally there is also a p+- reaction, which is not only difficult to burn, but can be easily induced to split into two alpha particles and a neutron.
In addition to the fusion reactions, the following reactions with neutrons are important in order to "breed" tritium in "dry" fusion bombs and some proposed fusion reactors:
The latter of the two equations was unknown when the U.S. conducted the Castle Bravo fusion bomb test in 1954. Being just the second fusion bomb ever tested (and the first to use lithium), the designers of the Castle Bravo "Shrimp" had understood the usefulness of Lithium-6 in tritium production, but had failed to recognize that Lithium-7 fission would greatly increase the yield of the bomb. While Li-7 has a small neutron cross-section for low neutron energies, it has a higher cross section above 5 MeV. Li-7 also undergoes a chain reaction due to its release of a neutron after fissioning. The 15 Mt yield was 150% greater than the predicted 6 Mt and caused casualties from the fallout generated.
To evaluate the usefulness of these reactions, in addition to the reactants, the products, and the energy released, one needs to know something about the cross section. Any given fusion device has a maximum plasma pressure it can sustain, and an economical device would always operate near this maximum. Given this pressure, the largest fusion output is obtained when the temperature is chosen so that <σv>/T2 is a maximum. This is also the temperature at which the value of the triple product "nT"τ required for ignition is a minimum, since that required value is inversely proportional to <σv>/T2 (see Lawson criterion). (A plasma is "ignited" if the fusion reactions produce enough power to maintain the temperature without external heating.) This optimum temperature and the value of <σv>/T2 at that temperature is given for a few of these reactions in the following table.
Note that many of the reactions form chains. For instance, a reactor fueled with and creates some , which is then possible to use in the - reaction if the energies are "right". An elegant idea is to combine the reactions (8) and (9). The from reaction (8) can react with in reaction (9) before completely thermalizing. This produces an energetic proton, which in turn undergoes reaction (8) before thermalizing. Detailed analysis shows that this idea would not work well, but it is a good example of a case where the usual assumption of a Maxwellian plasma is not appropriate.
Neutronicity, confinement requirement, and power density.
Any of the reactions above can in principle be the basis of fusion power production. In addition to the temperature and cross section discussed above, we must consider the total energy of the fusion products "E"fus, the energy of the charged fusion products "E"ch, and the atomic number "Z" of the non-hydrogenic reactant.
Specification of the - reaction entails some difficulties, though. To begin with, one must average over the two branches (2i) and (2ii). More difficult is to decide how to treat the and products. burns so well in a deuterium plasma that it is almost impossible to extract from the plasma. The - reaction is optimized at a much higher temperature, so the burnup at the optimum - temperature may be low, so it seems reasonable to assume the but not the gets burned up and adds its energy to the net reaction. Thus the total reaction would be the sum of (2i), (2ii), and (1):
We count the - fusion energy "per D-D reaction" (not per pair of deuterium atoms) as "E"fus = (4.03 MeV + 17.6 MeV)×50% + (3.27 MeV)×50% = 12.5 MeV and the energy in charged particles as "E"ch = (4.03 MeV + 3.5 MeV)×50% + (0.82 MeV)×50% = 4.2 MeV. (Note: if the tritium ion reacts with a deuteron while it still has a large kinetic energy, then the kinetic energy of the helium-4 produced may be quite different from 3.5 MeV, so this calculation of energy in charged particles is only approximate.)
Another unique aspect of the - reaction is that there is only one reactant, which must be taken into account when calculating the reaction rate.
With this choice, we tabulate parameters for four of the most important reactions
The last column is the neutronicity of the reaction, the fraction of the fusion energy released as neutrons. This is an important indicator of the magnitude of the problems associated with neutrons like radiation damage, biological shielding, remote handling, and safety. For the first two reactions it is calculated as ("E"fus-"E"ch)/"E"fus. For the last two reactions, where this calculation would give zero, the values quoted are rough estimates based on side reactions that produce neutrons in a plasma in thermal equilibrium.
Of course, the reactants should also be mixed in the optimal proportions. This is the case when each reactant ion plus its associated electrons accounts for half the pressure. Assuming that the total pressure is fixed, this means that density of the non-hydrogenic ion is smaller than that of the hydrogenic ion by a factor 2/("Z"+1). Therefore, the rate for these reactions is reduced by the same factor, on top of any differences in the values of <σv>/T2. On the other hand, because the - reaction has only one reactant, its rate is twice as high as when the fuel is divided between two different hydrogenic species, thus creating a more efficient reaction.
Thus there is a "penalty" of (2/(Z+1)) for non-hydrogenic fuels arising from the fact that they require more electrons, which take up pressure without participating in the fusion reaction. (It is usually a good assumption that the electron temperature will be nearly equal to the ion temperature. Some authors, however discuss the possibility that the electrons could be maintained substantially colder than the ions. In such a case, known as a "hot ion mode", the "penalty" would not apply.) There is at the same time a "bonus" of a factor 2 for - because each ion can react with any of the other ions, not just a fraction of them.
We can now compare these reactions in the following table.
The maximum value of <σv>/T2 is taken from a previous table. The "penalty/bonus" factor is that related to a non-hydrogenic reactant or a single-species reaction. The values in the column "reactivity" are found by dividing 1.24 by the product of the second and third columns. It indicates the factor by which the other reactions occur more slowly than the - reaction under comparable conditions. The column "Lawson criterion" weights these results with "E"ch and gives an indication of how much more difficult it is to achieve ignition with these reactions, relative to the difficulty for the - reaction. The last column is labeled "power density" and weights the practical reactivity with "E"fus. It indicates how much lower the fusion power density of the other reactions is compared to the - reaction and can be considered a measure of the economic potential.
Bremsstrahlung losses in quasineutral, isotropic plasmas.
The ions undergoing fusion in many systems will essentially never occur alone but will be mixed with electrons that in aggregate neutralize the ions' bulk electrical charge and form a plasma. The electrons will generally have a temperature comparable to or greater than that of the ions, so they will collide with the ions and emit x-ray radiation of 10–30 keV energy, a process known as Bremsstrahlung.
The huge size of the Sun and stars means that the x-rays produced in this process will not escape and will deposit their energy back into the plasma. They are said to be opaque to x-rays. But any terrestrial fusion reactor will be optically thin for x-rays of this energy range. X-rays are difficult to reflect but they are effectively absorbed (and converted into heat) in less than mm thickness of stainless steel (which is part of a reactor's shield). This means the bremsstrahlung process is carrying energy out of the plasma, cooling it.
The ratio of fusion power produced to x-ray radiation lost to walls is an important figure of merit. This ratio is generally maximized at a much higher temperature than that which maximizes the power density (see the previous subsection). The following table shows estimates of the optimum temperature and the power ratio at that temperature for several reactions.
The actual ratios of fusion to Bremsstrahlung power will likely be significantly lower for several reasons. For one, the calculation assumes that the energy of the fusion products is transmitted completely to the fuel ions, which then lose energy to the electrons by collisions, which in turn lose energy by Bremsstrahlung. However, because the fusion products move much faster than the fuel ions, they will give up a significant fraction of their energy directly to the electrons. Secondly, the ions in the plasma are assumed to be purely fuel ions. In practice, there will be a significant proportion of impurity ions, which will then lower the ratio. In particular, the fusion products themselves "must" remain in the plasma until they have given up their energy, and "will" remain some time after that in any proposed confinement scheme. Finally, all channels of energy loss other than Bremsstrahlung have been neglected. The last two factors are related. On theoretical and experimental grounds, particle and energy confinement seem to be closely related. In a confinement scheme that does a good job of retaining energy, fusion products will build up. If the fusion products are efficiently ejected, then energy confinement will be poor, too.
The temperatures maximizing the fusion power compared to the Bremsstrahlung are in every case higher than the temperature that maximizes the power density and minimizes the required value of the fusion triple product. This will not change the optimum operating point for - very much because the Bremsstrahlung fraction is low, but it will push the other fuels into regimes where the power density relative to - is even lower and the required confinement even more difficult to achieve. For - and -, Bremsstrahlung losses will be a serious, possibly prohibitive problem. For -, p+- and p+- the Bremsstrahlung losses appear to make a fusion reactor using these fuels with a quasineutral, isotropic plasma impossible. Some ways out of this dilemma are considered—and rejected—in "Fundamental limitations on plasma fusion systems not in thermodynamic equilibrium" by Todd Rider. This limitation does not apply to non-neutral and anisotropic plasmas; however, these have their own challenges to contend with.

</doc>
<doc id="25110709" url="https://en.wikipedia.org/wiki?curid=25110709" title="Nuclear magnetic resonance">
Nuclear magnetic resonance

Nuclear magnetic resonance (NMR) is a physical phenomenon in which nuclei in a magnetic field absorb and re-emit electromagnetic radiation. This energy is at a specific resonance frequency which depends on the strength of the magnetic field and the magnetic properties of the isotope of the atoms; in practical applications, the frequency is similar to VHF and UHF television broadcasts (60–1000 MHz).
NMR allows the observation of specific quantum mechanical magnetic properties of the atomic nucleus. Many scientific techniques exploit NMR phenomena to study molecular physics, crystals, and non-crystalline materials through NMR spectroscopy. NMR is also routinely used in advanced medical imaging techniques, such as in magnetic resonance imaging (MRI).
All isotopes that contain an odd number of protons and/or of neutrons (see Isotope) have an intrinsic magnetic moment and angular momentum, in other words a nonzero spin, while all nuclides with even numbers of both have a total spin of zero. The most commonly studied nuclei are and , although nuclei from isotopes of many other elements (e.g. , , , , , , , , , , , , , , ) have been studied by high-field NMR spectroscopy as well.
A key feature of NMR is that the resonance frequency of a particular substance is directly proportional to the strength of the applied magnetic field. It is this feature that is exploited in imaging techniques; if a sample is placed in a non-uniform magnetic field then the resonance frequencies of the sample's nuclei depend on where in the field they are located. Since the resolution of the imaging technique depends on the magnitude of magnetic field gradient, many efforts are made to develop increased field strength, often using superconductors. The effectiveness of NMR can also be improved using hyperpolarization, and/or using two-dimensional, three-dimensional and higher-dimensional multi-frequency techniques.
The principle of NMR usually involves two sequential steps:
The two fields are usually chosen to be perpendicular to each other as this maximizes the NMR signal strength. The resulting response by the total magnetization (M) of the nuclear spins is the phenomenon that is exploited in NMR spectroscopy and magnetic resonance imaging. Both use intense applied magnetic fields (H0) in order to achieve dispersion and very high stability to deliver spectral resolution, the details of which are described by chemical shifts, the Zeeman effect, and Knight shifts (in metals).
NMR phenomena are also utilized in low-field NMR, NMR spectroscopy and MRI in the Earth's magnetic field (referred to as Earth's field NMR), and in several types of magnetometers.
History.
Nuclear magnetic resonance was first described and measured in molecular beams by Isidor Rabi in 1938, by extending the Stern–Gerlach experiment, and in 1944, Rabi was awarded the Nobel Prize in Physics for this work. In 1946, Felix Bloch and Edward Mills Purcell expanded the technique for use on liquids and solids, for which they shared the Nobel Prize in Physics in 1952.
Yevgeny Zavoisky likely observed nuclear magnetic resonance in 1941, well before Felix Bloch and Edward Mills Purcell, but dismissed the results as not reproducible.
Purcell had worked on the development of radar during World War II at the Massachusetts Institute of Technology's Radiation Laboratory. His work during that project on the production and detection of radio frequency power and on the absorption of such RF power by matter laid the foundation for Rabi's discovery of NMR.
Rabi, Bloch, and Purcell observed that magnetic nuclei, like and , could absorb RF energy when placed in a magnetic field and when the RF was of a frequency specific to the identity of the nuclei. When this absorption occurs, the nucleus is described as being "in resonance". Different atomic nuclei within a molecule resonate at different (radio) frequencies for the same magnetic field strength. The observation of such magnetic resonance frequencies of the nuclei present in a molecule allows any trained user to discover essential chemical and structural information about the molecule.
The development of NMR as a technique in analytical chemistry and biochemistry parallels the development of electromagnetic technology and advanced electronics and their introduction into civilian use.
Theory of nuclear magnetic resonance.
Nuclear spin and magnets.
All nucleons, that is neutrons and protons, composing any atomic nucleus, have the intrinsic quantum property of spin. The overall spin of the nucleus is determined by the spin quantum number "S". If the number of both the protons and neutrons in a given nuclide are even then , i.e. there is no overall spin. Then, just as electrons pair up in atomic orbitals, so do even numbers of protons or even numbers of neutrons (which are also spin- particles and hence fermions) pair up giving zero overall spin.
However, a proton and neutron will have lower energy when their spins are parallel, not anti-parallel. Parallel spin alignment does not infringe upon the Pauli Exclusion Principle. The lowering of energy for parallel spins has to do with the quark structure of these two nucleons. Therefore, the spin ground state for the deuteron (the deuterium nucleus, or the 2H isotope of hydrogen)—that has only a proton and a neutron—corresponds to a spin value of 1, "not of zero". The single, isolated deuteron therefore exhibits an NMR absorption spectrum characteristic of a quadrupolar nucleus of spin 1, which in the "rigid" state at very low temperatures is a characteristic ('Pake') "doublet", (not a singlet as for a single, isolated 1H, or any other isolated fermion or dipolar nucleus of spin 1/2). On the other hand, because of the Pauli Exclusion Principle, the tritium isotope of hydrogen must have a pair of anti-parallel spin neutrons (of total spin zero for the neutron-spin pair), plus a proton of spin 1/2. Therefore, the character of the tritium nucleus is again magnetic dipolar, "not quadrupolar"—like its non-radioactive deuteron cousin—and the tritium nucleus total spin value is again 1/2, just like for the simpler, abundant hydrogen isotope, 1H nucleus (the "proton"). The NMR absorption (radio) frequency for tritium is however slightly higher than that of 1H because the tritium nucleus has a slightly higher gyromagnetic ratio than 1H. In many other cases of "non-radioactive" nuclei, the overall spin is also non-zero. For example, the nucleus has an overall spin value .
A non-zero spin is thus always associated with a non-zero magnetic moment ("μ") via the relation , where "γ" is the gyromagnetic ratio. It is this magnetic moment that allows the observation of NMR absorption spectra caused by transitions between nuclear spin levels. Most nuclides (with some rare exceptions) that have both even numbers of protons and even numbers of neutrons, also have zero nuclear magnetic moments, and they also have zero magnetic dipole and quadrupole moments. Hence, such nuclides do not exhibit any NMR absorption spectra. Thus, is an example of a nuclide that has no NMR absorption, whereas , , and are nuclides that do exhibit NMR absorption spectra. The last two nuclei are quadrupolar nuclei whereas the preceding two nuclei ( and ) are dipolar ones.
Electron spin resonance (ESR) is a related technique in which transitions between electronic spin levels are detected rather than nuclear ones. The basic principles are similar but the instrumentation, data analysis, and detailed theory are significantly different. Moreover, there is a much smaller number of molecules and materials with unpaired electron spins that exhibit ESR (or electron paramagnetic resonance (EPR)) absorption than those that have NMR absorption spectra. ESR has much higher sensitivity than NMR does.
Values of spin angular momentum.
The angular momentum associated with nuclear spin is quantized. This means both that the magnitude of angular momentum is quantized (i.e. "S" can only take on a restricted range of values), and also that the orientation of the associated angular momentum is quantized. The associated quantum number is known as the magnetic quantum number, "m", and can take values from +"S" to −"S", in integer steps. Hence for any given nucleus, there are a total of angular momentum states.
The z-component of the angular momentum vector (S) is therefore , where "ħ" is the reduced Planck constant. The z-component of the magnetic moment is simply:
Spin behavior in a magnetic field.
Consider nuclei which have a spin of one-half, like , or . The nucleus has two possible spin states: "m" = or "m" = − (also referred to as spin-up and spin-down, or sometimes α and β spin states, respectively). These states are degenerate, that is they have the same energy. Hence the number of atoms in these two states will be approximately equal at thermal equilibrium.
If a nucleus is placed in a magnetic field, however, the interaction between the nuclear magnetic moment and the external magnetic field mean the two states no longer have the same energy. The energy of a magnetic moment μ when in a magnetic field B0 is given by:
Usually the "z" axis is chosen to be along B0, and the above expression reduces to:
or alternatively:
As a result the different nuclear spin states have different energies in a non-zero magnetic field. In less formal language, we can talk about the two spin states of a spin as being "aligned" either with or against the magnetic field. If γ is positive (true for most isotopes) then is the lower energy state.
The energy difference between the two states is:
and this difference results in a small population bias toward the lower energy state.
Magnetic resonance by nuclei.
Resonant absorption by nuclear spins will occur only when electromagnetic radiation of the correct frequency (e.g., equaling the Larmor precession rate) is being applied to match the energy difference between the nuclear spin levels in a constant magnetic field of the appropriate strength. The energy of an absorbed photon is then , where ν0 is the resonance radiofrequency that has to match (that is, it has to be equal to the Larmor precession frequency νL of the nuclear magnetization in the constant magnetic field B0). Hence, a magnetic resonance absorption will only occur when , which is when . Such magnetic resonance frequencies typically correspond to the radio frequency (or RF) range of the electromagnetic spectrum for magnetic fields up to roughly 20 T. It is this magnetic resonant absorption which is detected in NMR. 
Nuclear shielding.
It might appear from the above that all nuclei of the same nuclide (and hence the same "γ") would resonate at the same frequency. This is not the case. The most important perturbation of the NMR frequency for applications of NMR is the "shielding" effect of the surrounding shells of electrons. Electrons, similar to the nucleus, are also charged and rotate with a spin to produce a magnetic field opposite to the magnetic field produced by the nucleus. In general, this electronic shielding reduces the magnetic field "at the nucleus" (which is what determines the NMR frequency).
As a result the energy gap is reduced, and the frequency required to achieve resonance is also reduced. This shift in the NMR frequency due to the electronic molecular orbital coupling to the external magnetic field is called chemical shift, and it explains why NMR is able to probe the chemical structure of molecules, which depends on the electron density distribution in the corresponding molecular orbitals. If a nucleus in a specific chemical group is shielded to a higher degree by a higher electron density of its surrounding molecular orbital, then its NMR frequency will be shifted "upfield" (that is, a lower chemical shift), whereas if it is less shielded by such surrounding electron density, then its NMR frequency will be shifted "downfield" (that is, a higher chemical shift).
Unless the local symmetry of such molecular orbitals is very high (leading to "isotropic" shift), the shielding effect will depend on the orientation of the molecule with respect to the external field (B0). In solid-state NMR spectroscopy, magic angle spinning is required to average out this orientation dependence in order to obtain values close to the average chemical shifts. This is unnecessary in conventional NMR investigations of molecules, since rapid "molecular tumbling" averages out the chemical shift anisotropy (CSA). In this case, the term "average" chemical shift (ACS) is used.
Relaxation.
The process called population relaxation refers to nuclei that return to the thermodynamic state in the magnet. This process is also called "T"1, "spin-lattice" or "longitudinal magnetic" relaxation, where "T"1 refers to the mean time for an individual nucleus to return to its thermal equilibrium state of the spins. Once the nuclear spin population is relaxed, it can be probed again, since it is in the initial, equilibrium (mixed) state.
The precessing nuclei can also fall out of alignment with each other (returning the net magnetization vector to a non-precessing field) and stop producing a signal. This is called "T"2 or "transverse relaxation". Because of the difference in the actual relaxation mechanisms involved (for example, inter-molecular vs. intra-molecular magnetic dipole-dipole interactions ), "T"1 is usually (except in rare cases) longer than "T"2 (that is, slower spin-lattice relaxation, for example because of smaller dipole-dipole interaction effects). In practice, the value of formula_6 which is the actually observed decay time of the observed NMR signal, or free induction decay, (to 1/e of the initial amplitude immediately after the resonant RF pulse)-- also depends on the static magnetic field inhomogeneity, which is quite significant. (There is also a
smaller but significant contribution to the observed FID shortening from the RF inhomogeneity of the resonant pulse). In the corresponding FT-NMR spectrum—meaning the Fourier transform of the free induction decay—the formula_6 time is inversely related to the width of the NMR signal in frequency units. Thus, a nucleus with a long "T"2 relaxation time gives rise to a very sharp NMR peak in the FT-NMR spectrum for a very homogeneous ("well-shimmed") static magnetic field, whereas nuclei with shorter "T"2 values give rise to broad FT-NMR peaks even when the magnet is shimmed well. Both "T"1 and "T"2 depend on the rate of molecular motions as well as the gyromagnetic ratios of both the resonating and their strongly interacting, next-neighbor nuclei that are not at resonance.
A Hahn echo decay experiment can be used to measure the dephasing time, as shown in the animation below. The size of the echo is recorded for different spacings of the two pulses. This reveals the decoherence which is not refocused by the formula_8 pulse. In simple cases, an exponential decay is measured which is described by the formula_9 time.
NMR spectroscopy.
NMR spectroscopy is one of the principal techniques used to obtain physical, chemical, electronic and structural information about molecules due to either the chemical shift, Zeeman effect, or the Knight shift effect, or a combination of both, on the resonant frequencies of the nuclei present in the sample. It is a powerful technique that can provide detailed information on the topology, dynamics and three-dimensional structure of molecules in solution and the solid state. Thus, structural and dynamic information is obtainable (with or without "magic angle" spinning (MAS)) from NMR studies of quadrupolar nuclei (that is, those nuclei with spin ) even in the presence of magnetic "dipole-dipole" interaction broadening (or simply, dipolar broadening) which is always much smaller than the quadrupolar interaction strength because it is a magnetic vs. an electric interaction effect.
Additional structural and chemical information may be obtained by performing double-quantum NMR experiments for quadrupolar nuclei such as . Also, nuclear magnetic resonance is one of the techniques that has been used to design quantum automata, and also build elementary quantum computers.
Continuous-wave (CW) spectroscopy.
In its first few decades, nuclear magnetic resonance spectrometers used a technique known as continuous-wave spectroscopy (CW spectroscopy). Although NMR spectra could be, and have been, obtained using a fixed magnetic field and sweeping the frequency of the electromagnetic radiation, this more typically involved using a fixed frequency source and varying the current (and hence magnetic field) in an electromagnet to observe the resonant absorption signals. This is the origin of the counterintuitive, but still common, "high field" and "low field" terminology for low frequency and high frequency regions respectively of the NMR spectrum.
CW spectroscopy is inefficient in comparison with Fourier analysis techniques (see below) since it probes the NMR response at individual frequencies in succession. Since the NMR signal is intrinsically weak, the observed spectrum suffers from a poor signal-to-noise ratio. This can be mitigated by signal averaging i.e. adding the spectra from repeated measurements. While the NMR signal is constant between scans and so adds linearly, the random noise adds more slowly – proportional to the square-root of the number of spectra (see random walk). Hence the overall signal-to-noise ratio increases as the square-root of the number of spectra measured.
Fourier-transform spectroscopy.
Most applications of NMR involve full NMR spectra, that is, the intensity of the NMR signal as a function of frequency. Early attempts to acquire the NMR spectrum more efficiently than simple CW methods involved illuminating the target simultaneously with more than one frequency. A revolution in NMR occurred when short pulses of radio-frequency radiation began to be used—centered at the middle of the NMR spectrum. In simple terms, a short pulse of a given "carrier" frequency "contains" a range of frequencies centered about the carrier frequency, with the range of excitation (bandwidth) being inversely proportional to the pulse duration, i.e. the Fourier transform of a short pulse contains contributions from all the frequencies in the neighborhood of the principal frequency. The restricted range of the NMR frequencies made it relatively easy to use short (millisecond to microsecond) radio frequency pulses to excite the entire NMR spectrum.
Applying such a pulse to a set of nuclear spins simultaneously excites all the single-quantum NMR transitions. In terms of the net magnetization vector, this corresponds to tilting the magnetization vector away from its equilibrium position (aligned along the external magnetic field). The out-of-equilibrium magnetization vector precesses about the external magnetic field vector at the NMR frequency of the spins. This oscillating magnetization vector induces a current in a nearby pickup coil, creating an electrical signal oscillating at the NMR frequency. This signal is known as the free induction decay (FID), and it contains the vector sum of the NMR responses from all the excited spins. In order to obtain the frequency-domain NMR spectrum (NMR absorption intensity vs. NMR frequency) this time-domain signal (intensity vs. time) must be Fourier transformed. Fortunately the development of Fourier Transform NMR coincided with the development of digital computers and the digital Fast Fourier Transform. Fourier methods can be applied to many types of spectroscopy. (See the full article on Fourier transform spectroscopy.)
Richard R. Ernst was one of the pioneers of pulse NMR, and he won a Nobel Prize in chemistry in 1991 for his work on Fourier Transform NMR and his development of multi-dimensional NMR (see below).
Multi-dimensional NMR Spectroscopy.
The use of pulses of different shapes, frequencies and durations in specifically designed patterns or "pulse sequences" allows the spectroscopist to extract many different types of information about the molecule. Multi-dimensional nuclear magnetic resonance spectroscopy is a kind of FT NMR in which there are at least two pulses and, as the experiment is repeated, the pulse sequence is
systematically varied. In "multidimensional nuclear magnetic resonance" there will be a sequence of pulses and, at least, one variable time period. In three dimensions, two time sequences will be varied. In four dimensions, three will be varied.
There are many such experiments. In one, these time intervals allow (amongst other things) magnetization transfer between nuclei and, therefore, the detection of the kinds of nuclear-nuclear interactions that allowed for the magnetization transfer. Interactions that can be detected are usually classified into two kinds. There are "through-bond" interactions and "through-space" interactions, the latter usually being a consequence of the nuclear Overhauser effect. Experiments of the nuclear Overhauser variety may be employed to establish distances between atoms, as for example by 2D-FT NMR of molecules in solution.
Although the fundamental concept of 2D-FT NMR was proposed by Jean Jeener from the Free University of Brussels at an International Conference, this idea was largely developed by Richard Ernst who won the 1991 Nobel prize in Chemistry for his work in FT NMR, including multi-dimensional FT NMR, and especially 2D-FT NMR of small molecules. Multi-dimensional FT NMR experiments were then further developed into powerful methodologies for studying biomolecules in solution, in particular for the determination of the structure of biopolymers such as proteins or even small nucleic acids.
In 2002 Kurt Wüthrich shared the Nobel Prize in Chemistry (with John Bennett Fenn and Koichi Tanaka) for his work with protein FT NMR in solution.
Solid-state NMR spectroscopy.
This technique complements X-ray crystallography in that it is frequently applicable to molecules in a liquid or liquid crystal phase, whereas crystallography, as the name implies, is performed on molecules in a solid phase. Though nuclear magnetic resonance is used to study solids, extensive atomic-level molecular structural detail is especially challenging to obtain in the solid state. There is little signal averaging by thermal motion in the solid state, where most molecules can only undergo restricted vibrations and rotations at room temperature, each in a slightly different electronic environment, therefore exhibiting a different NMR absorption peak. Such a variation in the electronic environment of the resonating nuclei results in a blurring of the observed spectra—which is often only a broad Gaussian band for non-quadrupolar spins in a solid- thus making the interpretation of such "dipolar" and "chemical shift anisotropy" (CSA) broadened spectra either very difficult or impossible.
Professor Raymond Andrew at the University of Nottingham in the UK pioneered the development of high-resolution solid-state nuclear magnetic resonance. He was the first to report the introduction of the MAS (magic angle sample spinning; MASS) technique that allowed him to achieve spectral resolution in solids sufficient to distinguish between chemical groups with either different chemical shifts or distinct Knight shifts. In MASS, the sample is spun at several kilohertz around an axis that makes the so-called magic angle "θ"m (which is ~54.74°, where cos2"θ"m = 1/3) with respect to the direction of the static magnetic field B0; as a result of such magic angle sample spinning, the chemical shift anisotropy bands are averaged to their corresponding average (isotropic) chemical shift values. The above expression involving cos2"θ"m has its origin in a calculation that predicts the magnetic dipolar interaction effects to cancel out for the specific value of "θ"m called the magic angle. One notes that correct alignment of the sample rotation axis as close as possible to "θ"m is essential for cancelling out the dipolar interactions whose strength for angles sufficiently far from "θ"m is usually greater than ~10 kHz for C-H bonds in solids, for example, and it is thus greater than their CSA values.
There are different angles for the sample spinning relative to the applied field for the averaging of quadrupole interactions and paramagnetic interactions, correspondingly ~30.6° and ~70.1°
A concept developed by Sven Hartmann and Erwin Hahn was utilized in transferring magnetization from protons to less sensitive nuclei (popularly known as cross-polarization) by M.G. Gibby, Alex Pines and John S. Waugh. Then, Jake Schaefer and Ed Stejskal demonstrated also the powerful use of cross-polarization under MASS conditions (CP-MAS) which is now routinely employed to measure high resolution spectra of low-abundance and low-sensitivity nuclei, namely carbon-13, in solids.
Sensitivity.
Because the intensity of nuclear magnetic resonance signals and, hence, the sensitivity of the technique depends on the strength of the magnetic field the technique has also advanced over the decades with the development of more powerful magnets. Advances made in audio-visual technology have also improved the signal-generation and processing capabilities of newer instruments.
As noted above, the sensitivity of nuclear magnetic resonance signals is also dependent on the presence of a magnetically susceptible nuclide and, therefore, either on the natural abundance of such nuclides or on the ability of the experimentalist to artificially enrich the molecules, under study, with such nuclides. The most abundant naturally occurring isotopes of hydrogen and phosphorus (for example) are both magnetically susceptible and readily useful for nuclear magnetic resonance spectroscopy. In contrast, carbon and nitrogen have useful isotopes but which occur only in very low natural abundance.
Other limitations on sensitivity arise from the quantum-mechanical nature of the phenomenon. For quantum states separated by energy equivalent to radio frequencies, thermal energy from the environment causes the populations of the states to be close to equal. Since incoming radiation is equally likely to cause stimulated emission (a transition from the upper to the lower state) as absorption, the NMR effect depends on an excess of nuclei in the lower states. Several factors can reduce sensitivity, including
Isotopes.
Many isotopes of chemical elements can be used for NMR analysis.
Commonly used nuclei:
Other nuclei (usually used in the studies of their complexes and chemical binding, or to detect presence of the element):
Applications.
Medicine.
The application of nuclear magnetic resonance best known to the general public is magnetic resonance imaging for medical diagnosis and magnetic resonance microscopy in research settings, however, it is also widely used in chemical studies, notably in NMR spectroscopy such as proton NMR, carbon-13 NMR, deuterium NMR and phosphorus-31 NMR. Biochemical information can also be obtained from living tissue (e.g. human brain tumors) with the technique known as in vivo magnetic resonance spectroscopy or chemical shift NMR Microscopy.
These studies are possible because nuclei are surrounded by orbiting electrons, which are charged particles that generate small, local magnetic fields that add to or subtract from the external magnetic field, and so will partially shield the nuclei. The amount of shielding depends on the exact local environment. For example, a hydrogen bonded to an oxygen will be shielded differently from a hydrogen bonded to a carbon atom. In addition, two hydrogen nuclei can interact via a process known as spin-spin coupling, if they are on the same molecule, which will split the lines of the spectra in a recognizable way.
As one of the two major spectroscopic techniques used in metabolomics, NMR is used to generate metabolic fingerprints from biological fluids to obtain information about disease states or toxic insults.
Chemistry.
By studying the peaks of nuclear magnetic resonance spectra, chemists can determine the structure of many compounds. It can be a very selective technique, distinguishing among many atoms within a molecule or collection of molecules of the same type but which differ only in terms of their local chemical environment. NMR spectroscopy is used to unambiguously identify known and novel compounds, and as such, is usually required by scientific journals for identity confirmation of synthesized new compounds. See the articles on carbon-13 NMR and proton NMR for detailed discussions.
By studying "T"2 information, a chemist can determine the identity of a compound by comparing the observed nuclear precession frequencies to known frequencies. Further structural data can be elucidated by observing "spin-spin coupling", a process by which the precession frequency of a nucleus can be influenced by the magnetization transfer from nearby chemically bound nuclei. Spin-spin coupling is observed in NMR of hydrogen-1 ( NMR), since its natural abundance is nearly 100%; isotope enrichment is required for most other elements.
Because the nuclear magnetic resonance "timescale" is rather slow, compared to other spectroscopic methods, changing the temperature of a "T"2*experiment can also give information about fast reactions, such as the Cope rearrangement or about structural dynamics, such as ring-flipping in cyclohexane. At low enough temperatures, a distinction can be made between the axial and equatorial hydrogens in cyclohexane.
An example of nuclear magnetic resonance being used in the determination of a structure is that of buckminsterfullerene (often called "buckyballs", composition C60). This now famous form of carbon has 60 carbon atoms forming a sphere. The carbon atoms are all in identical environments and so should see the same internal H field. Unfortunately, buckminsterfullerene contains no hydrogen and so nuclear magnetic resonance has to be used. spectra require longer acquisition times since carbon-13 is not the common isotope of carbon (unlike hydrogen, where is the common isotope). However, in 1990 the spectrum was obtained by R. Taylor and co-workers at the University of Sussex and was found to contain a single peak, confirming the unusual structure of buckminsterfullerene.
Purity determination (w/w NMR).
NMR is primarily used for structural determination, however it can also be used for purity determination, providing that the structure and molecular weight of the compound is known. This technique requires the use of an internal standard of a known purity. Typically this standard will have a high molecular weight to facilitate accurate weighing, but relatively few protons so as to give a clear peak for later integration e.g. 1,2,3,4-tetrachloro-5-nitrobenzene. Accurately weighed portions of both the standard and sample are combined and analysed by NMR. Suitable peaks are selected for both compounds and the purity of the sample determined via the following equation.
Where:
Non-destructive testing.
Nuclear magnetic resonance is extremely useful for analyzing samples non-destructively. Radio waves and static magnetic fields easily penetrate many types of matter and anything that is not inherently ferromagnetic. For example, various expensive biological samples, such as nucleic acids, including RNA and DNA, or proteins, can be studied using nuclear magnetic resonance for weeks or months before using destructive biochemical experiments. This also makes nuclear magnetic resonance a good choice for analyzing dangerous samples.
Acquisition of dynamic information.
In addition to providing static information on molecules by determining their 3D structures in solution, one of the remarkable advantages of NMR over X-ray crystallography is that it can be used to obtain important dynamic information.
Data acquisition in the petroleum industry.
Another use for nuclear magnetic resonance is data acquisition in the petroleum industry for petroleum and natural gas exploration and recovery. A borehole is drilled into rock and sedimentary strata into which nuclear magnetic resonance logging equipment is lowered. Nuclear magnetic resonance analysis of these boreholes is used to measure rock porosity, estimate permeability from pore size distribution and identify pore fluids (water, oil and gas). These instruments are typically low field NMR spectrometers.
Flow probes for NMR spectroscopy.
Recently, real-time applications of NMR in liquid media have been developed using specifically designed flow probes (flow cell assemblies) which can replace standard tube probes. This has enabled techniques that can incorporate the use of high performance liquid chromatography (HPLC) or other continuous flow sample introduction devices.
Process control.
NMR has now entered the arena of real-time process control and process optimization in oil refineries and petrochemical plants. Two different types of NMR analysis are utilized to provide real time analysis of feeds and products in order to control and optimize unit operations. Time-domain NMR (TD-NMR) spectrometers operating at low field (2–20 MHz for ) yield free induction decay data that can be used to determine absolute hydrogen content values, rheological information, and component composition. These spectrometers are used in mining, polymer production, cosmetics and food manufacturing as well as coal analysis. High resolution FT-NMR spectrometers operating in the 60 MHz range with shielded permanent magnet systems yield high resolution NMR spectra of refinery and petrochemical streams. The variation observed in these spectra with changing physical and chemical properties is modeled using chemometrics to yield predictions on unknown samples. The prediction results are provided to control systems via analogue or digital outputs from the spectrometer.
Earth's field NMR.
In the Earth's magnetic field, NMR frequencies are in the audio frequency range, or the very low frequency and ultra low frequency bands of the radio frequency spectrum. Earth's field NMR (EFNMR) is typically stimulated by applying a relatively strong dc magnetic field pulse to the sample and, after the end of the pulse, analyzing the resulting low frequency alternating magnetic field that occurs in the Earth's magnetic field due to free induction decay (FID). These effects are exploited in some types of magnetometers, EFNMR spectrometers, and MRI imagers. Their inexpensive portable nature makes these instruments valuable for field use and for teaching the principles of NMR and MRI.
An important feature of EFNMR spectrometry compared with high-field NMR is that some aspects of molecular structure can be observed more clearly at low fields and low frequencies, whereas other aspects observable at high fields are not observable at low fields. This is because:
Zero Field NMR.
In Zero Field NMR all magnetic fields are shielded such that magnetic fields below nT (nano-Tesla) are achieved and the nuclear precession frequencies of all nuclei are close to zero and indistinguishable. Under those circumstances the observed spectra are no-longer dictated by chemical shifts but primarily by J-coupling interactions which are independent of the external magnetic field. Since inductive detection schemes are not sensitive at very low frequencies, on the order of the J-couplings (typically between 0 and 1000 Hz), alternative detection schemes are used. Specifically, sensitive magnetometers turn out to be good detectors for Zero Field NMR.
A zero magnetic field environment does not provide any polarization hence it is the combination of zero-field NMR with hyperpolarization schemes that makes zero field NMR attractive.
Quantum computing.
NMR quantum computing uses the spin states of molecules as qubits. NMR differs from other implementations of quantum computers in that it uses an ensemble of systems, in this case molecules.
Magnetometers.
Various magnetometers use NMR effects to measure magnetic fields, including proton precession magnetometers (PPM) (also known as proton magnetometers), and Overhauser magnetometers. See also Earth's field NMR.
Makers of NMR equipment.
Major NMR instrument makers include Magritek, Oxford Instruments, Bruker, Spinlock SRL, General Electric, JEOL, Kimble Chase, Philips, Siemens AG, and Agilent Technologies, Inc. (who own Varian, Inc.).

</doc>
<doc id="21285" url="https://en.wikipedia.org/wiki?curid=21285" title="Nuclear physics">
Nuclear physics

Nuclear physics is the field of physics that studies atomic nuclei and their constituents and interactions. The most commonly known application of nuclear physics is nuclear power generation, but the research has led to applications in many fields, including nuclear medicine and magnetic resonance imaging, nuclear weapons, ion implantation in materials engineering, and radiocarbon dating in geology and archaeology.
The field of particle physics evolved out of nuclear physics and is typically taught in close association with nuclear physics.
History.
The history of nuclear physics as a discipline distinct from atomic physics starts with the discovery of radioactivity by Henri Becquerel in 1896, while investigating phosphorescence in uranium salts. The discovery of the electron by J. J. Thomson a year later was an indication that the atom had internal structure. At the beginning of the 20th century the accepted model of the atom was J. J. Thomson's "plum pudding" model in which the atom was a positively charged ball with smaller negatively charged electrons embedded inside it.
In the years that followed, radioactivity was extensively investigated, notably by the husband and wife team of Pierre Curie and Marie Curie and by Ernest Rutherford and his collaborators. By the turn of the century physicists had also discovered three types of radiation emanating from atoms, which they named alpha, beta, and gamma radiation. Experiments by Otto Hahn in 1911 and by James Chadwick in 1914 discovered that the beta decay spectrum was continuous rather than discrete. That is, electrons were ejected from the atom with a continuous range of energies, rather than the discrete amounts of energy that were observed in gamma and alpha decays. This was a problem for nuclear physics at the time, because it seemed to indicate that energy was not conserved in these decays.
The 1903 Nobel Prize in Physics was awarded jointly to Becquerel for his discovery and to Pierre Curie and Marie Curie for their subsequent research into radioactivity. Rutherford was awarded the Nobel Prize in Chemistry in 1908 for his "investigations into the disintegration of the elements and the chemistry of radioactive substances".
In 1905, Albert Einstein formulated the idea of mass–energy equivalence. While the work on radioactivity by Becquerel and Marie Curie predates this, an explanation of the source of the energy of radioactivity would have to wait for the discovery that the nucleus itself was composed of smaller constituents, the nucleons.
Rutherford's team discovers the nucleus.
In 1907 Ernest Rutherford published "Radiation of the α Particle from Radium in passing through Matter." Hans Geiger expanded on this work in a communication to the Royal Society with experiments he and Rutherford had done, passing alpha particles through air, aluminum foil and gold leaf. More work was published in 1909 by Geiger and Marsden, and further greatly expanded work was published in 1910 by Geiger. In 1911-1912 Rutherford went before the Royal Society to explain the experiments and propound the new theory of the atomic nucleus as we now understand it.
The key experiment behind this announcement was performed in 1910 at the University of Manchester: Ernest Rutherford's team performed a remarkable experiment in which Geiger and Ernest Marsden under Rutherford's supervision fired alpha particles (helium nuclei) at a thin film of gold foil. The plum pudding model had predicted that the alpha particles should come out of the foil with their trajectories being at most slightly bent. But Rutherford instructed his team to look for something that shocked him to observe: a few particles were scattered through large angles, even completely backwards in some cases. He likened it to firing a bullet at tissue paper and having it bounce off. The discovery, with Rutherford's analysis of the data in 1911, led to the Rutherford model of the atom, in which the atom had a very small, very dense nucleus containing most of its mass, and consisting of heavy positively charged particles with embedded electrons in order to balance out the charge (since the neutron was unknown). As an example, in this model (which is not the modern one) nitrogen-14 consisted of a nucleus with 14 protons and 7 electrons (21 total particles) and the nucleus was surrounded by 7 more orbiting electrons.
The Rutherford model worked quite well until studies of nuclear spin were carried out by Franco Rasetti at the California Institute of Technology in 1929. By 1925 it was known that protons and electrons each had a spin of . In the Rutherford model of nitrogen-14, 20 of the total 21 nuclear particles should have paired up to cancel each other's spin, and the final odd particle should have left the nucleus with a net spin of . Rasetti discovered, however, that nitrogen-14 had a spin of 1.
James Chadwick discovers the neutron.
In 1932 Chadwick realized that radiation that had been observed by Walther Bothe, Herbert Becker, Irène and Frédéric Joliot-Curie was actually due to a neutral particle of about the same mass as the proton, that he called the neutron (following a suggestion from Rutherford about the need for such a particle). In the same year Dmitri Ivanenko suggested that there were no electrons in the nucleus — only protons and neutrons — and that neutrons were spin particles which explained the mass not due to protons. The neutron spin immediately solved the problem of the spin of nitrogen-14, as the one unpaired proton and one unpaired neutron in this model each contributed a spin of in the same direction, giving a final total spin of 1.
With the discovery of the neutron, scientists could at last calculate what fraction of binding energy each nucleus had, by comparing the nuclear mass with that of the protons and neutrons which composed it. Differences between nuclear masses were calculated in this way. When nuclear reactions were measured, these were found to agree with Einstein's calculation of the equivalence of mass and energy to within 1% as of 1934.
Proca's equations of the massive vector boson field.
Alexandru Proca was the first to develop and report the massive vector boson field equations and a theory of the mesonic field of nuclear forces. Proca's equations were known to Wolfgang Pauli who mentioned the equations in his Nobel address, and they were also known to Yukawa, Wentzel, Taketani, Sakata, Kemmer, Heitler, and Fröhlich who appreciated the content of Proca's equations for developing a theory of the atomic nuclei in Nuclear Physics.
Yukawa's meson postulated to bind nuclei.
In 1935 Hideki Yukawa proposed the first significant theory of the strong force to explain how the nucleus holds together. In the Yukawa interaction a virtual particle, later called a meson, mediated a force between all nucleons, including protons and neutrons. This force explained why nuclei did not disintegrate under the influence of proton repulsion, and it also gave an explanation of why the attractive strong force had a more limited range than the electromagnetic repulsion between protons. Later, the discovery of the pi meson showed it to have the properties of Yukawa's particle.
With Yukawa's papers, the modern model of the atom was complete. The center of the atom contains a tight ball of neutrons and protons, which is held together by the strong nuclear force, unless it is too large. Unstable nuclei may undergo alpha decay, in which they emit an energetic helium nucleus, or beta decay, in which they eject an electron (or positron). After one of these decays the resultant nucleus may be left in an excited state, and in this case it decays to its ground state by emitting high energy photons (gamma decay).
The study of the strong and weak nuclear forces (the latter explained by Enrico Fermi via Fermi's interaction in 1934) led physicists to collide nuclei and electrons at ever higher energies. This research became the science of particle physics, the crown jewel of which is the standard model of particle physics which describes the strong, weak, and electromagnetic forces.
Modern nuclear physics.
A heavy nucleus can contain hundreds of nucleons. This means that with some approximation it can be treated as a classical system, rather than a quantum-mechanical one. In the resulting liquid-drop model, the nucleus has an energy which arises partly from surface tension and partly from electrical repulsion of the protons. The liquid-drop model is able to reproduce many features of nuclei, including the general trend of binding energy with respect to mass number, as well as the phenomenon of nuclear fission.
Superimposed on this classical picture, however, are quantum-mechanical effects, which can be described using the nuclear shell model, developed in large part by Maria Goeppert Mayer and J. Hans D. Jensen. Nuclei with certain numbers of neutrons and protons (the magic numbers 2, 8, 20, 28, 50, 82, 126, ...) are particularly stable, because their shells are filled.
Other more complicated models for the nucleus have also been proposed, such as the interacting boson model, in which pairs of neutrons and protons interact as bosons, analogously to Cooper pairs of electrons.
Much of current research in nuclear physics relates to the study of nuclei under extreme conditions such as high spin and excitation energy. Nuclei may also have extreme shapes (similar to that of Rugby balls or even pears) or extreme neutron-to-proton ratios. Experimenters can create such nuclei using artificially induced fusion or nucleon transfer reactions, employing ion beams from an accelerator. Beams with even higher energies can be used to create nuclei at very high temperatures, and there are signs that these experiments have produced a phase transition from normal nuclear matter to a new state, the quark–gluon plasma, in which the quarks mingle with one another, rather than being segregated in triplets as they are in neutrons and protons.
Nuclear decay.
Eighty elements have at least one stable isotope which is never observed to decay, amounting to a total of about 254 stable isotopes. However, thousands of isotopes have been characterized as unstable. These "radioisotopes" decay over time scales ranging from fractions of a second to trillions of years.
The most stable nuclei fall within certain ranges or balances of composition of neutrons and protons: too few or too many neutrons (in relation to the number of protons) will cause it to decay. For example, in beta decay a nitrogen-16 atom (7 protons, 9 neutrons) is converted to an oxygen-16 atom (8 protons, 8 neutrons) within a few seconds of being created. In this decay a neutron in the nitrogen nucleus is converted by the weak interaction into a proton, an electron and an antineutrino. The element is transmuted to another element, with a different number of protons.
In alpha decay (which typically occurs in the heaviest nuclei) the radioactive element decays by emitting a helium nucleus (2 protons and 2 neutrons), giving another element, plus helium-4. In many cases this process continues through several steps of this kind, including other types of decays (usually beta decay) until a stable element is formed.
In gamma decay, a nucleus decays from an excited state into a lower energy state, by emitting a gamma ray. The element is not changed to another element in the process (no nuclear transmutation is involved).
Other more exotic decays are possible (see the main article). For example, in internal conversion decay, the energy from an excited nucleus may eject one of the inner orbital electrons from the atom, in a process which produces high speed electrons, but is not beta decay, and (unlike beta decay) does not transmute one element to another.
Nuclear fusion.
In nuclear fusion, two low mass nuclei come into very close contact with each other, so that the strong force fuses them. It requires a large amount of energy for the strong or nuclear forces to overcome the electrical repulsion between the nuclei in order to fuse them; therefore nuclear fusion can only take place at very high temperatures or high pressures. When nucelei fuse, a very large amount of energy is released and the combined nucleus assumes a lower energy level. The binding energy per nucleon increases with mass number up to nickel-62. Stars like the Sun are powered by the fusion of four protons into a helium nucleus, two positrons, and two neutrinos. The uncontrolled fusion of hydrogen into helium is known as thermonuclear runaway. A frontier in current research at various institutions, for example the Joint European Torus (JET) and ITER, is the development of an economically viable method of using energy from a controlled fusion reaction.
Nuclear fusion is the origin of the energy (including in the form of light and other electromagnetic radiation) produced by the core of all stars including our own Sun.
Nuclear fission.
Nuclear fission is the reverse process to fusion. For nuclei heavier than nickel-62 the binding energy per nucleon decreases with the mass number. It is therefore possible for energy to be released if a heavy nucleus breaks apart into two lighter ones.
The process of alpha decay is in essence a special type of spontaneous nuclear fission. It is a highly asymmetrical fission because the four particles which make up the alpha particle are especially tightly bound to each other, making production of this nucleus in fission particularly likely.
From certain of the heaviest nuclei whose fission produces free neutrons, and which also easily absorb neutrons to initiate fission, a self-igniting type of neutron-initiated fission can be obtained, in a chain reaction. Chain reactions were known in chemistry before physics, and in fact many familiar processes like fires and chemical explosions are chemical chain reactions. The fission or "nuclear" chain-reaction, using fission-produced neutrons, is the source of energy for nuclear power plants and fission type nuclear bombs, such as those detonated in Hiroshima and Nagasaki, Japan, at the end of World War II. Heavy nuclei such as uranium and thorium may also undergo spontaneous fission, but they are much more likely to undergo decay by alpha decay.
For a neutron-initiated chain reaction to occur, there must be a critical mass of the relevant isotope present in a certain space under certain conditions. The conditions for the smallest critical mass require the conservation of the emitted neutrons and also their slowing or moderation so that there is a greater cross-section or probability of them initiating another fission. In two regions of Oklo, Gabon, Africa, natural nuclear fission reactors were active over 1.5 billion years ago. Measurements of natural neutrino emission have demonstrated that around half of the heat emanating from the Earth's core results from radioactive decay. However, it is not known if any of this results from fission chain reactions.
Production of "heavy" elements (atomic number greater than five).
According to the theory, as the Universe cooled after the Big Bang it eventually became possible for common subatomic particles as we know them (neutrons, protons and electrons) to exist. The most common particles created in the Big Bang which are still easily observable to us today were protons and electrons (in equal numbers). The protons would eventually form hydrogen atoms. Almost all the neutrons created in the Big Bang were absorbed into helium-4 in the first three minutes after the Big Bang, and this helium accounts for most of the helium in the universe today (see Big Bang nucleosynthesis).
Some relatively small quantities of elements beyond helium (lithium, beryllium, and perhaps some boron) were created in the Big Bang, as the protons and neutrons collided with each other, but all of the "heavier elements" (carbon, element number 6, and elements of greater atomic number) that we see today, were created inside stars during a series of fusion stages, such as the proton-proton chain, the CNO cycle and the triple-alpha process. Progressively heavier elements are created during the evolution of a star.
Since the binding energy per nucleon peaks around iron (56 nucleons), energy is only released in fusion processes involving smaller atoms than that. Since the creation of heavier nuclei by fusion requires energy, nature resorts to the process of neutron capture. Neutrons (due to their lack of charge) are readily absorbed by a nucleus. The heavy elements are created by either a "slow" neutron capture process (the so-called "s" process) or the "rapid", or "r" process. The "s" process occurs in thermally pulsing stars (called AGB, or asymptotic giant branch stars) and takes hundreds to thousands of years to reach the heaviest elements of lead and bismuth. The "r" process is thought to occur in supernova explosions which provide the necessary conditions of high temperature, high neutron flux and ejected matter. These stellar conditions make the successive neutron captures very fast, involving very neutron-rich species which then beta-decay to heavier elements, especially at the so-called waiting points that correspond to more stable nuclides with closed neutron shells (magic numbers).

</doc>
<doc id="21961" url="https://en.wikipedia.org/wiki?curid=21961" title="Nucleon">
Nucleon

In chemistry and physics, a nucleon is one of the particles that makes up the atomic nucleus. Each atomic nucleus consists of one or more nucleons, and each atom in turn consists of a cluster of nucleons surrounded by one or more electrons. There are two known kinds of nucleon: the neutron and the proton. The mass number of a given atomic isotope is identical to its number of nucleons. Thus the term nucleon number may be used in place of the more common terms mass number or atomic mass number.
Until the 1960s, nucleons were thought to be elementary particles, each of which would not then have been made up of smaller parts. Now they are known to be composite particles, made of three quarks bound together by the so-called strong interaction. The interaction between two or more nucleons is called internucleon interactions or nuclear force, which is also ultimately caused by the strong interaction. (Before the discovery of quarks, the term "strong interaction" referred to just internucleon interactions.)
Nucleons sit at the boundary where particle physics and nuclear physics overlap. Particle physics, particularly quantum chromodynamics, provides the fundamental equations that explain the properties of quarks and of the strong interaction. These equations explain quantitatively how quarks can bind together into protons and neutrons (and all the other hadrons). However, when multiple nucleons are assembled into an atomic nucleus (nuclide), these fundamental equations become too difficult to solve directly (see lattice QCD). Instead, nuclides are studied within nuclear physics, which studies nucleons and their interactions by approximations and models, such as the nuclear shell model. These models can successfully explain nuclide properties, for example, whether or not a certain nuclide undergoes radioactive decay.
The proton and neutron are both baryons and both fermions. They are quite similar. One carries a non-zero net charge and the other carries a zero net charge; the proton's mass is only 0.1% less than the neutron's. Thus, they can be viewed as two states of the same nucleon. They together form the isospin doublet (). In isospin space, neutrons can be rotationally transformed into protons, and vice versa. These nucleons are acted upon equally by the strong interaction. This implies that strong interaction is invariant when doing rotation transformation in isospin space. According to the Noether theorem, isospin is conserved with respect to the strong interaction.
Overview.
Properties.
Protons and neutrons are most important and best known for constituting atomic nuclei, but they can also be found on their own, not part of a larger nucleus. A proton on its own is the nucleus of the hydrogen-1 atom (1H). A neutron on its own is unstable (see below), but they can be found in nuclear reactions (see neutron radiation) and are used in scientific analysis (see neutron scattering).
Both the proton and neutron are made of three quarks. The proton is made of two up quarks and one down quark, while the neutron is one up quark and two down quarks. The quarks are held together by the strong force. It is also said that the quarks are held together by gluons, but this is just a different way to say the same thing (gluons mediate the strong force).
An up quark has electric charge + "e", and a down quark has charge − "e", so the total electric charge of the proton and neutron are +"e" and 0, respectively. The word "neutron" comes from the fact that it is electrically "neutral".
The mass of the proton and neutron is quite similar: The proton is or , while the neutron is or . The neutron is roughly 0.1% heavier. The similarity in mass can be explained roughly by the slight difference in mass of up quark and down quark composing the nucleons. However, detailed explanation remains an unsolved problem in particle physics.
The spin of both protons and neutrons is . This means they are fermions not bosons, and therefore, like electrons, they are subject to the Pauli exclusion principle. This is a very important fact in nuclear physics: Protons and neutrons in an atomic nucleus cannot all be in the same quantum state, but instead they spread out into nuclear shells analogous to electron shells in chemistry. Another reason that the spin of the proton and neutron is important is because it is the source of nuclear spin in larger nuclei. Nuclear spin is best known for its crucial role in the NMR/MRI technique for chemistry and biochemistry analysis.
The magnetic moment of a proton, denoted μp, is , while the magnetic moment of a neutron is μn = . These parameters are also important in NMR/MRI.
Stability.
A neutron by itself is an unstable particle: It undergoes decay (a type of radioactive decay) by turning into a proton, electron, and electron antineutrino, with a half-life around ten minutes. (See the Neutron article for further discussion of neutron decay.) A proton by itself is thought to be stable, or at least its lifetime is too long to measure. (This is an important issue in particle physics, see Proton decay.)
Inside a nucleus, on the other hand, both protons and neutrons can be stable or unstable, depending on the nuclide. Inside some nuclides, a neutron can turn into a proton (plus other particles) as described above; inside other nuclides the reverse can happen, where a proton turns into a neutron (plus other particles) through decay or electron capture; and inside still other nuclides, both protons and neutrons are stable and do not change form.
Antinucleons.
Both of the nucleons have corresponding antiparticles: The antiproton and the antineutron. These antimatter particles have the same mass and opposite charge as the proton and neutron respectively, and they interact in the same way. (This is generally believed to be "exactly" true, due to CPT symmetry. If there is a difference, it is too small to measure in all experiments to date.) In particular, antinucleons can bind into an "antinucleus". So far, scientists have created antideuterium and antihelium-3 nuclei.
Tables of detailed properties.
Nucleons.
 The masses of the proton and neutron are known with far greater precision in atomic mass units (u) than in MeV/c2, due to the relatively poorly known value of the elementary charge. The conversion factor used is 1 u = MeV/c2.
The masses of their antiparticles are assumed to be identical, and no experiments have refuted this to date. Current experiments show any percent difference between the masses of the proton and antiproton must be less than and the difference between the neutron and antineutron masses is on the order of MeV/c2.
† "The P11(939) nucleon represents the excited state of a normal proton or neutron, for example, within the nucleus of an atom. Such particles are usually stable within the nucleus, i.e. Lithium-6."
Quark model classification.
In the quark model with SU(2) flavour, the two nucleons are part of the ground state doublet. The proton has quark content of "uud", and the neutron, "udd". In SU(3) flavour, they are part of the ground state octet (8) of spin baryons, known as the Eightfold way. The other members of this octet are the hyperons strange isotriplet , , the and the strange isodoublet . One can extend this multiplet in SU(4) flavour (with the inclusion of the charm quark) to the ground state 20-plet, or to SU(6) flavour (with the inclusion of the top and bottom quarks) to the ground state 56-plet.
The article on isospin provides an explicit expression for the nucleon wave functions in terms of the quark flavour eigenstates.
Models.
Although it is known that the nucleon is made from three quarks, , it is not known how to solve the equations of motion for quantum chromodynamics. Thus, the study of the low-energy properties of the nucleon are performed by means of models. The only first-principles approach available is to attempt to solve the equations of QCD numerically, using lattice QCD. This requires complicated algorithms and very powerful supercomputers. However, several analytic models also exist:
The Skyrmion models the nucleon as a topological soliton in a non-linear SU(2) pion field. The topological stability of the Skyrmion is interpreted as the conservation of baryon number, that is, the non-decay of the nucleon. The local topological winding number density is identified with the local baryon number density of the nucleon. With the pion isospin vector field oriented in the shape of a hedgehog space, the model is readily solvable, and is thus sometimes called the hedgehog model. The hedgehog model is able to predict low-energy parameters, such as the nucleon mass, radius and axial coupling constant, to approximately 30% of experimental values.
The MIT bag model confines three non-interacting quarks to a spherical cavity, with the boundary condition that the quark vector current vanish on the boundary. The non-interacting treatment of the quarks is justified by appealing to the idea of asymptotic freedom, whereas the hard boundary condition is justified by quark confinement. Mathematically, the model vaguely resembles that of a radar cavity, with solutions to the Dirac equation standing in for solutions to the Maxwell equations and the vanishing vector current boundary condition standing for the conducting metal walls of the radar cavity. If the radius of the bag is set to the radius of the nucleon, the bag model predicts a nucleon mass that is within 30% of the actual mass. 
Although the basic bag model does not provide a pion-mediated interaction, it describes excellently the nucleon-nucleon forces through the 6-quark bag s-channel mechanism using the P matrix. 
The chiral bag model merges the MIT bag model and the Skyrmion model. In this model, a hole is punched out of the middle of the Skyrmion, and replaced with a bag model. The boundary condition is provided by the requirement of continuity of the axial vector current across the bag boundary. Very curiously, the missing part of the topological winding number (the baryon number) of the hole punched into the Skyrmion is exactly made up by the non-zero vacuum expectation value (or spectral asymmetry) of the quark fields inside the bag. , this remarkable trade-off between topology and the spectrum of an operator does not have any grounding or explanation in the mathematical theory of Hilbert spaces and their relationship to geometry. Several other properties of the chiral bag are notable: it provides a better fit to the low energy nucleon properties, to within 5–10%, and these are almost completely independent of the chiral bag radius (as long as the radius is less than the nucleon radius). This independence of radius is referred to as the Cheshire Cat principle, after the fading to a smile of Lewis Carroll's Cheshire Cat. It is expected that a first-principles solution of the equations of QCD will demonstrate a similar duality of quark-pion descriptions.

</doc>
<doc id="21527" url="https://en.wikipedia.org/wiki?curid=21527" title="Number theory">
Number theory

Number theory or arithmetic is a branch of pure mathematics devoted primarily to the study of the natural numbers and the integers. It is sometimes called "The Queen of Mathematics" because of its foundational place in the discipline. Number theorists study prime numbers as well as the properties of objects made out of integers (e.g., rational numbers) or defined as generalizations of the integers (e.g., algebraic integers).
Integers can be considered either in themselves or as solutions to equations (Diophantine geometry). Questions in number theory are often best understood through the study of analytical objects (e.g., the Riemann zeta function) that encode properties of the integers, primes or other number-theoretic objects in some fashion (analytic number theory). One may also study real numbers in relation to rational numbers, e.g., as approximated by the latter (Diophantine approximation).
The older term for number theory is "arithmetic". By the early twentieth century, it had been superseded by "number theory". (The word "arithmetic" is used by the general public to mean "elementary calculations"; it has also acquired other meanings in mathematical logic, as in "Peano arithmetic", and computer science, as in "floating point arithmetic".) The use of the term "arithmetic" for "number theory" regained some ground in the second half of the 20th century, arguably in part due to French influence. In particular, "arithmetical" is preferred as an adjective to "number-theoretic".
History.
Origins.
Dawn of arithmetic.
The first historical find of an arithmetical nature is a fragment of a table: the broken clay tablet Plimpton 322 (Larsa, Mesopotamia, ca. 1800 BCE) contains a list of "Pythagorean triples", i.e., integers formula_1 such that formula_2.
The triples are too many and too large to have been obtained by brute force. The heading over the first column reads: "The "takiltum" of the diagonal which has been subtracted such that the width..."
The table's layout suggests that it was constructed by means of what amounts, in modern language, to the identity
formula_3
which is implicit in routine Old Babylonian exercises. If some other method was used, the triples were first constructed and then reordered by formula_4, presumably for actual use as a "table", i.e., with a view to applications.
It is not known what these applications may have been, or whether there could have been any; Babylonian astronomy, for example, truly flowered only later. It has been suggested instead that the table was a source of numerical examples for school problems.
While Babylonian number theory—or what survives of Babylonian mathematics that can be called thus—consists of this single, striking fragment, Babylonian algebra (in the secondary-school sense of "algebra") was exceptionally well developed. Late Neoplatonic sources state that Pythagoras learned mathematics from the Babylonians. Much earlier sources state that Thales and Pythagoras traveled and studied in Egypt.
Euclid IX 21—34 is very probably Pythagorean; it is very simple material ("odd times even is even", "if an odd number measures [= divides] an even number, then it also measures [= divides] half of it"), but it is all that is needed to prove that formula_5
is irrational. Pythagorean mystics gave great importance to the odd and the even.
The discovery that formula_5 is irrational is credited to the early Pythagoreans (pre-Theodorus). By revealing (in modern terms) that numbers could be irrational, this discovery seems to have provoked the first foundational crisis in mathematical history; its proof or its divulgation are sometimes credited to Hippasus, who was expelled or split from the Pythagorean sect. This forced a distinction between "numbers" (integers and the rationals—the subjects of arithmetic), on the one hand, and "lengths" and "proportions" (which we would identify with real numbers, whether rational or not), on the other hand.
The Pythagorean tradition spoke also of so-called polygonal or figurate numbers. While square numbers, cubic numbers, etc., are seen now as more natural than triangular numbers, pentagonal numbers, etc., the study of the sums
of triangular and pentagonal numbers would prove fruitful in the early modern period (17th to early 19th century).
We know of no clearly arithmetical material in ancient Egyptian or Vedic sources, though there is some algebra in both. The Chinese remainder theorem appears as an exercise in Sun Zi's "Suan Ching", also known as "The Mathematical Classic of Sun Zi" (3rd, 4th or 5th century CE.) (There is one important step glossed over in Sun Zi's solution: it is the problem that was later solved by Āryabhaṭa's kuṭṭaka – see below.)
There is also some numerical mysticism in Chinese mathematics, but, unlike that of the Pythagoreans, it seems to have
led nowhere. Like the Pythagoreans' perfect numbers, magic squares have passed from superstition into recreation.
Classical Greece and the early Hellenistic period.
Aside from a few fragments, the mathematics of Classical Greece is known to us either through the reports of contemporary non-mathematicians or through mathematical works from the early Hellenistic period. In the case of number theory, this means, by and large, "Plato" and "Euclid", respectively.
Plato had a keen interest in mathematics, and distinguished clearly between arithmetic and calculation. (By "arithmetic" he meant, in part, theorising on number, rather than what "arithmetic" or "number theory" have come to mean.) It is through one of Plato's dialogues—namely, "Theaetetus"—that we know that Theodorus had proven that formula_7 are irrational. Theaetetus was, like Plato, a disciple of Theodorus's; he worked on distinguishing different kinds of incommensurables, and was thus arguably a pioneer in the study of number systems. (Book X of Euclid's Elements is described by Pappus as being largely based on Theaetetus's work.)
Euclid devoted part of his "Elements" to prime numbers and divisibility, topics that belong unambiguously to number theory and are basic to it (Books VII to IX of Euclid's Elements). In particular, he gave an algorithm for computing the greatest common divisor of two numbers (the Euclidean algorithm; "Elements", Prop. VII.2) and the first known proof of the infinitude of primes ("Elements", Prop. IX.20).
In 1773, Lessing published an epigram he had found in a manuscript during his work as a librarian; it claimed to be a letter sent by Archimedes to Eratosthenes. The epigram proposed what has become known as
Archimedes' cattle problem; its solution (absent from the manuscript) requires solving an indeterminate quadratic equation (which reduces to what would later be misnamed Pell's equation). As far as we know, such equations were first successfully treated by the Indian school. It is not known whether Archimedes himself had a method of solution.
Diophantus.
Very little is known about Diophantus of Alexandria; he probably lived in the third century CE, that is, about five hundred years after Euclid. Six out of the thirteen books of Diophantus's "Arithmetica" survive in the original Greek; four more books survive in an Arabic translation. The "Arithmetica" is a collection of worked-out problems where the task is invariably to find rational solutions to a system of polynomial equations, usually of the form formula_8 or formula_9. Thus, nowadays, we speak of "Diophantine equations" when we speak of polynomial equations to which rational or integer solutions must be found.
One may say that Diophantus was studying rational points — i.e., points whose coordinates are rational — on curves and algebraic varieties; however, unlike the Greeks of the Classical period, who did what we would now call basic algebra in geometrical terms, Diophantus did what we would now call basic algebraic geometry in purely algebraic terms. In modern language, what Diophantus did was to find rational parametrizations of varieties; that is, given an equation of the form (say)
formula_10, his aim was to find (in essence) three rational functions formula_11 such that, for all values of formula_12 and formula_13, setting
formula_14 for formula_15 gives a solution to formula_16
Diophantus also studied the equations of some non-rational curves, for which no rational parametrisation is possible. He managed to find some rational points on these curves (elliptic curves, as it happens, in what seems to be their first known occurrence) by means of what amounts to a tangent construction: translated into coordinate geometry
While Diophantus was concerned largely with rational solutions, he assumed some results on integer numbers, in particular that every integer is the sum of four squares (though he never stated as much explicitly).
Āryabhaṭa, Brahmagupta, Bhāskara.
While Greek astronomy probably influenced Indian learning, to the point of introducing trigonometry, it seems to be the case that Indian mathematics is otherwise an indigenous tradition; in particular, there is no evidence that Euclid's Elements reached India before the 18th century.
Āryabhaṭa (476–550 CE) showed that pairs of simultaneous congruences formula_17, formula_18 could be solved by a method he called "kuṭṭaka", or "pulveriser"; this is a procedure close to (a generalisation of) the Euclidean algorithm, which was probably discovered independently in India. Āryabhaṭa seems to have had in mind applications to astronomical calculations.
Brahmagupta (628 CE) started the systematic study of indefinite quadratic equations—in particular, the misnamed Pell equation, in which Archimedes may have first been interested, and which did not start to be solved in the West until the time of Fermat and Euler. Later Sanskrit authors would follow, using Brahmagupta's technical terminology. A general procedure (the chakravala, or "cyclic method") for solving Pell's equation was finally found by Jayadeva (cited in the eleventh century; his work is otherwise lost); the earliest surviving exposition appears in Bhāskara II's Bīja-gaṇita (twelfth century).
Unfortunately, Indian mathematics remained largely unknown in the West until the late eighteenth century; Brahmagupta and Bhāskara's work was translated into English in 1817 by Henry Colebrooke.
Arithmetic in the Islamic golden age.
In the early ninth century, the caliph Al-Ma'mun ordered translations of many Greek mathematical works and at least one Sanskrit work (the "Sindhind",
which may or may not be Brahmagupta's Brāhmasphuţasiddhānta).
Diophantus's main work, the "Arithmetica", was translated into Arabic by Qusta ibn Luqa (820–912).
Part of the treatise "al-Fakhri" (by al-Karajī, 953 – ca. 1029) builds on it to some extent. According to Rashed Roshdi, Al-Karajī's contemporary Ibn al-Haytham knew what would later be called Wilson's theorem.
Western Europe in the Middle Ages.
Other than a treatise on squares in arithmetic progression by Fibonacci — who lived and studied in north Africa and Constantinople during his formative years, ca. 1175–1200 — no number theory to speak of was done in western Europe during the Middle Ages. Matters started to change in Europe in the late Renaissance, thanks to a renewed study of the works of Greek antiquity. A catalyst was the textual emendation and translation into Latin of Diophantus's "Arithmetica" (Bachet, 1621, following a first attempt by Xylander, 1575).
Early modern number theory.
Fermat.
Pierre de Fermat (1601–1665) never published his writings; in particular, his work on number theory is contained almost entirely in letters to mathematicians and in private marginal notes. He wrote down nearly no proofs in number theory; he had no models in the area. He did make repeated use of mathematical induction, introducing the method of infinite descent.
One of Fermat's first interests was perfect numbers (which appear in Euclid, "Elements" IX) and amicable numbers; this led him to work on integer divisors, which were from the beginning among the subjects of the
correspondence (1636 onwards) that put him in touch with the mathematical community of the day. He had already studied Bachet's edition of Diophantus carefully; by 1643, his interests had shifted largely to Diophantine problems and sums of squares (also treated by Diophantus).
Fermat's achievements in arithmetic include:
Fermat's claim ("Fermat's last theorem") to have shown there are no solutions to
formula_25 for all formula_26 (the only known proof of which is beyond his methods) appears only in his annotations on the margin of his copy of Diophantus; he never claimed this to others and thus would have had no need to retract it if he found any mistake in his supposed proof.
Euler.
The interest of Leonhard Euler (1707–1783) in number theory was first spurred in 1729, when a friend of his, the amateur Goldbach, pointed him towards some of Fermat's work on the subject. This has been called the "rebirth" of modern number theory, after Fermat's relative lack of success in getting his contemporaries' attention for the subject. Euler's work on number theory includes the following:
Lagrange, Legendre and Gauss.
Joseph-Louis Lagrange (1736–1813) was the first to give full proofs of some of Fermat's and Euler's work and observations - for instance, the four-square theorem and the basic theory of the misnamed "Pell's equation" (for which an algorithmic solution was found by Fermat and his contemporaries, and also by Jayadeva and Bhaskara II before them.) He also studied quadratic forms in full generality (as opposed to formula_31) — defining their equivalence relation, showing how to put them in reduced form, etc.
Adrien-Marie Legendre (1752–1833) was the first to state the law of quadratic reciprocity. He also
conjectured what amounts to the prime number theorem and Dirichlet's theorem on arithmetic progressions. He gave a full treatment of the equation formula_32 and worked on quadratic forms along the lines later developed fully by Gauss. In his old age, he was the first to prove "Fermat's last theorem" for formula_33 (completing work by Peter Gustav Lejeune Dirichlet, and crediting both him and Sophie Germain).
In his "Disquisitiones Arithmeticae" (1798), Carl Friedrich Gauss (1777–1855) proved the law of quadratic reciprocity and developed the theory of quadratic forms (in particular, defining their composition). He also introduced some basic notation (congruences) and devoted a section to computational matters, including primality tests. The last section of the "Disquisitiones" established a link between roots of unity and number theory:
The theory of the division of the circle...which is treated in sec. 7 does not belong
by itself to arithmetic, but its principles can only be drawn from higher arithmetic.
In this way, Gauss arguably made a first foray towards both Évariste Galois's work and algebraic number theory.
Maturity and division into subfields.
Starting early in the nineteenth century, the following developments gradually took place:
Algebraic number theory may be said to start with the study of reciprocity and cyclotomy, but truly came into its own with the development of abstract algebra and early ideal theory and valuation theory; see below. A conventional starting point for analytic number theory is Dirichlet's theorem on arithmetic progressions (1837), whose proof introduced L-functions and involved some asymptotic analysis and a limiting process on a real variable. The first use of analytic ideas in number theory actually
goes back to Euler (1730s), who used formal power series and non-rigorous (or implicit) limiting arguments. The use of "complex" analysis in number theory comes later: the work of Bernhard Riemann (1859) on the zeta function is the canonical starting point; Jacobi's four-square theorem (1839), which predates it, belongs to an initially different strand that has by now taken a leading role in analytic number theory (modular forms).
The history of each subfield is briefly addressed in its own section below; see the main article of each subfield for fuller treatments. Many of the most interesting questions in each area remain open and are being actively worked on.
Main subdivisions.
Elementary tools.
The term "elementary" generally denotes a method that does not use complex analysis. For example, the prime number theorem was first proven using complex analysis in 1896, but an elementary proof was found only in 1949 by Erdős and Selberg. The term is somewhat ambiguous: for example, proofs based on complex Tauberian theorems (e.g. Wiener–Ikehara) are often seen as quite enlightening but not elementary, in spite of using Fourier analysis, rather than complex analysis as such. Here as elsewhere, an "elementary" proof may be longer and more difficult for most readers than a non-elementary one.
Number theory has the reputation of being a field many of whose results can be stated to the layperson. At the same time, the proofs of these results are not particularly accessible, in part because the range of tools they use is, if anything, unusually broad within mathematics.
Analytic number theory.
"Analytic number theory" may be defined
Some subjects generally considered to be part of analytic number theory, e.g., sieve theory, are better covered by the second rather than the first definition: some of sieve theory, for instance, uses little analysis, yet it does belong to analytic number theory.
The following are examples of problems in analytic number theory: the prime number theorem, the Goldbach conjecture (or the twin prime conjecture, or the Hardy–Littlewood conjectures), the Waring problem and the Riemann Hypothesis. Some of the most important tools of analytic number theory are the circle method, sieve methods and L-functions (or, rather, the study of their properties). The theory of modular forms (and, more generally, automorphic forms) also occupies an increasingly central place in the toolbox of analytic number theory.
One may ask analytic questions about algebraic numbers, and use analytic means to answer such questions; it is thus that algebraic and analytic number theory intersect. For example, one may define prime ideals (generalizations of prime numbers in the field of algebraic numbers) and ask how many prime ideals there are up to a certain size. This question can be answered by means of an examination of Dedekind zeta functions, which are generalizations of the Riemann zeta function, a key analytic object at the roots of the subject. This is an example of a general procedure in analytic number theory: deriving information about the distribution of a sequence (here, prime ideals or prime numbers) from the analytic behavior of an appropriately constructed complex-valued function.
Algebraic number theory.
An "algebraic number" is any complex number that is a solution to some polynomial equation formula_34 with rational coefficients; for example, every solution formula_35 of formula_36 (say) is an algebraic number. Fields of algebraic numbers are also called "algebraic number fields", or shortly "number fields". Algebraic number theory studies algebraic number fields. Thus, analytic and algebraic number theory can and do overlap: the former is defined by its methods, the latter by its objects of study.
It could be argued that the simplest kind of number fields (viz., quadratic fields) were already studied by Gauss, as the discussion of quadratic forms in "Disquisitiones arithmeticae" can be restated in terms of ideals and
norms in quadratic fields. (A "quadratic field" consists of all
numbers of the form formula_37, where
formula_38 and formula_39 are rational numbers and formula_40
is a fixed rational number whose square root is not rational.)
For that matter, the 11th-century chakravala method amounts—in modern terms—to an algorithm for finding the units of a real quadratic number field. However, neither Bhāskara nor Gauss knew of number fields as such.
The grounds of the subject as we know it were set in the late nineteenth century, when "ideal numbers", the "theory of ideals" and "valuation theory" were developed; these are three complementary ways of dealing with the lack of unique factorisation in algebraic number fields. (For example, in the field generated by the rationals
and formula_41, the number formula_42 can be factorised both as formula_43 and
formula_44; all of formula_45, formula_46, formula_47 and
formula_48
are irreducible, and thus, in a naïve sense, analogous to primes among the integers.) The initial impetus for the development of ideal numbers (by Kummer) seems to have come from the study of higher reciprocity laws,i.e., generalisations of quadratic reciprocity.
Number fields are often studied as extensions of smaller number fields: a field "L" is said to be an "extension" of a field "K" if "L" contains "K".
Classifying the possible extensions of a given number field is a difficult and partially open problem. Abelian extensions—that is, extensions "L" of "K" such that the Galois group Gal("L"/"K") of "L" over "K" is an abelian group—are relatively well understood.
Their classification was the object of the programme of class field theory, which was initiated in the late 19th century (partly by Kronecker and Eisenstein) and carried out largely in 1900—1950.
An example of an active area of research in algebraic number theory is Iwasawa theory. The Langlands program, one of the main current large-scale research plans in mathematics, is sometimes described as an attempt to generalise class field theory to non-abelian extensions of number fields.
Diophantine geometry.
The central problem of "Diophantine geometry" is to determine when a Diophantine equation has solutions, and if it does, how many. The approach taken is to think of the solutions of an equation as a geometric object.
For example, an equation in two variables defines a curve in the plane. More generally, an equation, or system of equations, in two or more variables defines a curve, a surface or some other such object in "n"-dimensional space. In Diophantine geometry, one asks whether there are any "rational points" (points all of whose coordinates are rationals) or
"integral points" (points all of whose coordinates are integers) on the curve or surface. If there are any such points, the next step is to ask how many there are and how they are distributed. A basic question in this direction is: are there finitely
or infinitely many rational points on a given curve (or surface)? What about integer points?
An example here may be helpful. Consider the Pythagorean equation formula_49;
we would like to study its rational solutions, i.e., its solutions
formula_50 such that
"x" and "y" are both rational. This is the same as asking for all integer solutions
to formula_51; any solution to the latter equation gives
us a solution formula_52, formula_53 to the former. It is also the
same as asking for all points with rational coordinates on the curve
described by formula_54. (This curve happens to be a circle of radius 1 around the origin.)
The rephrasing of questions on equations in terms of points on curves turns out to be felicitous. The finiteness or not of the number of rational or integer points on an algebraic curve—that is, rational or integer solutions to an equation formula_55, where formula_56 is a polynomial in two variables—turns out to depend crucially on the "genus" of the curve. The "genus" can be defined as follows: allow the variables in formula_55 to be complex numbers; then formula_55 defines a 2-dimensional surface in (projective) 4-dimensional space (since two complex variables can be decomposed into four real variables, i.e., four dimensions). Count
the number of (doughnut) holes in the surface; call this number the "genus" of formula_55. Other geometrical notions turn out to be just as crucial.
There is also the closely linked area of Diophantine approximations: given a number formula_35, how well can it be approximated by rationals? (We are looking for approximations that are good relative to the amount of space that it takes to write the rational: call formula_61 (with formula_62) a good approximation to formula_35 if formula_64, where formula_65 is large.) This question is of special interest if formula_35 is an algebraic number. If formula_35 cannot be well approximated, then some equations do not have integer or rational solutions. Moreover, several concepts (especially that of height) turn out to be crucial both in Diophantine geometry and in the study of Diophantine approximations. This question is also of special interest in transcendental number theory: if a number can be better approximated than any algebraic number, then it is a transcendental number. It is by this argument that and e have been shown to be transcendental.
Diophantine geometry should not be confused with the geometry of numbers, which is a collection of graphical methods for answering certain questions in algebraic number theory. "Arithmetic geometry", on the other hand, is a contemporary term
for much the same domain as that covered by the term "Diophantine geometry". The term "arithmetic geometry" is arguably used
most often when one wishes to emphasise the connections to modern algebraic geometry (as in, for instance, Faltings' theorem) rather than to techniques in Diophantine approximations.
Recent approaches and subfields.
The areas below date as such from no earlier than the mid-twentieth century, even if they are based on older material. For example, as is explained below, the matter of algorithms in number theory is very old, in some sense older than the concept of proof; at the same time, the modern study of computability dates only from the 1930s and 1940s, and computational complexity theory from the 1970s.
Probabilistic number theory.
Take a number at random between one and a million. How likely is it to be prime? This is just another way of asking how many primes there are between one and a million. Further: how many prime divisors will it have, on average? How many divisors will it have altogether, and with what likelihood? What is the probability that it will have many more or many fewer divisors or prime divisors than the average?
Much of probabilistic number theory can be seen as an important special case of the study of variables that are almost, but not quite, mutually independent. For example, the event that a random integer between one and a million be divisible by two and the event that it be divisible by three are almost independent, but not quite.
It is sometimes said that probabilistic combinatorics uses the fact that whatever happens with probability greater than formula_68 must happen sometimes; one may say with equal justice that many applications of probabilistic number theory hinge on the fact that whatever is unusual must be rare. If certain algebraic objects (say, rational or integer solutions to certain equations) can be shown to be in the tail of certain sensibly defined distributions, it follows that there must be few of them; this is a very concrete non-probabilistic statement following from a probabilistic one.
At times, a non-rigorous, probabilistic approach leads to a number of heuristic algorithms and open problems, notably Cramér's conjecture.
Arithmetic combinatorics.
Let "A" be a set of "N" integers. Consider the set "A" + "A" = { "m" + "n" | "m", "n" ∈ "A" } consisting of all sums of two elements of "A". Is "A + A" much larger than "A"? Barely larger? If "A + A" is barely larger than "A", must "A" have plenty of arithmetic structure, for example, does "A" resemble an arithmetic progression?
If we begin from a fairly "thick" infinite set formula_69, does it contain many elements in arithmetic progression: formula_38,
formula_71, formula_72, formula_73, formula_74 , formula_75, say? Should it be possible to write large integers as sums of elements of formula_69?
These questions are characteristic of "arithmetic combinatorics". This is a presently coalescing field; it subsumes "additive number theory" (which concerns itself with certain very specific sets formula_69 of arithmetic significance, such as the primes or the squares) and, arguably, some of the "geometry of numbers",
together with some rapidly developing new material. Its focus on issues of growth and distribution accounts in part for its developing links with ergodic theory, finite group theory, model theory, and other fields. The term "additive combinatorics" is also used; however, the sets formula_69 being studied need not be sets of integers, but rather subsets of non-commutative groups, for which the multiplication symbol, not the addition symbol, is traditionally used; they can also be subsets of rings, in which case the growth of formula_79 and formula_69·formula_69 may be
compared.
Computations in number theory.
While the word "algorithm" goes back only to certain readers of al-Khwārizmī, careful descriptions of methods of solution are older than proofs: such methods (that is, algorithms) are as old as any recognisable mathematics—ancient Egyptian, Babylonian, Vedic, Chinese—whereas proofs appeared only with the Greeks of the classical period.
An interesting early case is that of what we now call the Euclidean algorithm. In its basic form (namely, as an algorithm for computing the greatest common divisor) it appears as Proposition 2 of Book VII in "Elements", together with a proof of correctness. However, in the form that is often used in number theory (namely, as an algorithm for finding integer solutions to an equation formula_82,
or, what is the same, for finding the quantities whose existence is assured by the Chinese remainder theorem) it first appears in the works of Āryabhaṭa (5th–6th century CE) as an algorithm called
"kuṭṭaka" ("pulveriser"), without a proof of correctness.
There are two main questions: "can we compute this?" and "can we compute it rapidly?". Anybody can test whether a number is prime or, if it is not, split it into prime factors; doing so rapidly is another matter. We now know fast algorithms for testing primality, but, in spite of much work (both theoretical and practical), no truly fast algorithm for factoring.
The difficulty of a computation can be useful: modern protocols for encrypting messages (e.g., RSA) depend on functions that are known to all, but whose inverses (a) are known only to a chosen few, and (b) would take one too long a time to figure out on one's own. For example, these functions can be such that their inverses can be computed only if certain large integers are factorized. While many difficult computational problems outside number theory are known, most working encryption protocols nowadays are based on the difficulty of a few number-theoretical problems.
On a different note — some things may not be computable at all; in fact, this can be proven in some instances. For instance, in 1970, it was proven, as a solution to Hilbert's 10th problem, that there is no Turing machine which can solve all Diophantine equations. In particular, this means that, given a computably enumerable set of axioms, there are Diophantine equations for which there is no proof, starting from the axioms, of whether the set of equations has or does not have integer solutions. (We would necessarily be speaking of Diophantine equations for which there are no integer solutions, since, given a Diophantine equation with at least one solution, the solution itself provides a proof of the fact that a solution exists. We cannot prove, of course, that a particular Diophantine equation is of this kind, since this would imply that it has no solutions.)
Applications.
The number-theorist Leonard Dickson (1874-1954) said "Thank God that number theory is unsullied by any application". Such a view is no longer applicable to number theory. In 1974, Donald Knuth said "...virtually every theorem in elementary number theory arises in a natural, motivated way in connection with the problem of making computers do high-speed numerical calculations".
Elementary number theory is taught in discrete mathematics courses for computer scientists; and, on the other hand, number theory also has applications to the continuous in numerical analysis. As well as the well-known applications to cryptography, there are also applications to many other areas of mathematics.
Literature.
Two of the most popular introductions to the subject are:
Hardy and Wright's book is a comprehensive classic, though its clarity sometimes suffers due to the authors' insistence on elementary methods.
Vinogradov's main attraction consists in its set of problems, which quickly lead to Vinogradov's own research interests; the text itself is very basic and close to minimal. Other popular first introductions are:
Popular choices for a second textbook include:
Prizes.
The American Mathematical Society awards the "Cole Prize in Number Theory". Moreover number theory is one of the three mathematical subdisciplines rewarded by the "Fermat Prize".
Sources.
 

</doc>
<doc id="294248" url="https://en.wikipedia.org/wiki?curid=294248" title="Observable">
Observable

In physics, particularly in quantum physics, a system observable is a measurable operator, or gauge, where the property of the system state can be determined by some sequence of physical operations. For example, these operations might involve submitting the system to various electromagnetic fields and eventually reading a value. In systems governed by classical mechanics, any experimentally observable value can be shown to be given by a real-valued function on the set of all possible system states. 
Physically meaningful observables must also satisfy transformation laws which relate observations performed by different observers in different frames of reference. These transformation laws are automorphisms of the state space, that is bijective transformations which preserve some mathematical property. 
Quantum mechanics.
In quantum physics, the relation between system state and the value of an observable requires some basic linear algebra for its description. In the mathematical formulation of quantum mechanics, states are given by non-zero vectors in a Hilbert space "V" (where two vectors are considered to specify the same state if, and only if, they are scalar multiples of each other) and observables are given by self-adjoint operators on "V". However, as indicated below, not every self-adjoint operator corresponds to a physically meaningful observable. For the case of a system of particles, the space "V" consists of functions called wave functions or state vectors.
In the case of transformation laws in quantum mechanics, the requisite automorphisms are unitary (or antiunitary) linear transformations of the Hilbert space "V". Under Galilean relativity or special relativity, the mathematics of frames of reference is particularly simple, and in fact restricts considerably the set of physically meaningful observables.
In quantum mechanics, measurement of observables exhibits some seemingly unintuitive properties. Specifically, if a system is in a state described by a vector in a Hilbert space, the measurement process affects the state in a non-deterministic, but statistically predictable way. In particular, after a measurement is applied, the state description by a single vector may be destroyed, being replaced by a statistical ensemble. The irreversible nature of measurement operations in quantum physics is sometimes referred to as the measurement problem and is described mathematically by quantum operations. By the structure of quantum operations, this description is mathematically equivalent to that offered by relative state interpretation where the original system is regarded as a subsystem of a larger system and the state of the original system is given by the partial trace of the state of the larger system.
In quantum mechanics each dynamical variable (e.g. position, translational momentum, orbital angular momentum, spin, total angular momentum, energy, etc.) is associated with a Hermitian operator that acts on the state of the quantum system and whose eigenvalues correspond to the possible values of the dynamical variable. For example, suppose formula_1 is an eigenket (eigenvector) of the observable formula_2, with eigenvalue formula_3, and exists in a d-dimensional Hilbert space. Then
This eigenket equation says that if a measurement of the observable formula_8 is made while the system of interest is in the state formula_9, then the observed value of that particular measurement must return the eigenvalue formula_3 with certainty. However, if the system of interest is in the general state formula_11, then the eigenvalue formula_3 is returned with probability formula_13 (Born rule). One must note that the above definition is somewhat dependent upon our convention of choosing real numbers to represent real physical quantities. Indeed, just because dynamical variables are "real" and not "unreal" in the metaphysical sense does not mean that they must correspond to real numbers in the mathematical sense. 
To be more precise, the dynamical variable/observable is a (not necessarily bounded) Hermitian operator in a Hilbert space and thus is represented by a Hermitian matrix if the space is finite-dimensional. In an infinite-dimensional Hilbert space, the observable is represented by a symmetric operator, which may not be "defined everywhere" (i.e. its domain is not the whole space - there exist some states that are not in the domain of the operator). The reason for such a change is that in an infinite-dimensional Hilbert space, the operator becomes unbounded, which means that it no longer has a largest eigenvalue. This is not the case in a finite-dimensional Hilbert space, where every operator is bounded - it has a largest eigenvalue. For example, if we consider the position of a point particle moving along a line, this particle's position variable can take on any number on the real-line, which is uncountably infinite. Since the eigenvalue of an observable represents a real physical quantity for that particular dynamical variable, then we must conclude that there is no largest eigenvalue for the position observable in this uncountably infinite-dimensional Hilbert space, since the field we're working over consists of the real-line. Nonetheless, whether we are working in an infinite-dimensional or finite-dimensional Hilbert space, the role of an observable in quantum mechanics is to assign real numbers to outcomes of "particular measurements"; this means that only certain measurements can determine the value of an observable for some state of a quantum system. In classical mechanics, "any" measurement can be made to determine the value of an observable.
Incompatibility of observables in quantum mechanics.
A crucial difference between classical quantities and quantum mechanical observables is that the latter may not be simultaneously measurable. This is mathematically expressed by non-commutativity of the corresponding operators, to the effect that
This inequality expresses a dependence of measurement results on the order in which measurements of observables formula_8 and formula_16 are performed. Observables corresponding to non-commutative operators are called "incompatible".

</doc>
<doc id="207020" url="https://en.wikipedia.org/wiki?curid=207020" title="Omega baryon">
Omega baryon

The Omega baryons are a family of subatomic hadron particles that are represented by the symbol
 and are either neutral or have a +2, +1 or −1 elementary charge. They are baryons containing no up or down quarks. Omega baryons containing top quarks are not expected to be observed. This is because the Standard Model predicts the mean lifetime of top quarks to be roughly , which is about a twentieth of the timescale for strong interactions, and therefore that they do not form hadrons.
The first Omega baryon discovered was the , made of three strange quarks, in 1964. The discovery was a great triumph in the study of quark processes, since it was found only after its existence, mass, and decay products had been predicted in 1962 by the American physicist Murray Gell-Mann and, independently, by the Israeli physicist Yuval Ne'eman. Besides the , a charmed Omega particle () was discovered, in which a strange quark is replaced by a charm quark. The decays only via the weak interaction and has therefore a relatively long lifetime. Spin ("J") and parity ("P") values for unobserved baryons are predicted by the quark model.
Since Omega baryons do not have any up or down quarks, they all have isospin 0.
Omega baryons.
† Particle (or quantity, i.e. spin) has neither been observed nor indicated.
Recent discoveries.
The particle is a "doubly strange" baryon containing two strange quarks and a bottom quark. A discovery of this particle was first claimed in September 2008 by physicists working on the DØ experiment at the Tevatron facility of the Fermi National Accelerator Laboratory. However, the reported mass, , was significantly higher than expected in the quark model. The apparent discrepancy from the Standard Model has since been dubbed the " puzzle". In May 2009, the CDF collaboration made public their results on the search for the based on analysis of a data sample roughly four times the size of the one used by the DØ experiment. CDF measured the mass to be , which was in excellent agreement with the Standard Model prediction. No signal has been observed at the DØ reported value. The two results differ by , which is equivalent to 6.2 standard deviations and are therefore inconsistent. Excellent agreement between the CDF measured mass and theoretical expectations is a strong indication that the particle discovered by CDF is indeed the .

</doc>
